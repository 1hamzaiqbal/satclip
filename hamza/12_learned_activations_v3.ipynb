{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learned Activations v3: Corrected Architecture & Fair Evaluation\n",
    "\n",
    "## Fixes from v1/v2 based on independent review:\n",
    "\n",
    "1. **Shared gating network** - Gate is now shared across all layers (not per-layer)\n",
    "2. **Fair SatCLIP baseline** - Ridge regression on frozen embeddings (not MLP)\n",
    "3. **Correct SIREN config** - w0_initial=30 for first layer, w0=1 for subsequent (matches SatCLIP)\n",
    "4. **Correct HybridEncoder init** - SIREN-specific initialization when using sine activations\n",
    "5. **Spatial blocking** - Grid-based train/test splits to prevent spatial leakage\n",
    "\n",
    "## Architecture Summary\n",
    "\n",
    "### SatCLIP (baseline)\n",
    "- Spherical Harmonics (L=10: 100 features, L=40: 1600 features)\n",
    "- SIREN network: w0_initial=30 (first layer), w0=1 (subsequent layers)\n",
    "- Output: 256-dim embedding\n",
    "\n",
    "### Our Learned Activations\n",
    "- Fourier-parameterized: g(x) = sum_k a_k*sin(w_k*x) + b_k*cos(w_k*x)\n",
    "- Spatially-varying: Mixture of experts with **shared** location-based gating\n",
    "- Direct (lat, lon) input OR hybrid with SH features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    !rm -rf sample_data .config satclip gpw_data 2>/dev/null\n",
    "    !git clone https://github.com/1hamzaiqbal/satclip.git\n",
    "    !pip install lightning torchgeo huggingface_hub rasterio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and extract GPW data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "GPW_DIR = './gpw_data'\n",
    "os.makedirs(GPW_DIR, exist_ok=True)\n",
    "\n",
    "SOURCE_ZIP_PATH = '/content/drive/MyDrive/grad/learned_activations/dataverse_files.zip'\n",
    "\n",
    "print(\"Extracting GPW data...\")\n",
    "with zipfile.ZipFile(SOURCE_ZIP_PATH, 'r') as z:\n",
    "    z.extractall(GPW_DIR)\n",
    "\n",
    "zip_path = os.path.join(GPW_DIR, 'gpw-v4-population-density-rev11_2020_15_min_tif.zip')\n",
    "if os.path.exists(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        z.extractall(GPW_DIR)\n",
    "    print(\"Extracted 15-min resolution\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import warnings\n",
    "import math\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    sys.path.append('./satclip/satclip')\n",
    "    GPW_DIR = './gpw_data'\n",
    "else:\n",
    "    sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'satclip'))\n",
    "    GPW_DIR = './gpw_data'\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from load import get_satclip\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load SatCLIP models\n",
    "print(\"Loading SatCLIP models...\")\n",
    "satclip_l10 = get_satclip(hf_hub_download(\"microsoft/SatCLIP-ViT16-L10\", \"satclip-vit16-l10.ckpt\"), device=device)\n",
    "satclip_l40 = get_satclip(hf_hub_download(\"microsoft/SatCLIP-ViT16-L40\", \"satclip-vit16-l40.ckpt\"), device=device)\n",
    "satclip_l10.eval()\n",
    "satclip_l40.eval()\n",
    "print(\"SatCLIP models loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Loading with Spatial Blocking\n",
    "\n",
    "**Key fix**: Grid-based spatial blocking to prevent train/test leakage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "def load_gpw_raster(resolution='15_min', year=2020):\n",
    "    tif_file = f\"{GPW_DIR}/gpw_v4_population_density_rev11_{year}_{resolution}.tif\"\n",
    "    if not os.path.exists(tif_file):\n",
    "        print(f\"File not found: {tif_file}\")\n",
    "        return None, None\n",
    "    \n",
    "    img = Image.open(tif_file)\n",
    "    data = np.array(img)\n",
    "    height, width = data.shape\n",
    "    \n",
    "    lon_step = 360 / width\n",
    "    lat_step = 180 / height\n",
    "    lons = np.linspace(-180 + lon_step/2, 180 - lon_step/2, width)\n",
    "    lats = np.linspace(90 - lat_step/2, -90 + lat_step/2, height)\n",
    "    \n",
    "    return data, (lons, lats)\n",
    "\n",
    "\n",
    "def sample_with_spatial_blocking(data, coords, n_samples=10000, seed=42, bounds=None,\n",
    "                                  grid_size=5.0, test_ratio=0.3):\n",
    "    \"\"\"\n",
    "    Sample with grid-based spatial blocking to prevent leakage.\n",
    "    \n",
    "    Args:\n",
    "        data: Population raster\n",
    "        coords: (lons, lats) arrays\n",
    "        n_samples: Total samples to draw\n",
    "        seed: Random seed\n",
    "        bounds: Optional (lon_min, lat_min, lon_max, lat_max)\n",
    "        grid_size: Size of grid cells in degrees (default 5.0 = ~500km at equator)\n",
    "        test_ratio: Fraction of grid cells held out for testing\n",
    "    \n",
    "    Returns:\n",
    "        coords_train, values_train, coords_test, values_test\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    lons, lats = coords\n",
    "    valid_mask = data > -1e30\n",
    "    \n",
    "    if bounds is not None:\n",
    "        lon_min, lat_min, lon_max, lat_max = bounds\n",
    "        lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "        bounds_mask = (\n",
    "            (lon_grid >= lon_min) & (lon_grid <= lon_max) &\n",
    "            (lat_grid >= lat_min) & (lat_grid <= lat_max)\n",
    "        )\n",
    "        valid_mask = valid_mask & bounds_mask\n",
    "    else:\n",
    "        lon_min, lon_max = -180, 180\n",
    "        lat_min, lat_max = -90, 90\n",
    "    \n",
    "    # Create grid cell assignments\n",
    "    n_lon_cells = int(np.ceil((lon_max - lon_min) / grid_size))\n",
    "    n_lat_cells = int(np.ceil((lat_max - lat_min) / grid_size))\n",
    "    total_cells = n_lon_cells * n_lat_cells\n",
    "    \n",
    "    # Randomly assign cells to train or test\n",
    "    cell_indices = np.arange(total_cells)\n",
    "    np.random.shuffle(cell_indices)\n",
    "    n_test_cells = int(total_cells * test_ratio)\n",
    "    test_cells = set(cell_indices[:n_test_cells])\n",
    "    \n",
    "    # Get valid points\n",
    "    valid_idx = np.where(valid_mask)\n",
    "    n_valid = len(valid_idx[0])\n",
    "    \n",
    "    if n_valid < n_samples:\n",
    "        sample_idx = np.arange(n_valid)\n",
    "    else:\n",
    "        sample_idx = np.random.choice(n_valid, n_samples, replace=False)\n",
    "    \n",
    "    row_idx = valid_idx[0][sample_idx]\n",
    "    col_idx = valid_idx[1][sample_idx]\n",
    "    \n",
    "    sample_lons = lons[col_idx]\n",
    "    sample_lats = lats[row_idx]\n",
    "    sample_values = data[row_idx, col_idx]\n",
    "    \n",
    "    # Assign each sample to train or test based on its grid cell\n",
    "    train_mask = []\n",
    "    for lon, lat in zip(sample_lons, sample_lats):\n",
    "        lon_cell = int((lon - lon_min) / grid_size)\n",
    "        lat_cell = int((lat - lat_min) / grid_size)\n",
    "        lon_cell = min(lon_cell, n_lon_cells - 1)\n",
    "        lat_cell = min(lat_cell, n_lat_cells - 1)\n",
    "        cell_id = lat_cell * n_lon_cells + lon_cell\n",
    "        train_mask.append(cell_id not in test_cells)\n",
    "    \n",
    "    train_mask = np.array(train_mask)\n",
    "    \n",
    "    coords_arr = np.stack([sample_lons, sample_lats], axis=1)\n",
    "    \n",
    "    coords_train = coords_arr[train_mask]\n",
    "    coords_test = coords_arr[~train_mask]\n",
    "    values_train = sample_values[train_mask]\n",
    "    values_test = sample_values[~train_mask]\n",
    "    \n",
    "    print(f\"  Spatial blocking: {n_lon_cells}x{n_lat_cells} grid, {len(test_cells)} test cells\")\n",
    "    print(f\"  Train: {len(coords_train)}, Test: {len(coords_test)}\")\n",
    "    \n",
    "    return coords_train, values_train, coords_test, values_test\n",
    "\n",
    "\n",
    "# Load data\n",
    "print(\"Loading population data...\")\n",
    "pop_data, pop_coords = load_gpw_raster('15_min')\n",
    "print(f\"Shape: {pop_data.shape}\")\n",
    "\n",
    "REGIONS = {\n",
    "    'Global': None,\n",
    "    'USA': (-125, 24, -66, 50),\n",
    "    'Europe': (-10, 35, 40, 70),\n",
    "    'China': (73, 18, 135, 54),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Corrected Model Architectures\n",
    "\n",
    "### Key fixes:\n",
    "1. **SIREN**: w0_initial=30 for first layer, w0=1 for subsequent (matches SatCLIP)\n",
    "2. **SpatiallyVaryingActivation**: Shared gating network across all layers\n",
    "3. **Proper initialization**: SIREN uses its own init, others use Kaiming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LEARNED ACTIVATION FUNCTION\n",
    "# =============================================================================\n",
    "\n",
    "class LearnedActivation(nn.Module):\n",
    "    \"\"\"Fourier-parameterized learned activation function.\n",
    "    \n",
    "    g(x) = scale * (sum_k a_k*sin(w_k*x) + b_k*cos(w_k*x)) + bias\n",
    "    \n",
    "    Args:\n",
    "        n_frequencies: Number of Fourier components K\n",
    "        freq_init: 'linear', 'log', or 'random'\n",
    "        learnable_freq: Whether frequencies are learnable\n",
    "        max_freq: Maximum frequency value\n",
    "    \"\"\"\n",
    "    def __init__(self, n_frequencies=25, freq_init='linear', learnable_freq=False, max_freq=10.0):\n",
    "        super().__init__()\n",
    "        self.n_frequencies = n_frequencies\n",
    "        \n",
    "        if freq_init == 'linear':\n",
    "            freqs = torch.linspace(0.1, max_freq, n_frequencies)\n",
    "        elif freq_init == 'log':\n",
    "            freqs = torch.logspace(-1, np.log10(max_freq), n_frequencies)\n",
    "        else:\n",
    "            freqs = torch.rand(n_frequencies) * max_freq\n",
    "        \n",
    "        if learnable_freq:\n",
    "            self.frequencies = nn.Parameter(freqs)\n",
    "        else:\n",
    "            self.register_buffer('frequencies', freqs)\n",
    "        \n",
    "        self.sin_coeffs = nn.Parameter(torch.randn(n_frequencies) * 0.1)\n",
    "        self.cos_coeffs = nn.Parameter(torch.randn(n_frequencies) * 0.1)\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        self.scale = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        wx = x.unsqueeze(-1) * self.frequencies\n",
    "        sin_terms = torch.sin(wx) * self.sin_coeffs\n",
    "        cos_terms = torch.cos(wx) * self.cos_coeffs\n",
    "        result = (sin_terms + cos_terms).sum(dim=-1)\n",
    "        return self.scale * result + self.bias\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# SHARED GATING NETWORK (FIX: shared across all layers)\n",
    "# =============================================================================\n",
    "\n",
    "class SharedGatingNetwork(nn.Module):\n",
    "    \"\"\"Shared gating network for spatially-varying activations.\n",
    "    \n",
    "    This network is shared across all layers, producing expert weights\n",
    "    based on location. This matches the paper description.\n",
    "    \n",
    "    Args:\n",
    "        n_experts: Number of expert activations\n",
    "        hidden_dim: Hidden dimension for gating MLP\n",
    "    \"\"\"\n",
    "    def __init__(self, n_experts=8, hidden_dim=64):\n",
    "        super().__init__()\n",
    "        self.n_experts = n_experts\n",
    "        \n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(2, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(hidden_dim, n_experts),\n",
    "        )\n",
    "    \n",
    "    def forward(self, coords):\n",
    "        \"\"\"Get expert weights for given coordinates.\n",
    "        \n",
    "        Args:\n",
    "            coords: (batch, 2) normalized coordinates in [-1, 1]\n",
    "        \n",
    "        Returns:\n",
    "            weights: (batch, n_experts) softmax weights\n",
    "        \"\"\"\n",
    "        return F.softmax(self.gate(coords), dim=-1)\n",
    "\n",
    "\n",
    "class SpatiallyVaryingActivation(nn.Module):\n",
    "    \"\"\"Mixture of expert activations with SHARED location-based gating.\n",
    "    \n",
    "    FIX: The gating network is passed in and shared across layers,\n",
    "    rather than each layer having its own gate.\n",
    "    \n",
    "    Args:\n",
    "        shared_gate: SharedGatingNetwork instance (shared across layers)\n",
    "        n_experts: Number of expert activation functions\n",
    "        n_frequencies: Frequencies per expert activation\n",
    "    \"\"\"\n",
    "    def __init__(self, shared_gate, n_experts=8, n_frequencies=25):\n",
    "        super().__init__()\n",
    "        self.shared_gate = shared_gate\n",
    "        self.n_experts = n_experts\n",
    "        \n",
    "        # Each layer has its own expert activations, but shares the gate\n",
    "        self.experts = nn.ModuleList([\n",
    "            LearnedActivation(n_frequencies=n_frequencies)\n",
    "            for _ in range(n_experts)\n",
    "        ])\n",
    "    \n",
    "    def forward(self, x, coords):\n",
    "        \"\"\"Apply spatially-varying activation.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch, features)\n",
    "            coords: Normalized coordinates (batch, 2)\n",
    "        \n",
    "        Returns:\n",
    "            Activated tensor, same shape as x\n",
    "        \"\"\"\n",
    "        # Get gating weights from SHARED gate\n",
    "        weights = self.shared_gate(coords)  # (batch, n_experts)\n",
    "        \n",
    "        # Apply each expert\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=-1)\n",
    "        \n",
    "        # Weighted combination\n",
    "        weights = weights.unsqueeze(1)  # (batch, 1, n_experts)\n",
    "        result = (expert_outputs * weights).sum(dim=-1)\n",
    "        \n",
    "        return result\n",
    "\n",
    "\n",
    "# =============================================================================\n",
    "# CORRECTED SIREN (matches SatCLIP: w0_initial=30, w0=1)\n",
    "# =============================================================================\n",
    "\n",
    "class Sine(nn.Module):\n",
    "    \"\"\"Sine activation with configurable omega.\"\"\"\n",
    "    def __init__(self, w0=1.0):\n",
    "        super().__init__()\n",
    "        self.w0 = w0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.w0 * x)\n",
    "\n",
    "\n",
    "class SirenLayer(nn.Module):\n",
    "    \"\"\"Single SIREN layer with proper initialization.\n",
    "    \n",
    "    Matches SatCLIP's implementation in location_encoder.py.\n",
    "    \n",
    "    Args:\n",
    "        dim_in: Input dimension\n",
    "        dim_out: Output dimension\n",
    "        w0: Frequency for sine activation\n",
    "        is_first: Whether this is the first layer (uses different init)\n",
    "        c: Constant for initialization (default 6.0)\n",
    "    \"\"\"\n",
    "    def __init__(self, dim_in, dim_out, w0=1.0, is_first=False, c=6.0):\n",
    "        super().__init__()\n",
    "        self.dim_in = dim_in\n",
    "        self.is_first = is_first\n",
    "        self.w0 = w0\n",
    "        \n",
    "        self.linear = nn.Linear(dim_in, dim_out)\n",
    "        self.activation = Sine(w0)\n",
    "        \n",
    "        # SIREN initialization\n",
    "        self._init_weights(c)\n",
    "    \n",
    "    def _init_weights(self, c):\n",
    "        with torch.no_grad():\n",
    "            if self.is_first:\n",
    "                # First layer: uniform(-1/dim_in, 1/dim_in)\n",
    "                bound = 1.0 / self.dim_in\n",
    "            else:\n",
    "                # Subsequent layers: uniform(-sqrt(c/dim_in)/w0, sqrt(c/dim_in)/w0)\n",
    "                bound = math.sqrt(c / self.dim_in) / self.w0\n",
    "            \n",
    "            self.linear.weight.uniform_(-bound, bound)\n",
    "            if self.linear.bias is not None:\n",
    "                self.linear.bias.uniform_(-bound, bound)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.activation(self.linear(x))\n",
    "\n",
    "\n",
    "print(\"Model components defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# LOCATION ENCODERS\n",
    "# =============================================================================\n",
    "\n",
    "class LocationEncoderReLU(nn.Module):\n",
    "    \"\"\"Simple ReLU-based location encoder.\"\"\"\n",
    "    def __init__(self, input_dim=2, hidden_dim=256, output_dim=256, n_layers=3):\n",
    "        super().__init__()\n",
    "        \n",
    "        dims = [input_dim] + [hidden_dim] * n_layers + [output_dim]\n",
    "        layers = []\n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "            if i < len(dims) - 2:  # No activation after last layer\n",
    "                layers.append(nn.ReLU())\n",
    "        \n",
    "        self.net = nn.Sequential(*layers)\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.kaiming_normal_(m.weight)\n",
    "                nn.init.zeros_(m.bias)\n",
    "    \n",
    "    def forward(self, coords):\n",
    "        x = coords.clone()\n",
    "        x[:, 0] = x[:, 0] / 180.0  # Normalize lon to [-1, 1]\n",
    "        x[:, 1] = x[:, 1] / 90.0   # Normalize lat to [-1, 1]\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "class LocationEncoderSIREN(nn.Module):\n",
    "    \"\"\"SIREN-based location encoder matching SatCLIP configuration.\n",
    "    \n",
    "    FIX: Uses w0_initial=30 for first layer, w0=1 for subsequent layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2, hidden_dim=256, output_dim=256, n_layers=3,\n",
    "                 w0_initial=30.0, w0=1.0):\n",
    "        super().__init__()\n",
    "        \n",
    "        layers = []\n",
    "        for i in range(n_layers):\n",
    "            is_first = (i == 0)\n",
    "            layer_w0 = w0_initial if is_first else w0\n",
    "            layer_dim_in = input_dim if is_first else hidden_dim\n",
    "            \n",
    "            layers.append(SirenLayer(\n",
    "                dim_in=layer_dim_in,\n",
    "                dim_out=hidden_dim,\n",
    "                w0=layer_w0,\n",
    "                is_first=is_first\n",
    "            ))\n",
    "        \n",
    "        # Final layer (no activation, but still SIREN init)\n",
    "        self.layers = nn.ModuleList(layers)\n",
    "        self.final = nn.Linear(hidden_dim, output_dim)\n",
    "        \n",
    "        # Init final layer\n",
    "        with torch.no_grad():\n",
    "            bound = math.sqrt(6.0 / hidden_dim) / w0\n",
    "            self.final.weight.uniform_(-bound, bound)\n",
    "            self.final.bias.uniform_(-bound, bound)\n",
    "    \n",
    "    def forward(self, coords):\n",
    "        x = coords.clone()\n",
    "        x[:, 0] = x[:, 0] / 180.0\n",
    "        x[:, 1] = x[:, 1] / 90.0\n",
    "        \n",
    "        for layer in self.layers:\n",
    "            x = layer(x)\n",
    "        \n",
    "        return self.final(x)\n",
    "\n",
    "\n",
    "class LocationEncoderLearned(nn.Module):\n",
    "    \"\"\"Location encoder with learned Fourier activations.\"\"\"\n",
    "    def __init__(self, input_dim=2, hidden_dim=256, output_dim=256, n_layers=3,\n",
    "                 n_frequencies=25):\n",
    "        super().__init__()\n",
    "        \n",
    "        dims = [input_dim] + [hidden_dim] * n_layers + [output_dim]\n",
    "        self.linears = nn.ModuleList([nn.Linear(dims[i], dims[i+1]) for i in range(len(dims)-1)])\n",
    "        self.activations = nn.ModuleList([LearnedActivation(n_frequencies=n_frequencies) for _ in range(n_layers)])\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for linear in self.linears:\n",
    "            nn.init.kaiming_normal_(linear.weight)\n",
    "            nn.init.zeros_(linear.bias)\n",
    "    \n",
    "    def forward(self, coords):\n",
    "        x = coords.clone()\n",
    "        x[:, 0] = x[:, 0] / 180.0\n",
    "        x[:, 1] = x[:, 1] / 90.0\n",
    "        \n",
    "        for linear, act in zip(self.linears[:-1], self.activations):\n",
    "            x = act(linear(x))\n",
    "        x = self.linears[-1](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class LocationEncoderSpatial(nn.Module):\n",
    "    \"\"\"Location encoder with spatially-varying activations.\n",
    "    \n",
    "    FIX: Uses a SHARED gating network across all layers.\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2, hidden_dim=256, output_dim=256, n_layers=3,\n",
    "                 n_experts=8, n_frequencies=25, gate_hidden=64):\n",
    "        super().__init__()\n",
    "        \n",
    "        # SHARED gating network (FIX: one gate for all layers)\n",
    "        self.shared_gate = SharedGatingNetwork(n_experts=n_experts, hidden_dim=gate_hidden)\n",
    "        \n",
    "        dims = [input_dim] + [hidden_dim] * n_layers + [output_dim]\n",
    "        self.linears = nn.ModuleList([nn.Linear(dims[i], dims[i+1]) for i in range(len(dims)-1)])\n",
    "        \n",
    "        # Each layer has its own experts, but they share the gate\n",
    "        self.activations = nn.ModuleList([\n",
    "            SpatiallyVaryingActivation(self.shared_gate, n_experts=n_experts, n_frequencies=n_frequencies)\n",
    "            for _ in range(n_layers)\n",
    "        ])\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for linear in self.linears:\n",
    "            nn.init.kaiming_normal_(linear.weight)\n",
    "            nn.init.zeros_(linear.bias)\n",
    "    \n",
    "    def forward(self, coords):\n",
    "        x = coords.clone()\n",
    "        x[:, 0] = x[:, 0] / 180.0\n",
    "        x[:, 1] = x[:, 1] / 90.0\n",
    "        \n",
    "        norm_coords = x.clone()  # For gating\n",
    "        \n",
    "        for linear, act in zip(self.linears[:-1], self.activations):\n",
    "            x = act(linear(x), norm_coords)\n",
    "        x = self.linears[-1](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test architectures\n",
    "print(\"\\nArchitecture Parameter Counts:\")\n",
    "print(\"-\" * 50)\n",
    "for name, enc_cls in [('ReLU', LocationEncoderReLU), \n",
    "                       ('SIREN', LocationEncoderSIREN),\n",
    "                       ('Learned', LocationEncoderLearned),\n",
    "                       ('Spatial', LocationEncoderSpatial)]:\n",
    "    enc = enc_cls()\n",
    "    n_params = sum(p.numel() for p in enc.parameters())\n",
    "    print(f\"  {name:10s}: {n_params:>10,} params\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =============================================================================\n",
    "# HYBRID ENCODER (SH features + custom activations)\n",
    "# =============================================================================\n",
    "\n",
    "class HybridEncoder(nn.Module):\n",
    "    \"\"\"Spherical harmonics input + configurable activations.\n",
    "    \n",
    "    FIX: Uses SIREN-specific initialization when activation='siren'.\n",
    "    \n",
    "    Args:\n",
    "        sh_model: SatCLIP location encoder (provides .posenc)\n",
    "        hidden_dim: Hidden layer dimension\n",
    "        output_dim: Output embedding dimension\n",
    "        n_layers: Number of hidden layers\n",
    "        activation: 'relu', 'siren', or 'learned'\n",
    "        n_frequencies: For learned activations\n",
    "        w0_initial: For SIREN first layer\n",
    "        w0: For SIREN subsequent layers\n",
    "    \"\"\"\n",
    "    def __init__(self, sh_model, hidden_dim=256, output_dim=256, n_layers=3,\n",
    "                 activation='learned', n_frequencies=25, w0_initial=30.0, w0=1.0):\n",
    "        super().__init__()\n",
    "        self.sh_model = sh_model\n",
    "        self.activation_type = activation\n",
    "        \n",
    "        # Freeze SH\n",
    "        for param in self.sh_model.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        # Get SH output dim\n",
    "        with torch.no_grad():\n",
    "            test_coord = torch.tensor([[0.0, 0.0]]).double().to(next(sh_model.parameters()).device)\n",
    "            sh_out = sh_model.posenc(test_coord)\n",
    "            sh_dim = sh_out.shape[-1]\n",
    "        \n",
    "        self.sh_dim = sh_dim\n",
    "        print(f\"  SH dim: {sh_dim}\")\n",
    "        \n",
    "        # Build network based on activation type\n",
    "        if activation == 'siren':\n",
    "            # Use proper SIREN layers\n",
    "            layers = []\n",
    "            for i in range(n_layers):\n",
    "                is_first = (i == 0)\n",
    "                layer_w0 = w0_initial if is_first else w0\n",
    "                layer_dim_in = sh_dim if is_first else hidden_dim\n",
    "                layers.append(SirenLayer(layer_dim_in, hidden_dim, w0=layer_w0, is_first=is_first))\n",
    "            \n",
    "            self.layers = nn.ModuleList(layers)\n",
    "            self.final = nn.Linear(hidden_dim, output_dim)\n",
    "            \n",
    "            # SIREN init for final layer\n",
    "            with torch.no_grad():\n",
    "                bound = math.sqrt(6.0 / hidden_dim) / w0\n",
    "                self.final.weight.uniform_(-bound, bound)\n",
    "                self.final.bias.uniform_(-bound, bound)\n",
    "            \n",
    "            self.use_siren = True\n",
    "        else:\n",
    "            # Standard Linear + activation\n",
    "            dims = [sh_dim] + [hidden_dim] * n_layers + [output_dim]\n",
    "            self.linears = nn.ModuleList([nn.Linear(dims[i], dims[i+1]) for i in range(len(dims)-1)])\n",
    "            \n",
    "            if activation == 'learned':\n",
    "                self.activations = nn.ModuleList([LearnedActivation(n_frequencies=n_frequencies) for _ in range(n_layers)])\n",
    "            else:  # relu\n",
    "                self.activations = nn.ModuleList([nn.ReLU() for _ in range(n_layers)])\n",
    "            \n",
    "            # Kaiming init for non-SIREN\n",
    "            for linear in self.linears:\n",
    "                nn.init.kaiming_normal_(linear.weight)\n",
    "                nn.init.zeros_(linear.bias)\n",
    "            \n",
    "            self.use_siren = False\n",
    "    \n",
    "    def forward(self, coords):\n",
    "        # Get SH features\n",
    "        with torch.no_grad():\n",
    "            x = self.sh_model.posenc(coords.double()).float()\n",
    "        \n",
    "        if self.use_siren:\n",
    "            for layer in self.layers:\n",
    "                x = layer(x)\n",
    "            x = self.final(x)\n",
    "        else:\n",
    "            for linear, act in zip(self.linears[:-1], self.activations):\n",
    "                x = act(linear(x))\n",
    "            x = self.linears[-1](x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "\n",
    "print(\"HybridEncoder defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_satclip_embeddings(model, coords, device, batch_size=512):\n",
    "    \"\"\"Extract SatCLIP embeddings for coordinates.\"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    coords_tensor = torch.tensor(coords, dtype=torch.float64)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(coords), batch_size):\n",
    "            batch = coords_tensor[i:i+batch_size].to(device)\n",
    "            emb = model(batch).cpu().numpy()\n",
    "            embeddings.append(emb)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "\n",
    "def evaluate_sklearn(X_train, y_train, X_test, y_test, alpha=1.0):\n",
    "    \"\"\"Evaluate using Ridge regression (fair SatCLIP comparison).\"\"\"\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "def evaluate_neural(encoder, coords_train, y_train, coords_test, y_test,\n",
    "                   epochs=100, lr=1e-3, batch_size=256, device='cuda', verbose=True):\n",
    "    \"\"\"Train encoder end-to-end and evaluate.\"\"\"\n",
    "    \n",
    "    class Predictor(nn.Module):\n",
    "        def __init__(self, encoder):\n",
    "            super().__init__()\n",
    "            self.encoder = encoder\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(256, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 1)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.head(self.encoder(x)).squeeze(-1)\n",
    "    \n",
    "    model = Predictor(encoder).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    train_coords = torch.tensor(coords_train, dtype=torch.float32)\n",
    "    train_y = torch.tensor(np.log1p(y_train), dtype=torch.float32)\n",
    "    test_coords = torch.tensor(coords_test, dtype=torch.float32)\n",
    "    test_y = torch.tensor(np.log1p(y_test), dtype=torch.float32)\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(train_coords, train_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    best_r2 = -float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for coords_batch, y_batch in train_loader:\n",
    "            coords_batch, y_batch = coords_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(coords_batch), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(test_coords.to(device)).cpu().numpy()\n",
    "        r2 = r2_score(test_y.numpy(), preds)\n",
    "        best_r2 = max(best_r2, r2)\n",
    "        \n",
    "        if verbose and (epoch + 1) % 25 == 0:\n",
    "            print(f\"    Epoch {epoch+1}/{epochs}: R²={r2:.4f}\")\n",
    "    \n",
    "    return best_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT: Corrected Learned Activations vs SatCLIP\")\n",
    "print(\"=\"*80)\n",
    "print(\"\\nFixes applied:\")\n",
    "print(\"  1. Spatial blocking for train/test splits\")\n",
    "print(\"  2. Ridge regression for SatCLIP baseline\")\n",
    "print(\"  3. Correct SIREN config (w0_initial=30, w0=1)\")\n",
    "print(\"  4. Shared gating network for spatial activations\")\n",
    "\n",
    "N_SAMPLES = 20000  # More samples to compensate for blocking\n",
    "EPOCHS = 100\n",
    "GRID_SIZE = 5.0  # 5 degree grid cells\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for region_name, bounds in REGIONS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Region: {region_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    # Sample with spatial blocking\n",
    "    coords_train, values_train, coords_test, values_test = sample_with_spatial_blocking(\n",
    "        pop_data, pop_coords, n_samples=N_SAMPLES, bounds=bounds,\n",
    "        grid_size=GRID_SIZE, test_ratio=0.3\n",
    "    )\n",
    "    \n",
    "    if len(coords_train) < 500 or len(coords_test) < 200:\n",
    "        print(\"Skipping - too few samples\")\n",
    "        continue\n",
    "    \n",
    "    y_train_log = np.log1p(values_train)\n",
    "    y_test_log = np.log1p(values_test)\n",
    "    \n",
    "    # =================================================================\n",
    "    # BASELINES: SatCLIP with Ridge (fair comparison)\n",
    "    # =================================================================\n",
    "    print(\"\\n--- SatCLIP Baselines (Ridge) ---\")\n",
    "    \n",
    "    print(\"  SatCLIP L=10...\")\n",
    "    emb_train = get_satclip_embeddings(satclip_l10, coords_train, device)\n",
    "    emb_test = get_satclip_embeddings(satclip_l10, coords_test, device)\n",
    "    r2_l10 = evaluate_sklearn(emb_train, y_train_log, emb_test, y_test_log)\n",
    "    print(f\"    R²: {r2_l10:.4f}\")\n",
    "    all_results.append({'region': region_name, 'model': 'SatCLIP L=10', 'r2': r2_l10, 'type': 'baseline'})\n",
    "    \n",
    "    print(\"  SatCLIP L=40...\")\n",
    "    emb_train = get_satclip_embeddings(satclip_l40, coords_train, device)\n",
    "    emb_test = get_satclip_embeddings(satclip_l40, coords_test, device)\n",
    "    r2_l40 = evaluate_sklearn(emb_train, y_train_log, emb_test, y_test_log)\n",
    "    print(f\"    R²: {r2_l40:.4f}\")\n",
    "    all_results.append({'region': region_name, 'model': 'SatCLIP L=40', 'r2': r2_l40, 'type': 'baseline'})\n",
    "    \n",
    "    # =================================================================\n",
    "    # DIRECT ENCODERS (our approaches)\n",
    "    # =================================================================\n",
    "    print(\"\\n--- Direct Encoders (end-to-end) ---\")\n",
    "    \n",
    "    print(\"  Direct + ReLU...\")\n",
    "    encoder = LocationEncoderReLU()\n",
    "    r2 = evaluate_neural(encoder, coords_train, values_train, coords_test, values_test, epochs=EPOCHS, device=device)\n",
    "    print(f\"    Best R²: {r2:.4f}\")\n",
    "    all_results.append({'region': region_name, 'model': 'Direct + ReLU', 'r2': r2, 'type': 'direct'})\n",
    "    \n",
    "    print(\"  Direct + SIREN (corrected)...\")\n",
    "    encoder = LocationEncoderSIREN(w0_initial=30.0, w0=1.0)  # Matches SatCLIP\n",
    "    r2 = evaluate_neural(encoder, coords_train, values_train, coords_test, values_test, epochs=EPOCHS, device=device)\n",
    "    print(f\"    Best R²: {r2:.4f}\")\n",
    "    all_results.append({'region': region_name, 'model': 'Direct + SIREN', 'r2': r2, 'type': 'direct'})\n",
    "    \n",
    "    print(\"  Direct + Learned...\")\n",
    "    encoder = LocationEncoderLearned(n_frequencies=25)\n",
    "    r2 = evaluate_neural(encoder, coords_train, values_train, coords_test, values_test, epochs=EPOCHS, device=device)\n",
    "    print(f\"    Best R²: {r2:.4f}\")\n",
    "    all_results.append({'region': region_name, 'model': 'Direct + Learned', 'r2': r2, 'type': 'direct'})\n",
    "    \n",
    "    print(\"  Direct + Spatial (shared gate)...\")\n",
    "    encoder = LocationEncoderSpatial(n_experts=8, n_frequencies=25)\n",
    "    r2 = evaluate_neural(encoder, coords_train, values_train, coords_test, values_test, epochs=EPOCHS, device=device)\n",
    "    print(f\"    Best R²: {r2:.4f}\")\n",
    "    all_results.append({'region': region_name, 'model': 'Direct + Spatial', 'r2': r2, 'type': 'direct'})\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(f\"\\n\\nTotal experiments: {len(results_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# HYBRID EXPERIMENTS (SH + learned activations)\n",
    "# =================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYBRID EXPERIMENTS: SH features + different activations\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "hybrid_results = []\n",
    "\n",
    "for region_name, bounds in REGIONS.items():\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"Region: {region_name}\")\n",
    "    print(f\"{'='*60}\")\n",
    "    \n",
    "    coords_train, values_train, coords_test, values_test = sample_with_spatial_blocking(\n",
    "        pop_data, pop_coords, n_samples=N_SAMPLES, bounds=bounds,\n",
    "        grid_size=GRID_SIZE, test_ratio=0.3, seed=123  # Different seed for variety\n",
    "    )\n",
    "    \n",
    "    if len(coords_train) < 500:\n",
    "        continue\n",
    "    \n",
    "    # Test hybrid with L=10\n",
    "    for act_type in ['relu', 'siren', 'learned']:\n",
    "        print(f\"\\n  SH(L=10) + {act_type}...\")\n",
    "        encoder = HybridEncoder(satclip_l10, activation=act_type, n_frequencies=25)\n",
    "        r2 = evaluate_neural(encoder, coords_train, values_train, coords_test, values_test,\n",
    "                            epochs=EPOCHS, device=device)\n",
    "        print(f\"    Best R²: {r2:.4f}\")\n",
    "        hybrid_results.append({'region': region_name, 'model': f'SH(L=10)+{act_type}', 'r2': r2})\n",
    "    \n",
    "    # Test hybrid with L=40\n",
    "    for act_type in ['relu', 'learned']:\n",
    "        print(f\"\\n  SH(L=40) + {act_type}...\")\n",
    "        encoder = HybridEncoder(satclip_l40, activation=act_type, n_frequencies=25)\n",
    "        r2 = evaluate_neural(encoder, coords_train, values_train, coords_test, values_test,\n",
    "                            epochs=EPOCHS, device=device)\n",
    "        print(f\"    Best R²: {r2:.4f}\")\n",
    "        hybrid_results.append({'region': region_name, 'model': f'SH(L=40)+{act_type}', 'r2': r2})\n",
    "\n",
    "hybrid_df = pd.DataFrame(hybrid_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# COMBINED RESULTS\n",
    "# =================================================================\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMBINED RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Main results\n",
    "print(\"\\n--- Direct Approaches ---\")\n",
    "pivot1 = results_df.pivot(index='model', columns='region', values='r2')\n",
    "col_order = ['Global', 'USA', 'Europe', 'China']\n",
    "pivot1 = pivot1[[c for c in col_order if c in pivot1.columns]]\n",
    "print(pivot1.round(3).to_string())\n",
    "\n",
    "# Hybrid results\n",
    "if len(hybrid_df) > 0:\n",
    "    print(\"\\n--- Hybrid Approaches ---\")\n",
    "    pivot2 = hybrid_df.pivot(index='model', columns='region', values='r2')\n",
    "    pivot2 = pivot2[[c for c in col_order if c in pivot2.columns]]\n",
    "    print(pivot2.round(3).to_string())\n",
    "\n",
    "# Save\n",
    "all_combined = pd.concat([results_df, hybrid_df])\n",
    "all_combined.to_csv('learned_activations_v3_results.csv', index=False)\n",
    "print(\"\\nResults saved to learned_activations_v3_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# VISUALIZATION\n",
    "# =================================================================\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Direct approaches comparison\n",
    "ax = axes[0]\n",
    "pivot1.T.plot(kind='bar', ax=ax)\n",
    "ax.set_ylabel('R² Score')\n",
    "ax.set_xlabel('Region')\n",
    "ax.set_title('Direct Approaches\\n(with spatial blocking)')\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Hybrid approaches\n",
    "if len(hybrid_df) > 0:\n",
    "    ax = axes[1]\n",
    "    pivot2.T.plot(kind='bar', ax=ax)\n",
    "    ax.set_ylabel('R² Score')\n",
    "    ax.set_xlabel('Region')\n",
    "    ax.set_title('Hybrid Approaches\\n(SH features + activations)')\n",
    "    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('learned_activations_v3_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =================================================================\n",
    "# KEY FINDINGS\n",
    "# =================================================================\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. SatCLIP BASELINE (with fair Ridge regression):\")\n",
    "for region in ['Global', 'USA', 'Europe', 'China']:\n",
    "    l10 = results_df[(results_df['model'] == 'SatCLIP L=10') & (results_df['region'] == region)]['r2'].values\n",
    "    l40 = results_df[(results_df['model'] == 'SatCLIP L=40') & (results_df['region'] == region)]['r2'].values\n",
    "    if len(l10) > 0 and len(l40) > 0:\n",
    "        print(f\"  {region}: L=10={l10[0]:.3f}, L=40={l40[0]:.3f}, diff={l40[0]-l10[0]:+.3f}\")\n",
    "\n",
    "print(\"\\n2. LEARNED ACTIVATIONS vs SatCLIP:\")\n",
    "for region in ['Global', 'USA', 'Europe', 'China']:\n",
    "    l10 = results_df[(results_df['model'] == 'SatCLIP L=10') & (results_df['region'] == region)]['r2'].values\n",
    "    learned = results_df[(results_df['model'] == 'Direct + Learned') & (results_df['region'] == region)]['r2'].values\n",
    "    if len(l10) > 0 and len(learned) > 0:\n",
    "        print(f\"  {region}: Learned={learned[0]:.3f} vs L=10={l10[0]:.3f} ({learned[0]-l10[0]:+.3f})\")\n",
    "\n",
    "print(\"\\n3. SPATIAL ACTIVATIONS (with shared gate):\")\n",
    "for region in ['Global', 'USA', 'Europe', 'China']:\n",
    "    learned = results_df[(results_df['model'] == 'Direct + Learned') & (results_df['region'] == region)]['r2'].values\n",
    "    spatial = results_df[(results_df['model'] == 'Direct + Spatial') & (results_df['region'] == region)]['r2'].values\n",
    "    if len(learned) > 0 and len(spatial) > 0:\n",
    "        print(f\"  {region}: Spatial={spatial[0]:.3f} vs Learned={learned[0]:.3f} ({spatial[0]-learned[0]:+.3f})\")\n",
    "\n",
    "if len(hybrid_df) > 0:\n",
    "    print(\"\\n4. HYBRID APPROACHES (best per region):\")\n",
    "    for region in ['Global', 'USA', 'Europe', 'China']:\n",
    "        region_data = hybrid_df[hybrid_df['region'] == region]\n",
    "        if len(region_data) > 0:\n",
    "            best = region_data.loc[region_data['r2'].idxmax()]\n",
    "            print(f\"  {region}: {best['model']} = {best['r2']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save summary\n",
    "import json\n",
    "\n",
    "summary = {\n",
    "    'fixes_applied': [\n",
    "        'Spatial blocking for train/test splits (5 degree grid)',\n",
    "        'Ridge regression for SatCLIP baseline (not MLP)',\n",
    "        'Correct SIREN config (w0_initial=30, w0=1)',\n",
    "        'Shared gating network across layers for spatial activations',\n",
    "        'SIREN-specific initialization in HybridEncoder'\n",
    "    ],\n",
    "    'direct_results': results_df.to_dict('records'),\n",
    "    'hybrid_results': hybrid_df.to_dict('records') if len(hybrid_df) > 0 else [],\n",
    "}\n",
    "\n",
    "with open('learned_activations_v3_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"Summary saved to learned_activations_v3_summary.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
