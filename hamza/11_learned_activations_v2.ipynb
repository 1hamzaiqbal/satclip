{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Learned Activations v2: Fair Comparisons & Hybrid Approaches\n",
    "\n",
    "## Issues from Notebook 10\n",
    "1. **SatCLIP comparison was broken** - MLP head overfitted on frozen embeddings\n",
    "2. **Unfair param count** - SatCLIP had 33K params, Direct had 231K\n",
    "3. **SIREN initialization wrong** - ω₀=30 too high for [-1,1] coordinates\n",
    "\n",
    "## This notebook fixes these issues:\n",
    "1. **Fair SatCLIP baseline** - Use sklearn (Ridge) like notebook 07\n",
    "2. **Hybrid approach** - SH features + learned activations\n",
    "3. **Fixed SIREN** - Lower ω₀ values\n",
    "4. **Ablation study** - Number of frequencies, layers, experts\n",
    "\n",
    "## Key Questions\n",
    "1. Can learned activations match pretrained SatCLIP?\n",
    "2. Does SH + learned activations beat SH + SIREN?\n",
    "3. What's the optimal number of frequencies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    !rm -rf sample_data .config satclip gpw_data 2>/dev/null\n",
    "    !git clone https://github.com/1hamzaiqbal/satclip.git\n",
    "    !pip install lightning torchgeo huggingface_hub rasterio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and extract GPW data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "GPW_DIR = './gpw_data'\n",
    "os.makedirs(GPW_DIR, exist_ok=True)\n",
    "\n",
    "SOURCE_ZIP_PATH = '/content/drive/MyDrive/grad/learned_activations/dataverse_files.zip'\n",
    "\n",
    "print(\"Extracting GPW data...\")\n",
    "with zipfile.ZipFile(SOURCE_ZIP_PATH, 'r') as z:\n",
    "    z.extractall(GPW_DIR)\n",
    "\n",
    "# Extract 15-min resolution\n",
    "zip_path = os.path.join(GPW_DIR, 'gpw-v4-population-density-rev11_2020_15_min_tif.zip')\n",
    "if os.path.exists(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        z.extractall(GPW_DIR)\n",
    "    print(\"Extracted 15-min resolution\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    sys.path.append('./satclip/satclip')\n",
    "    GPW_DIR = './gpw_data'\n",
    "else:\n",
    "    sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'satclip'))\n",
    "    GPW_DIR = './gpw_data'\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from load import get_satclip\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load SatCLIP models\n",
    "print(\"Loading SatCLIP models...\")\n",
    "satclip_l10 = get_satclip(hf_hub_download(\"microsoft/SatCLIP-ViT16-L10\", \"satclip-vit16-l10.ckpt\"), device=device)\n",
    "satclip_l40 = get_satclip(hf_hub_download(\"microsoft/SatCLIP-ViT16-L40\", \"satclip-vit16-l40.ckpt\"), device=device)\n",
    "satclip_l10.eval()\n",
    "satclip_l40.eval()\n",
    "print(\"SatCLIP models loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Data Loading (same as notebook 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "def load_gpw_raster(resolution='15_min', year=2020):\n",
    "    tif_file = f\"{GPW_DIR}/gpw_v4_population_density_rev11_{year}_{resolution}.tif\"\n",
    "    if not os.path.exists(tif_file):\n",
    "        print(f\"File not found: {tif_file}\")\n",
    "        return None, None\n",
    "    \n",
    "    img = Image.open(tif_file)\n",
    "    data = np.array(img)\n",
    "    height, width = data.shape\n",
    "    \n",
    "    lon_step = 360 / width\n",
    "    lat_step = 180 / height\n",
    "    lons = np.linspace(-180 + lon_step/2, 180 - lon_step/2, width)\n",
    "    lats = np.linspace(90 - lat_step/2, -90 + lat_step/2, height)\n",
    "    \n",
    "    return data, (lons, lats)\n",
    "\n",
    "def sample_from_raster(data, coords, n_samples=10000, seed=42, bounds=None):\n",
    "    np.random.seed(seed)\n",
    "    lons, lats = coords\n",
    "    valid_mask = data > -1e30\n",
    "    \n",
    "    if bounds is not None:\n",
    "        lon_min, lat_min, lon_max, lat_max = bounds\n",
    "        lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "        bounds_mask = (\n",
    "            (lon_grid >= lon_min) & (lon_grid <= lon_max) &\n",
    "            (lat_grid >= lat_min) & (lat_grid <= lat_max)\n",
    "        )\n",
    "        valid_mask = valid_mask & bounds_mask\n",
    "    \n",
    "    valid_idx = np.where(valid_mask)\n",
    "    n_valid = len(valid_idx[0])\n",
    "    \n",
    "    if n_valid < n_samples:\n",
    "        sample_idx = np.arange(n_valid)\n",
    "    else:\n",
    "        sample_idx = np.random.choice(n_valid, n_samples, replace=False)\n",
    "    \n",
    "    row_idx = valid_idx[0][sample_idx]\n",
    "    col_idx = valid_idx[1][sample_idx]\n",
    "    \n",
    "    sample_lons = lons[col_idx]\n",
    "    sample_lats = lats[row_idx]\n",
    "    sample_values = data[row_idx, col_idx]\n",
    "    \n",
    "    coords_arr = np.stack([sample_lons, sample_lats], axis=1)\n",
    "    return coords_arr, sample_values\n",
    "\n",
    "# Load data\n",
    "print(\"Loading population data...\")\n",
    "pop_data, pop_coords = load_gpw_raster('15_min')\n",
    "print(f\"Shape: {pop_data.shape}\")\n",
    "\n",
    "REGIONS = {\n",
    "    'Global': None,\n",
    "    'USA': (-125, 24, -66, 50),\n",
    "    'Europe': (-10, 35, 40, 70),\n",
    "    'China': (73, 18, 135, 54),\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Fixed Model Architectures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedActivation(nn.Module):\n",
    "    \"\"\"Fourier-parameterized learned activation function.\"\"\"\n",
    "    def __init__(self, n_frequencies=25, freq_init='linear', learnable_freq=False, max_freq=10.0):\n",
    "        super().__init__()\n",
    "        self.n_frequencies = n_frequencies\n",
    "        \n",
    "        if freq_init == 'linear':\n",
    "            freqs = torch.linspace(0.1, max_freq, n_frequencies)\n",
    "        elif freq_init == 'log':\n",
    "            freqs = torch.logspace(-1, np.log10(max_freq), n_frequencies)\n",
    "        else:\n",
    "            freqs = torch.rand(n_frequencies) * max_freq\n",
    "        \n",
    "        if learnable_freq:\n",
    "            self.frequencies = nn.Parameter(freqs)\n",
    "        else:\n",
    "            self.register_buffer('frequencies', freqs)\n",
    "        \n",
    "        self.sin_coeffs = nn.Parameter(torch.randn(n_frequencies) * 0.1)\n",
    "        self.cos_coeffs = nn.Parameter(torch.randn(n_frequencies) * 0.1)\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        self.scale = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        wx = x.unsqueeze(-1) * self.frequencies\n",
    "        sin_terms = torch.sin(wx) * self.sin_coeffs\n",
    "        cos_terms = torch.cos(wx) * self.cos_coeffs\n",
    "        result = (sin_terms + cos_terms).sum(dim=-1)\n",
    "        return self.scale * result + self.bias\n",
    "\n",
    "\n",
    "class SineActivation(nn.Module):\n",
    "    \"\"\"Sine activation with configurable omega.\"\"\"\n",
    "    def __init__(self, omega_0=30.0):\n",
    "        super().__init__()\n",
    "        self.omega_0 = omega_0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.omega_0 * x)\n",
    "\n",
    "\n",
    "class LocationEncoder(nn.Module):\n",
    "    \"\"\"Location encoder with configurable activations.\"\"\"\n",
    "    def __init__(self, input_dim=2, hidden_dim=256, output_dim=256, n_layers=3,\n",
    "                 activation='relu', n_frequencies=25, omega_0=30.0):\n",
    "        super().__init__()\n",
    "        self.activation_type = activation\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        dims = [input_dim] + [hidden_dim] * n_layers + [output_dim]\n",
    "        self.linears = nn.ModuleList([nn.Linear(dims[i], dims[i+1]) for i in range(len(dims)-1)])\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activations = nn.ModuleList([nn.ReLU() for _ in range(n_layers)])\n",
    "        elif activation == 'siren':\n",
    "            self.activations = nn.ModuleList([SineActivation(omega_0=omega_0) for _ in range(n_layers)])\n",
    "        elif activation == 'learned':\n",
    "            self.activations = nn.ModuleList([LearnedActivation(n_frequencies=n_frequencies) for _ in range(n_layers)])\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for i, linear in enumerate(self.linears):\n",
    "            if self.activation_type == 'siren':\n",
    "                omega_0 = self.activations[0].omega_0 if i > 0 else 1.0\n",
    "                bound = np.sqrt(6/linear.in_features) / omega_0\n",
    "                nn.init.uniform_(linear.weight, -bound, bound)\n",
    "            else:\n",
    "                nn.init.kaiming_normal_(linear.weight)\n",
    "            nn.init.zeros_(linear.bias)\n",
    "    \n",
    "    def forward(self, coords):\n",
    "        x = coords.clone()\n",
    "        x[:, 0] = x[:, 0] / 180.0\n",
    "        x[:, 1] = x[:, 1] / 90.0\n",
    "        \n",
    "        for linear, act in zip(self.linears[:-1], self.activations):\n",
    "            x = act(linear(x))\n",
    "        x = self.linears[-1](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class HybridEncoder(nn.Module):\n",
    "    \"\"\"Spherical harmonics input + learned activations.\n",
    "    \n",
    "    This tests: can learned activations improve on SIREN\n",
    "    when using the same SH positional encoding?\n",
    "    \"\"\"\n",
    "    def __init__(self, sh_model, hidden_dim=256, output_dim=256, n_layers=3,\n",
    "                 activation='learned', n_frequencies=25, freeze_sh=True):\n",
    "        super().__init__()\n",
    "        self.sh_model = sh_model\n",
    "        self.activation_type = activation\n",
    "        \n",
    "        if freeze_sh:\n",
    "            for param in self.sh_model.parameters():\n",
    "                param.requires_grad = False\n",
    "        \n",
    "        # Get SH output dim (L=10: 100, L=40: 1600)\n",
    "        with torch.no_grad():\n",
    "            test_coord = torch.tensor([[0.0, 0.0]]).double().to(next(sh_model.parameters()).device)\n",
    "            sh_out = sh_model.spherical_harmonics(test_coord)\n",
    "            sh_dim = sh_out.shape[-1]\n",
    "        \n",
    "        self.sh_dim = sh_dim\n",
    "        print(f\"  SH dim: {sh_dim}\")\n",
    "        \n",
    "        # Build MLP with learned activations\n",
    "        dims = [sh_dim] + [hidden_dim] * n_layers + [output_dim]\n",
    "        self.linears = nn.ModuleList([nn.Linear(dims[i], dims[i+1]) for i in range(len(dims)-1)])\n",
    "        \n",
    "        if activation == 'learned':\n",
    "            self.activations = nn.ModuleList([LearnedActivation(n_frequencies=n_frequencies) for _ in range(n_layers)])\n",
    "        elif activation == 'relu':\n",
    "            self.activations = nn.ModuleList([nn.ReLU() for _ in range(n_layers)])\n",
    "        elif activation == 'siren':\n",
    "            self.activations = nn.ModuleList([SineActivation(omega_0=30.0) for _ in range(n_layers)])\n",
    "        \n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for linear in self.linears:\n",
    "            nn.init.kaiming_normal_(linear.weight)\n",
    "            nn.init.zeros_(linear.bias)\n",
    "    \n",
    "    def forward(self, coords):\n",
    "        # Get SH features\n",
    "        with torch.no_grad():\n",
    "            sh_features = self.sh_model.spherical_harmonics(coords.double()).float()\n",
    "        \n",
    "        x = sh_features\n",
    "        for linear, act in zip(self.linears[:-1], self.activations):\n",
    "            x = act(linear(x))\n",
    "        x = self.linears[-1](x)\n",
    "        return x\n",
    "\n",
    "\n",
    "# Test\n",
    "print(\"Testing architectures:\")\n",
    "for act in ['relu', 'siren', 'learned']:\n",
    "    enc = LocationEncoder(activation=act, omega_0=1.0)  # Fixed: lower omega for direct coords\n",
    "    print(f\"  Direct + {act}: {sum(p.numel() for p in enc.parameters()):,} params\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Fair Evaluation Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_satclip_embeddings(model, coords, device, batch_size=512):\n",
    "    \"\"\"Extract SatCLIP embeddings for coordinates.\"\"\"\n",
    "    model.eval()\n",
    "    embeddings = []\n",
    "    \n",
    "    coords_tensor = torch.tensor(coords, dtype=torch.float64)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(coords), batch_size):\n",
    "            batch = coords_tensor[i:i+batch_size].to(device)\n",
    "            emb = model(batch).cpu().numpy()\n",
    "            embeddings.append(emb)\n",
    "    \n",
    "    return np.vstack(embeddings)\n",
    "\n",
    "\n",
    "def evaluate_sklearn(X_train, y_train, X_test, y_test, alpha=1.0):\n",
    "    \"\"\"Evaluate using Ridge regression (fair comparison).\"\"\"\n",
    "    model = Ridge(alpha=alpha)\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    return r2_score(y_test, y_pred)\n",
    "\n",
    "\n",
    "def evaluate_neural(encoder, coords_train, y_train, coords_test, y_test, \n",
    "                   epochs=100, lr=1e-3, batch_size=256, device='cuda'):\n",
    "    \"\"\"Train encoder end-to-end and evaluate.\"\"\"\n",
    "    \n",
    "    # Create prediction head\n",
    "    class Predictor(nn.Module):\n",
    "        def __init__(self, encoder):\n",
    "            super().__init__()\n",
    "            self.encoder = encoder\n",
    "            self.head = nn.Sequential(\n",
    "                nn.Linear(256, 128),\n",
    "                nn.ReLU(),\n",
    "                nn.Linear(128, 1)\n",
    "            )\n",
    "        \n",
    "        def forward(self, x):\n",
    "            return self.head(self.encoder(x)).squeeze(-1)\n",
    "    \n",
    "    model = Predictor(encoder).to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    # Prepare data\n",
    "    train_coords = torch.tensor(coords_train, dtype=torch.float32)\n",
    "    train_y = torch.tensor(np.log1p(y_train), dtype=torch.float32)\n",
    "    test_coords = torch.tensor(coords_test, dtype=torch.float32)\n",
    "    test_y = torch.tensor(np.log1p(y_test), dtype=torch.float32)\n",
    "    \n",
    "    train_dataset = torch.utils.data.TensorDataset(train_coords, train_y)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    best_r2 = -float('inf')\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for coords_batch, y_batch in train_loader:\n",
    "            coords_batch, y_batch = coords_batch.to(device), y_batch.to(device)\n",
    "            optimizer.zero_grad()\n",
    "            loss = criterion(model(coords_batch), y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            preds = model(test_coords.to(device)).cpu().numpy()\n",
    "        r2 = r2_score(test_y.numpy(), preds)\n",
    "        best_r2 = max(best_r2, r2)\n",
    "        \n",
    "        if (epoch + 1) % 25 == 0:\n",
    "            print(f\"    Epoch {epoch+1}/{epochs}: R²={r2:.4f}\")\n",
    "    \n",
    "    return best_r2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Experiment 1: Fair SatCLIP vs Learned Activations\n",
    "\n",
    "Using sklearn Ridge regression on embeddings for a fair comparison."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT 1: Fair Comparison (sklearn on embeddings)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "N_SAMPLES = 15000\n",
    "results_exp1 = []\n",
    "\n",
    "for region_name, bounds in REGIONS.items():\n",
    "    print(f\"\\n{'─'*60}\")\n",
    "    print(f\"Region: {region_name}\")\n",
    "    print(f\"{'─'*60}\")\n",
    "    \n",
    "    # Sample data\n",
    "    coords, values = sample_from_raster(pop_data, pop_coords, n_samples=N_SAMPLES, bounds=bounds)\n",
    "    y = np.log1p(values)  # Log transform\n",
    "    \n",
    "    # Train/test split\n",
    "    np.random.seed(42)\n",
    "    idx = np.random.permutation(len(coords))\n",
    "    split = len(coords) // 2\n",
    "    train_idx, test_idx = idx[:split], idx[split:]\n",
    "    \n",
    "    coords_train, coords_test = coords[train_idx], coords[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "    \n",
    "    print(f\"Train: {len(coords_train)}, Test: {len(coords_test)}\")\n",
    "    \n",
    "    # 1. SatCLIP L=10 + Ridge\n",
    "    print(\"\\n  SatCLIP L=10 + Ridge...\")\n",
    "    emb_train = get_satclip_embeddings(satclip_l10, coords_train, device)\n",
    "    emb_test = get_satclip_embeddings(satclip_l10, coords_test, device)\n",
    "    r2_l10 = evaluate_sklearn(emb_train, y_train, emb_test, y_test)\n",
    "    print(f\"    R²: {r2_l10:.4f}\")\n",
    "    results_exp1.append({'region': region_name, 'model': 'SatCLIP L=10 (Ridge)', 'r2': r2_l10})\n",
    "    \n",
    "    # 2. SatCLIP L=40 + Ridge\n",
    "    print(\"  SatCLIP L=40 + Ridge...\")\n",
    "    emb_train = get_satclip_embeddings(satclip_l40, coords_train, device)\n",
    "    emb_test = get_satclip_embeddings(satclip_l40, coords_test, device)\n",
    "    r2_l40 = evaluate_sklearn(emb_train, y_train, emb_test, y_test)\n",
    "    print(f\"    R²: {r2_l40:.4f}\")\n",
    "    results_exp1.append({'region': region_name, 'model': 'SatCLIP L=40 (Ridge)', 'r2': r2_l40})\n",
    "    \n",
    "    # 3. Direct + ReLU (end-to-end)\n",
    "    print(\"  Direct + ReLU (end-to-end)...\")\n",
    "    encoder = LocationEncoder(activation='relu')\n",
    "    r2_relu = evaluate_neural(encoder, coords_train, values[train_idx], coords_test, values[test_idx], \n",
    "                              epochs=100, device=device)\n",
    "    print(f\"    Best R²: {r2_relu:.4f}\")\n",
    "    results_exp1.append({'region': region_name, 'model': 'Direct + ReLU', 'r2': r2_relu})\n",
    "    \n",
    "    # 4. Direct + Learned (end-to-end)\n",
    "    print(\"  Direct + Learned (end-to-end)...\")\n",
    "    encoder = LocationEncoder(activation='learned', n_frequencies=25)\n",
    "    r2_learned = evaluate_neural(encoder, coords_train, values[train_idx], coords_test, values[test_idx],\n",
    "                                 epochs=100, device=device)\n",
    "    print(f\"    Best R²: {r2_learned:.4f}\")\n",
    "    results_exp1.append({'region': region_name, 'model': 'Direct + Learned', 'r2': r2_learned})\n",
    "    \n",
    "    # 5. Direct + SIREN (fixed omega=1.0 for [-1,1] range)\n",
    "    print(\"  Direct + SIREN (ω=1.0, fixed)...\")\n",
    "    encoder = LocationEncoder(activation='siren', omega_0=1.0)\n",
    "    r2_siren = evaluate_neural(encoder, coords_train, values[train_idx], coords_test, values[test_idx],\n",
    "                               epochs=100, device=device)\n",
    "    print(f\"    Best R²: {r2_siren:.4f}\")\n",
    "    results_exp1.append({'region': region_name, 'model': 'Direct + SIREN (ω=1)', 'r2': r2_siren})\n",
    "\n",
    "# Summary\n",
    "df1 = pd.DataFrame(results_exp1)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 1 RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(df1.pivot(index='model', columns='region', values='r2').round(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Experiment 2: Hybrid Approach (SH + Learned Activations)\n",
    "\n",
    "Can we improve SatCLIP by replacing SIREN with learned activations?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT 2: Hybrid (SH input + Learned Activations)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results_exp2 = []\n",
    "\n",
    "for region_name, bounds in REGIONS.items():\n",
    "    print(f\"\\n{'─'*60}\")\n",
    "    print(f\"Region: {region_name}\")\n",
    "    print(f\"{'─'*60}\")\n",
    "    \n",
    "    # Sample data\n",
    "    coords, values = sample_from_raster(pop_data, pop_coords, n_samples=N_SAMPLES, bounds=bounds)\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    idx = np.random.permutation(len(coords))\n",
    "    split = len(coords) // 2\n",
    "    train_idx, test_idx = idx[:split], idx[split:]\n",
    "    \n",
    "    coords_train, coords_test = coords[train_idx], coords[test_idx]\n",
    "    \n",
    "    # Test hybrid approaches with L=10 SH\n",
    "    for act_type in ['relu', 'siren', 'learned']:\n",
    "        print(f\"\\n  SH(L=10) + {act_type}...\")\n",
    "        encoder = HybridEncoder(satclip_l10, activation=act_type, n_frequencies=25)\n",
    "        r2 = evaluate_neural(encoder, coords_train, values[train_idx], coords_test, values[test_idx],\n",
    "                            epochs=100, device=device)\n",
    "        print(f\"    Best R²: {r2:.4f}\")\n",
    "        results_exp2.append({'region': region_name, 'model': f'SH(L=10) + {act_type}', 'r2': r2})\n",
    "    \n",
    "    # Test with L=40 SH\n",
    "    for act_type in ['relu', 'learned']:\n",
    "        print(f\"\\n  SH(L=40) + {act_type}...\")\n",
    "        encoder = HybridEncoder(satclip_l40, activation=act_type, n_frequencies=25)\n",
    "        r2 = evaluate_neural(encoder, coords_train, values[train_idx], coords_test, values[test_idx],\n",
    "                            epochs=100, device=device)\n",
    "        print(f\"    Best R²: {r2:.4f}\")\n",
    "        results_exp2.append({'region': region_name, 'model': f'SH(L=40) + {act_type}', 'r2': r2})\n",
    "\n",
    "# Summary\n",
    "df2 = pd.DataFrame(results_exp2)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"EXPERIMENT 2 RESULTS: Hybrid Approaches\")\n",
    "print(\"=\"*70)\n",
    "print(df2.pivot(index='model', columns='region', values='r2').round(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Experiment 3: Frequency Ablation\n",
    "\n",
    "How many Fourier frequencies do we need?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT 3: Frequency Ablation\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Use Global region for ablation\n",
    "coords, values = sample_from_raster(pop_data, pop_coords, n_samples=15000, bounds=None)\n",
    "\n",
    "np.random.seed(42)\n",
    "idx = np.random.permutation(len(coords))\n",
    "split = len(coords) // 2\n",
    "train_idx, test_idx = idx[:split], idx[split:]\n",
    "coords_train, coords_test = coords[train_idx], coords[test_idx]\n",
    "\n",
    "FREQ_VALUES = [5, 10, 25, 50, 100]\n",
    "results_exp3 = []\n",
    "\n",
    "for n_freq in FREQ_VALUES:\n",
    "    print(f\"\\n  Testing n_frequencies={n_freq}...\")\n",
    "    encoder = LocationEncoder(activation='learned', n_frequencies=n_freq)\n",
    "    n_params = sum(p.numel() for p in encoder.parameters())\n",
    "    \n",
    "    r2 = evaluate_neural(encoder, coords_train, values[train_idx], coords_test, values[test_idx],\n",
    "                        epochs=100, device=device)\n",
    "    print(f\"    R²: {r2:.4f}, Params: {n_params:,}\")\n",
    "    results_exp3.append({'n_frequencies': n_freq, 'r2': r2, 'params': n_params})\n",
    "\n",
    "df3 = pd.DataFrame(results_exp3)\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"FREQUENCY ABLATION RESULTS\")\n",
    "print(\"=\"*70)\n",
    "print(df3.to_string(index=False))\n",
    "\n",
    "# Plot\n",
    "fig, ax = plt.subplots(figsize=(8, 5))\n",
    "ax.plot(df3['n_frequencies'], df3['r2'], 'o-', markersize=10, linewidth=2)\n",
    "ax.set_xlabel('Number of Frequencies')\n",
    "ax.set_ylabel('R² Score')\n",
    "ax.set_title('Frequency Ablation: More frequencies = better?')\n",
    "ax.grid(True, alpha=0.3)\n",
    "for i, row in df3.iterrows():\n",
    "    ax.annotate(f\"{row['r2']:.3f}\", (row['n_frequencies'], row['r2']), \n",
    "                textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
    "plt.tight_layout()\n",
    "plt.savefig('frequency_ablation.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Combined Results & Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"COMBINED RESULTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Combine experiment 1 and 2\n",
    "all_results = pd.concat([df1, df2])\n",
    "pivot = all_results.pivot(index='model', columns='region', values='r2')\n",
    "\n",
    "# Reorder columns\n",
    "col_order = ['Global', 'USA', 'Europe', 'China']\n",
    "pivot = pivot[[c for c in col_order if c in pivot.columns]]\n",
    "\n",
    "print(\"\\nR² Scores by Model and Region:\")\n",
    "print(pivot.round(3).to_string())\n",
    "\n",
    "# Save\n",
    "all_results.to_csv('learned_activations_v2_results.csv', index=False)\n",
    "print(\"\\nResults saved to learned_activations_v2_results.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Experiment 1: Direct approaches\n",
    "ax = axes[0]\n",
    "exp1_pivot = df1.pivot(index='model', columns='region', values='r2')\n",
    "exp1_pivot.T.plot(kind='bar', ax=ax)\n",
    "ax.set_ylabel('R² Score')\n",
    "ax.set_xlabel('Region')\n",
    "ax.set_title('Exp 1: Fair Comparison\\n(SatCLIP+Ridge vs Direct+Neural)')\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "# Experiment 2: Hybrid approaches\n",
    "ax = axes[1]\n",
    "exp2_pivot = df2.pivot(index='model', columns='region', values='r2')\n",
    "exp2_pivot.T.plot(kind='bar', ax=ax)\n",
    "ax.set_ylabel('R² Score')\n",
    "ax.set_xlabel('Region')\n",
    "ax.set_title('Exp 2: Hybrid Approaches\\n(SH features + different activations)')\n",
    "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8)\n",
    "ax.set_ylim(0, 1)\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('learned_activations_v2_comparison.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key findings analysis\n",
    "print(\"=\"*80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. SatCLIP BASELINE (with fair sklearn comparison):\")\n",
    "for region in ['Global', 'USA', 'Europe', 'China']:\n",
    "    l10 = df1[(df1['model'] == 'SatCLIP L=10 (Ridge)') & (df1['region'] == region)]['r2'].values\n",
    "    l40 = df1[(df1['model'] == 'SatCLIP L=40 (Ridge)') & (df1['region'] == region)]['r2'].values\n",
    "    if len(l10) > 0 and len(l40) > 0:\n",
    "        print(f\"  {region}: L=10={l10[0]:.3f}, L=40={l40[0]:.3f}, diff={l40[0]-l10[0]:+.3f}\")\n",
    "\n",
    "print(\"\\n2. DIRECT LEARNED vs SatCLIP:\")\n",
    "for region in ['Global', 'USA', 'Europe', 'China']:\n",
    "    l10 = df1[(df1['model'] == 'SatCLIP L=10 (Ridge)') & (df1['region'] == region)]['r2'].values\n",
    "    learned = df1[(df1['model'] == 'Direct + Learned') & (df1['region'] == region)]['r2'].values\n",
    "    if len(l10) > 0 and len(learned) > 0:\n",
    "        print(f\"  {region}: Learned={learned[0]:.3f} vs L=10={l10[0]:.3f} ({learned[0]-l10[0]:+.3f})\")\n",
    "\n",
    "print(\"\\n3. HYBRID APPROACHES (best per region):\")\n",
    "for region in ['Global', 'USA', 'Europe', 'China']:\n",
    "    region_data = df2[df2['region'] == region]\n",
    "    if len(region_data) > 0:\n",
    "        best = region_data.loc[region_data['r2'].idxmax()]\n",
    "        print(f\"  {region}: {best['model']} = {best['r2']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Conclusions & Next Steps\n",
    "\n",
    "### Key Findings:\n",
    "1. **Fair comparison changes everything** - SatCLIP with Ridge regression performs much better than the broken MLP head approach\n",
    "2. **Learned activations vs SatCLIP** - How do they compare with fair evaluation?\n",
    "3. **Hybrid potential** - Does SH + learned activations beat SH + SIREN?\n",
    "\n",
    "### Next Steps Based on Results:\n",
    "- If learned activations match SatCLIP: Scale up to contrastive pretraining\n",
    "- If hybrid is best: Use SH encoding with learned activations\n",
    "- If neither works: The pretrained nature of SatCLIP is the key advantage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all results\n",
    "summary = {\n",
    "    'experiment_1': df1.to_dict('records'),\n",
    "    'experiment_2': df2.to_dict('records'),\n",
    "    'experiment_3': df3.to_dict('records') if 'df3' in dir() else [],\n",
    "}\n",
    "\n",
    "import json\n",
    "with open('learned_activations_v2_summary.json', 'w') as f:\n",
    "    json.dump(summary, f, indent=2)\n",
    "\n",
    "print(\"All results saved!\")\n",
    "print(\"\\nFiles created:\")\n",
    "print(\"  - learned_activations_v2_results.csv\")\n",
    "print(\"  - learned_activations_v2_summary.json\")\n",
    "print(\"  - learned_activations_v2_comparison.png\")\n",
    "print(\"  - frequency_ablation.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
