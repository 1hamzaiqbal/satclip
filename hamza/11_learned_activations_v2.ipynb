{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EDaw9QusdcdX"
      },
      "source": [
        "# Learned Activations v2: Fair Comparisons & Hybrid Approaches\n",
        "\n",
        "## Issues from Notebook 10\n",
        "1. **SatCLIP comparison was broken** - MLP head overfitted on frozen embeddings\n",
        "2. **Unfair param count** - SatCLIP had 33K params, Direct had 231K\n",
        "3. **SIREN initialization wrong** - ω₀=30 too high for [-1,1] coordinates\n",
        "\n",
        "## This notebook fixes these issues:\n",
        "1. **Fair SatCLIP baseline** - Use sklearn (Ridge) like notebook 07\n",
        "2. **Hybrid approach** - SH features + learned activations\n",
        "3. **Fixed SIREN** - Lower ω₀ values\n",
        "4. **Ablation study** - Number of frequencies, layers, experts\n",
        "\n",
        "## Key Questions\n",
        "1. Can learned activations match pretrained SatCLIP?\n",
        "2. Does SH + learned activations beat SH + SIREN?\n",
        "3. What's the optimal number of frequencies?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E5d46DpEdcdY",
        "outputId": "ce24f931-6b5f-4875-dfa6-bfbf2f431bf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'satclip'...\n",
            "remote: Enumerating objects: 473, done.\u001b[K\n",
            "remote: Counting objects: 100% (286/286), done.\u001b[K\n",
            "remote: Compressing objects: 100% (151/151), done.\u001b[K\n",
            "remote: Total 473 (delta 213), reused 169 (delta 135), pack-reused 187 (from 2)\u001b[K\n",
            "Receiving objects: 100% (473/473), 81.25 MiB | 23.48 MiB/s, done.\n",
            "Resolving deltas: 100% (241/241), done.\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m846.0/846.0 kB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m650.7/650.7 kB\u001b[0m \u001b[31m52.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m243.9/243.9 kB\u001b[0m \u001b[31m24.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m859.3/859.3 kB\u001b[0m \u001b[31m61.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m849.5/849.5 kB\u001b[0m \u001b[31m66.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.8/154.8 kB\u001b[0m \u001b[31m14.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m983.2/983.2 kB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m165.6/165.6 kB\u001b[0m \u001b[31m16.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m16.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m122.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m760.5/760.5 kB\u001b[0m \u001b[31m57.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "# Setup\n",
        "import os\n",
        "import sys\n",
        "\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    !rm -rf sample_data .config satclip gpw_data 2>/dev/null\n",
        "    !git clone https://github.com/1hamzaiqbal/satclip.git\n",
        "    !pip install lightning torchgeo huggingface_hub rasterio --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wERaL_rldcdZ",
        "outputId": "1da62321-4184-477a-d249-f82071ceedb1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Extracting GPW data...\n",
            "Extracted 15-min resolution\n",
            "Done!\n"
          ]
        }
      ],
      "source": [
        "# Mount Google Drive and extract GPW data\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "import os\n",
        "import zipfile\n",
        "\n",
        "GPW_DIR = './gpw_data'\n",
        "os.makedirs(GPW_DIR, exist_ok=True)\n",
        "\n",
        "SOURCE_ZIP_PATH = '/content/drive/MyDrive/grad/learned_activations/dataverse_files.zip'\n",
        "\n",
        "print(\"Extracting GPW data...\")\n",
        "with zipfile.ZipFile(SOURCE_ZIP_PATH, 'r') as z:\n",
        "    z.extractall(GPW_DIR)\n",
        "\n",
        "# Extract 15-min resolution\n",
        "zip_path = os.path.join(GPW_DIR, 'gpw-v4-population-density-rev11_2020_15_min_tif.zip')\n",
        "if os.path.exists(zip_path):\n",
        "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
        "        z.extractall(GPW_DIR)\n",
        "    print(\"Extracted 15-min resolution\")\n",
        "\n",
        "print(\"Done!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 223,
          "referenced_widgets": [
            "cd6a6641f442432aa7abcbad80658647",
            "c472a5bb9f4c4a829666410473653856",
            "3eb573d2f45e45f6ae7e98d9ac2c4c35",
            "299b6b44faf442378fbcbd5bddde392b",
            "b9d8136e8fe746239f0b2c6cd45e4c45",
            "575ca4714d9a4ac099ae0904a755d586",
            "702cecc9989c481cb361eb5a074679ab",
            "17833f6301894ca0bc8f71f38757f2ed",
            "74199fe627584691bff1ba0850bec523",
            "264020de34a94461b1bbbd89873b0987",
            "8d21d473ef1846a3a0885d5c2bc498d6",
            "d03169ba3c33462eaf09ad9906bd80c8",
            "b9eb1860466144f79df3f7c56c330796",
            "b95efef721f64fd3b47362c87df84671",
            "fc66a2da8fa34aa6bbe4641b2cd8cf67",
            "d54a288babe0456ba8dd989403496391",
            "e60ad04c9e90429387ef116097f9fdcc",
            "5fa3e4a6304b46308e10fea28e0739e1",
            "fc5f30273c7844058b11018735142884",
            "159dc92f5ab6405b8e03722fb723150f",
            "f6cbfc2bb84248759841db6202eda09d",
            "1957487960df4026b89e6443dd999a0d"
          ]
        },
        "id": "QDZNBTKJdcdZ",
        "outputId": "38e59b77-e2fb-4783-a534-52e27ae9b8bc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading SatCLIP models...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "satclip-vit16-l10.ckpt:   0%|          | 0.00/103M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cd6a6641f442432aa7abcbad80658647"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using pretrained moco vit16\n",
            "Downloading: \"https://hf.co/torchgeo/vit_small_patch16_224_sentinel2_all_moco/resolve/1cb683f6c14739634cdfaaceb076529adf898c74/vit_small_patch16_224_sentinel2_all_moco-67c9032d.pth\" to /root/.cache/torch/hub/checkpoints/vit_small_patch16_224_sentinel2_all_moco-67c9032d.pth\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 86.5M/86.5M [00:00<00:00, 299MB/s]\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "satclip-vit16-l40.ckpt:   0%|          | 0.00/121M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "d03169ba3c33462eaf09ad9906bd80c8"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "using pretrained moco vit16\n",
            "SatCLIP models loaded!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "import torch.optim as optim\n",
        "\n",
        "from sklearn.linear_model import Ridge\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "if 'COLAB_GPU' in os.environ:\n",
        "    sys.path.append('./satclip/satclip')\n",
        "    GPW_DIR = './gpw_data'\n",
        "else:\n",
        "    sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'satclip'))\n",
        "    GPW_DIR = './gpw_data'\n",
        "\n",
        "from huggingface_hub import hf_hub_download\n",
        "from load import get_satclip\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# Load SatCLIP models\n",
        "print(\"Loading SatCLIP models...\")\n",
        "satclip_l10 = get_satclip(hf_hub_download(\"microsoft/SatCLIP-ViT16-L10\", \"satclip-vit16-l10.ckpt\"), device=device)\n",
        "satclip_l40 = get_satclip(hf_hub_download(\"microsoft/SatCLIP-ViT16-L40\", \"satclip-vit16-l40.ckpt\"), device=device)\n",
        "satclip_l10.eval()\n",
        "satclip_l40.eval()\n",
        "print(\"SatCLIP models loaded!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ruQaxJ0OdcdZ"
      },
      "source": [
        "---\n",
        "## 1. Data Loading (same as notebook 10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I1dlH9xZdcda",
        "outputId": "0fd20b26-0ca5-4460-8576-81c31cb92f94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loading population data...\n",
            "Shape: (720, 1440)\n"
          ]
        }
      ],
      "source": [
        "Image.MAX_IMAGE_PIXELS = None\n",
        "\n",
        "def load_gpw_raster(resolution='15_min', year=2020):\n",
        "    tif_file = f\"{GPW_DIR}/gpw_v4_population_density_rev11_{year}_{resolution}.tif\"\n",
        "    if not os.path.exists(tif_file):\n",
        "        print(f\"File not found: {tif_file}\")\n",
        "        return None, None\n",
        "\n",
        "    img = Image.open(tif_file)\n",
        "    data = np.array(img)\n",
        "    height, width = data.shape\n",
        "\n",
        "    lon_step = 360 / width\n",
        "    lat_step = 180 / height\n",
        "    lons = np.linspace(-180 + lon_step/2, 180 - lon_step/2, width)\n",
        "    lats = np.linspace(90 - lat_step/2, -90 + lat_step/2, height)\n",
        "\n",
        "    return data, (lons, lats)\n",
        "\n",
        "def sample_from_raster(data, coords, n_samples=10000, seed=42, bounds=None):\n",
        "    np.random.seed(seed)\n",
        "    lons, lats = coords\n",
        "    valid_mask = data > -1e30\n",
        "\n",
        "    if bounds is not None:\n",
        "        lon_min, lat_min, lon_max, lat_max = bounds\n",
        "        lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
        "        bounds_mask = (\n",
        "            (lon_grid >= lon_min) & (lon_grid <= lon_max) &\n",
        "            (lat_grid >= lat_min) & (lat_grid <= lat_max)\n",
        "        )\n",
        "        valid_mask = valid_mask & bounds_mask\n",
        "\n",
        "    valid_idx = np.where(valid_mask)\n",
        "    n_valid = len(valid_idx[0])\n",
        "\n",
        "    if n_valid < n_samples:\n",
        "        sample_idx = np.arange(n_valid)\n",
        "    else:\n",
        "        sample_idx = np.random.choice(n_valid, n_samples, replace=False)\n",
        "\n",
        "    row_idx = valid_idx[0][sample_idx]\n",
        "    col_idx = valid_idx[1][sample_idx]\n",
        "\n",
        "    sample_lons = lons[col_idx]\n",
        "    sample_lats = lats[row_idx]\n",
        "    sample_values = data[row_idx, col_idx]\n",
        "\n",
        "    coords_arr = np.stack([sample_lons, sample_lats], axis=1)\n",
        "    return coords_arr, sample_values\n",
        "\n",
        "# Load data\n",
        "print(\"Loading population data...\")\n",
        "pop_data, pop_coords = load_gpw_raster('15_min')\n",
        "print(f\"Shape: {pop_data.shape}\")\n",
        "\n",
        "REGIONS = {\n",
        "    'Global': None,\n",
        "    'USA': (-125, 24, -66, 50),\n",
        "    'Europe': (-10, 35, 40, 70),\n",
        "    'China': (73, 18, 135, 54),\n",
        "}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9G_6djK4dcda"
      },
      "source": [
        "---\n",
        "## 2. Fixed Model Architectures"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DZIJj4i-dcda",
        "outputId": "cf765a13-7002-4bb2-df45-615efee22d4f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Testing architectures:\n",
            "  Direct + relu: 198,144 params\n",
            "  Direct + siren: 198,144 params\n",
            "  Direct + learned: 198,300 params\n"
          ]
        }
      ],
      "source": [
        "class LearnedActivation(nn.Module):\n",
        "    \"\"\"Fourier-parameterized learned activation function.\"\"\"\n",
        "    def __init__(self, n_frequencies=25, freq_init='linear', learnable_freq=False, max_freq=10.0):\n",
        "        super().__init__()\n",
        "        self.n_frequencies = n_frequencies\n",
        "\n",
        "        if freq_init == 'linear':\n",
        "            freqs = torch.linspace(0.1, max_freq, n_frequencies)\n",
        "        elif freq_init == 'log':\n",
        "            freqs = torch.logspace(-1, np.log10(max_freq), n_frequencies)\n",
        "        else:\n",
        "            freqs = torch.rand(n_frequencies) * max_freq\n",
        "\n",
        "        if learnable_freq:\n",
        "            self.frequencies = nn.Parameter(freqs)\n",
        "        else:\n",
        "            self.register_buffer('frequencies', freqs)\n",
        "\n",
        "        self.sin_coeffs = nn.Parameter(torch.randn(n_frequencies) * 0.1)\n",
        "        self.cos_coeffs = nn.Parameter(torch.randn(n_frequencies) * 0.1)\n",
        "        self.bias = nn.Parameter(torch.zeros(1))\n",
        "        self.scale = nn.Parameter(torch.ones(1))\n",
        "\n",
        "    def forward(self, x):\n",
        "        wx = x.unsqueeze(-1) * self.frequencies\n",
        "        sin_terms = torch.sin(wx) * self.sin_coeffs\n",
        "        cos_terms = torch.cos(wx) * self.cos_coeffs\n",
        "        result = (sin_terms + cos_terms).sum(dim=-1)\n",
        "        return self.scale * result + self.bias\n",
        "\n",
        "\n",
        "class SineActivation(nn.Module):\n",
        "    \"\"\"Sine activation with configurable omega.\"\"\"\n",
        "    def __init__(self, omega_0=30.0):\n",
        "        super().__init__()\n",
        "        self.omega_0 = omega_0\n",
        "\n",
        "    def forward(self, x):\n",
        "        return torch.sin(self.omega_0 * x)\n",
        "\n",
        "\n",
        "class LocationEncoder(nn.Module):\n",
        "    \"\"\"Location encoder with configurable activations.\"\"\"\n",
        "    def __init__(self, input_dim=2, hidden_dim=256, output_dim=256, n_layers=3,\n",
        "                 activation='relu', n_frequencies=25, omega_0=30.0):\n",
        "        super().__init__()\n",
        "        self.activation_type = activation\n",
        "        self.n_layers = n_layers\n",
        "\n",
        "        dims = [input_dim] + [hidden_dim] * n_layers + [output_dim]\n",
        "        self.linears = nn.ModuleList([nn.Linear(dims[i], dims[i+1]) for i in range(len(dims)-1)])\n",
        "\n",
        "        if activation == 'relu':\n",
        "            self.activations = nn.ModuleList([nn.ReLU() for _ in range(n_layers)])\n",
        "        elif activation == 'siren':\n",
        "            self.activations = nn.ModuleList([SineActivation(omega_0=omega_0) for _ in range(n_layers)])\n",
        "        elif activation == 'learned':\n",
        "            self.activations = nn.ModuleList([LearnedActivation(n_frequencies=n_frequencies) for _ in range(n_layers)])\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for i, linear in enumerate(self.linears):\n",
        "            if self.activation_type == 'siren':\n",
        "                omega_0 = self.activations[0].omega_0 if i > 0 else 1.0\n",
        "                bound = np.sqrt(6/linear.in_features) / omega_0\n",
        "                nn.init.uniform_(linear.weight, -bound, bound)\n",
        "            else:\n",
        "                nn.init.kaiming_normal_(linear.weight)\n",
        "            nn.init.zeros_(linear.bias)\n",
        "\n",
        "    def forward(self, coords):\n",
        "        x = coords.clone()\n",
        "        x[:, 0] = x[:, 0] / 180.0\n",
        "        x[:, 1] = x[:, 1] / 90.0\n",
        "\n",
        "        for linear, act in zip(self.linears[:-1], self.activations):\n",
        "            x = act(linear(x))\n",
        "        x = self.linears[-1](x)\n",
        "        return x\n",
        "\n",
        "\n",
        "class HybridEncoder(nn.Module):\n",
        "    \"\"\"Spherical harmonics input + learned activations.\n",
        "\n",
        "    This tests: can learned activations improve on SIREN\n",
        "    when using the same SH positional encoding?\n",
        "    \"\"\"\n",
        "    def __init__(self, sh_model, hidden_dim=256, output_dim=256, n_layers=3,\n",
        "                 activation='learned', n_frequencies=25, freeze_sh=True):\n",
        "        super().__init__()\n",
        "        self.sh_model = sh_model\n",
        "        self.activation_type = activation\n",
        "\n",
        "        if freeze_sh:\n",
        "            for param in self.sh_model.parameters():\n",
        "                param.requires_grad = False\n",
        "\n",
        "        # Get SH output dim (L=10: 100, L=40: 1600)\n",
        "        # The SatCLIP location encoder has .posenc for spherical harmonics\n",
        "        with torch.no_grad():\n",
        "            test_coord = torch.tensor([[0.0, 0.0]]).double().to(next(sh_model.parameters()).device)\n",
        "            sh_out = sh_model.posenc(test_coord)  # Use .posenc not .spherical_harmonics\n",
        "            sh_dim = sh_out.shape[-1]\n",
        "\n",
        "        self.sh_dim = sh_dim\n",
        "        print(f\"  SH dim: {sh_dim}\")\n",
        "\n",
        "        # Build MLP with learned activations\n",
        "        dims = [sh_dim] + [hidden_dim] * n_layers + [output_dim]\n",
        "        self.linears = nn.ModuleList([nn.Linear(dims[i], dims[i+1]) for i in range(len(dims)-1)])\n",
        "\n",
        "        if activation == 'learned':\n",
        "            self.activations = nn.ModuleList([LearnedActivation(n_frequencies=n_frequencies) for _ in range(n_layers)])\n",
        "        elif activation == 'relu':\n",
        "            self.activations = nn.ModuleList([nn.ReLU() for _ in range(n_layers)])\n",
        "        elif activation == 'siren':\n",
        "            self.activations = nn.ModuleList([SineActivation(omega_0=30.0) for _ in range(n_layers)])\n",
        "\n",
        "        self._init_weights()\n",
        "\n",
        "    def _init_weights(self):\n",
        "        for linear in self.linears:\n",
        "            nn.init.kaiming_normal_(linear.weight)\n",
        "            nn.init.zeros_(linear.bias)\n",
        "\n",
        "    def forward(self, coords):\n",
        "        # Get SH features using .posenc (positional encoding)\n",
        "        with torch.no_grad():\n",
        "            sh_features = self.sh_model.posenc(coords.double()).float()\n",
        "\n",
        "        x = sh_features\n",
        "        for linear, act in zip(self.linears[:-1], self.activations):\n",
        "            x = act(linear(x))\n",
        "        x = self.linears[-1](x)\n",
        "        return x\n",
        "\n",
        "\n",
        "# Test\n",
        "print(\"Testing architectures:\")\n",
        "for act in ['relu', 'siren', 'learned']:\n",
        "    enc = LocationEncoder(activation=act, omega_0=1.0)  # Fixed: lower omega for direct coords\n",
        "    print(f\"  Direct + {act}: {sum(p.numel() for p in enc.parameters()):,} params\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sHYunEZwdcda"
      },
      "source": [
        "---\n",
        "## 3. Fair Evaluation Functions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "n4AWjMO0dcda"
      },
      "outputs": [],
      "source": [
        "def get_satclip_embeddings(model, coords, device, batch_size=512):\n",
        "    \"\"\"Extract SatCLIP embeddings for coordinates.\"\"\"\n",
        "    model.eval()\n",
        "    embeddings = []\n",
        "\n",
        "    coords_tensor = torch.tensor(coords, dtype=torch.float64)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for i in range(0, len(coords), batch_size):\n",
        "            batch = coords_tensor[i:i+batch_size].to(device)\n",
        "            emb = model(batch).cpu().numpy()\n",
        "            embeddings.append(emb)\n",
        "\n",
        "    return np.vstack(embeddings)\n",
        "\n",
        "\n",
        "def evaluate_sklearn(X_train, y_train, X_test, y_test, alpha=1.0):\n",
        "    \"\"\"Evaluate using Ridge regression (fair comparison).\"\"\"\n",
        "    model = Ridge(alpha=alpha)\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    return r2_score(y_test, y_pred)\n",
        "\n",
        "\n",
        "def evaluate_neural(encoder, coords_train, y_train, coords_test, y_test,\n",
        "                   epochs=100, lr=1e-3, batch_size=256, device='cuda'):\n",
        "    \"\"\"Train encoder end-to-end and evaluate.\"\"\"\n",
        "\n",
        "    # Create prediction head\n",
        "    class Predictor(nn.Module):\n",
        "        def __init__(self, encoder):\n",
        "            super().__init__()\n",
        "            self.encoder = encoder\n",
        "            self.head = nn.Sequential(\n",
        "                nn.Linear(256, 128),\n",
        "                nn.ReLU(),\n",
        "                nn.Linear(128, 1)\n",
        "            )\n",
        "\n",
        "        def forward(self, x):\n",
        "            return self.head(self.encoder(x)).squeeze(-1)\n",
        "\n",
        "    model = Predictor(encoder).to(device)\n",
        "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "    criterion = nn.MSELoss()\n",
        "\n",
        "    # Prepare data\n",
        "    train_coords = torch.tensor(coords_train, dtype=torch.float32)\n",
        "    train_y = torch.tensor(np.log1p(y_train), dtype=torch.float32)\n",
        "    test_coords = torch.tensor(coords_test, dtype=torch.float32)\n",
        "    test_y = torch.tensor(np.log1p(y_test), dtype=torch.float32)\n",
        "\n",
        "    train_dataset = torch.utils.data.TensorDataset(train_coords, train_y)\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "    best_r2 = -float('inf')\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        model.train()\n",
        "        for coords_batch, y_batch in train_loader:\n",
        "            coords_batch, y_batch = coords_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            loss = criterion(model(coords_batch), y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        # Evaluate\n",
        "        model.eval()\n",
        "        with torch.no_grad():\n",
        "            preds = model(test_coords.to(device)).cpu().numpy()\n",
        "        r2 = r2_score(test_y.numpy(), preds)\n",
        "        best_r2 = max(best_r2, r2)\n",
        "\n",
        "        if (epoch + 1) % 25 == 0:\n",
        "            print(f\"    Epoch {epoch+1}/{epochs}: R²={r2:.4f}\")\n",
        "\n",
        "    return best_r2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sjXgkB2tdcdb"
      },
      "source": [
        "---\n",
        "## 4. Experiment 1: Fair SatCLIP vs Learned Activations\n",
        "\n",
        "Using sklearn Ridge regression on embeddings for a fair comparison."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bg2HUX39dcdb",
        "outputId": "3891e429-b4fc-44b3-cc95-38f4a0c1ac52"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "================================================================================\n",
            "EXPERIMENT 1: Fair Comparison (sklearn on embeddings)\n",
            "================================================================================\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "Region: Global\n",
            "────────────────────────────────────────────────────────────\n",
            "Train: 7500, Test: 7500\n",
            "\n",
            "  SatCLIP L=10 + Ridge...\n",
            "    R²: 0.7597\n",
            "  SatCLIP L=40 + Ridge...\n",
            "    R²: 0.6872\n",
            "  Direct + ReLU (end-to-end)...\n",
            "    Epoch 25/100: R²=0.6704\n",
            "    Epoch 50/100: R²=0.6862\n",
            "    Epoch 75/100: R²=0.7326\n",
            "    Epoch 100/100: R²=0.7591\n",
            "    Best R²: 0.7591\n",
            "  Direct + Learned (end-to-end)...\n",
            "    Epoch 25/100: R²=0.7708\n",
            "    Epoch 50/100: R²=0.8007\n",
            "    Epoch 75/100: R²=0.7861\n",
            "    Epoch 100/100: R²=0.7886\n",
            "    Best R²: 0.8007\n",
            "  Direct + SIREN (ω=1.0, fixed)...\n",
            "    Epoch 25/100: R²=0.7044\n",
            "    Epoch 50/100: R²=0.7488\n",
            "    Epoch 75/100: R²=0.7439\n",
            "    Epoch 100/100: R²=0.7614\n",
            "    Best R²: 0.7724\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "Region: USA\n",
            "────────────────────────────────────────────────────────────\n",
            "Train: 7500, Test: 7500\n",
            "\n",
            "  SatCLIP L=10 + Ridge...\n",
            "    R²: 0.4935\n",
            "  SatCLIP L=40 + Ridge...\n",
            "    R²: 0.5497\n",
            "  Direct + ReLU (end-to-end)...\n",
            "    Epoch 25/100: R²=0.4244\n",
            "    Epoch 50/100: R²=0.4366\n",
            "    Epoch 75/100: R²=0.4507\n",
            "    Epoch 100/100: R²=0.4526\n",
            "    Best R²: 0.4559\n",
            "  Direct + Learned (end-to-end)...\n",
            "    Epoch 25/100: R²=0.5046\n",
            "    Epoch 50/100: R²=0.5577\n",
            "    Epoch 75/100: R²=0.5783\n",
            "    Epoch 100/100: R²=0.6170\n",
            "    Best R²: 0.6170\n",
            "  Direct + SIREN (ω=1.0, fixed)...\n",
            "    Epoch 25/100: R²=0.4181\n",
            "    Epoch 50/100: R²=0.4245\n",
            "    Epoch 75/100: R²=0.4472\n",
            "    Epoch 100/100: R²=0.4687\n",
            "    Best R²: 0.4687\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "Region: Europe\n",
            "────────────────────────────────────────────────────────────\n",
            "Train: 7500, Test: 7500\n",
            "\n",
            "  SatCLIP L=10 + Ridge...\n",
            "    R²: 0.6277\n",
            "  SatCLIP L=40 + Ridge...\n",
            "    R²: 0.6822\n",
            "  Direct + ReLU (end-to-end)...\n",
            "    Epoch 25/100: R²=0.4923\n",
            "    Epoch 50/100: R²=0.5638\n",
            "    Epoch 75/100: R²=0.5448\n",
            "    Epoch 100/100: R²=0.5759\n",
            "    Best R²: 0.5910\n",
            "  Direct + Learned (end-to-end)...\n",
            "    Epoch 25/100: R²=0.6596\n",
            "    Epoch 50/100: R²=0.6961\n",
            "    Epoch 75/100: R²=0.7056\n",
            "    Epoch 100/100: R²=0.7265\n",
            "    Best R²: 0.7345\n",
            "  Direct + SIREN (ω=1.0, fixed)...\n",
            "    Epoch 25/100: R²=0.5653\n",
            "    Epoch 50/100: R²=0.5823\n",
            "    Epoch 75/100: R²=0.5800\n",
            "    Epoch 100/100: R²=0.6170\n",
            "    Best R²: 0.6205\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "Region: China\n",
            "────────────────────────────────────────────────────────────\n",
            "Train: 7500, Test: 7500\n",
            "\n",
            "  SatCLIP L=10 + Ridge...\n",
            "    R²: 0.8435\n",
            "  SatCLIP L=40 + Ridge...\n",
            "    R²: 0.8667\n",
            "  Direct + ReLU (end-to-end)...\n",
            "    Epoch 25/100: R²=0.6888\n",
            "    Epoch 50/100: R²=0.7972\n",
            "    Epoch 75/100: R²=0.7942\n",
            "    Epoch 100/100: R²=0.8236\n",
            "    Best R²: 0.8368\n",
            "  Direct + Learned (end-to-end)...\n",
            "    Epoch 25/100: R²=0.8890\n",
            "    Epoch 50/100: R²=0.8969\n",
            "    Epoch 75/100: R²=0.9034\n",
            "    Epoch 100/100: R²=0.9120\n",
            "    Best R²: 0.9120\n",
            "  Direct + SIREN (ω=1.0, fixed)...\n",
            "    Epoch 25/100: R²=0.7816\n",
            "    Epoch 50/100: R²=0.8153\n",
            "    Epoch 75/100: R²=0.8344\n",
            "    Epoch 100/100: R²=0.8494\n",
            "    Best R²: 0.8494\n",
            "\n",
            "======================================================================\n",
            "EXPERIMENT 1 RESULTS\n",
            "======================================================================\n",
            "region                China  Europe  Global    USA\n",
            "model                                             \n",
            "Direct + Learned      0.912   0.734   0.801  0.617\n",
            "Direct + ReLU         0.837   0.591   0.759  0.456\n",
            "Direct + SIREN (ω=1)  0.849   0.621   0.772  0.469\n",
            "SatCLIP L=10 (Ridge)  0.844   0.628   0.760  0.493\n",
            "SatCLIP L=40 (Ridge)  0.867   0.682   0.687  0.550\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"EXPERIMENT 1: Fair Comparison (sklearn on embeddings)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "N_SAMPLES = 15000\n",
        "results_exp1 = []\n",
        "\n",
        "for region_name, bounds in REGIONS.items():\n",
        "    print(f\"\\n{'─'*60}\")\n",
        "    print(f\"Region: {region_name}\")\n",
        "    print(f\"{'─'*60}\")\n",
        "\n",
        "    # Sample data\n",
        "    coords, values = sample_from_raster(pop_data, pop_coords, n_samples=N_SAMPLES, bounds=bounds)\n",
        "    y = np.log1p(values)  # Log transform\n",
        "\n",
        "    # Train/test split\n",
        "    np.random.seed(42)\n",
        "    idx = np.random.permutation(len(coords))\n",
        "    split = len(coords) // 2\n",
        "    train_idx, test_idx = idx[:split], idx[split:]\n",
        "\n",
        "    coords_train, coords_test = coords[train_idx], coords[test_idx]\n",
        "    y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "    print(f\"Train: {len(coords_train)}, Test: {len(coords_test)}\")\n",
        "\n",
        "    # 1. SatCLIP L=10 + Ridge\n",
        "    print(\"\\n  SatCLIP L=10 + Ridge...\")\n",
        "    emb_train = get_satclip_embeddings(satclip_l10, coords_train, device)\n",
        "    emb_test = get_satclip_embeddings(satclip_l10, coords_test, device)\n",
        "    r2_l10 = evaluate_sklearn(emb_train, y_train, emb_test, y_test)\n",
        "    print(f\"    R²: {r2_l10:.4f}\")\n",
        "    results_exp1.append({'region': region_name, 'model': 'SatCLIP L=10 (Ridge)', 'r2': r2_l10})\n",
        "\n",
        "    # 2. SatCLIP L=40 + Ridge\n",
        "    print(\"  SatCLIP L=40 + Ridge...\")\n",
        "    emb_train = get_satclip_embeddings(satclip_l40, coords_train, device)\n",
        "    emb_test = get_satclip_embeddings(satclip_l40, coords_test, device)\n",
        "    r2_l40 = evaluate_sklearn(emb_train, y_train, emb_test, y_test)\n",
        "    print(f\"    R²: {r2_l40:.4f}\")\n",
        "    results_exp1.append({'region': region_name, 'model': 'SatCLIP L=40 (Ridge)', 'r2': r2_l40})\n",
        "\n",
        "    # 3. Direct + ReLU (end-to-end)\n",
        "    print(\"  Direct + ReLU (end-to-end)...\")\n",
        "    encoder = LocationEncoder(activation='relu')\n",
        "    r2_relu = evaluate_neural(encoder, coords_train, values[train_idx], coords_test, values[test_idx],\n",
        "                              epochs=100, device=device)\n",
        "    print(f\"    Best R²: {r2_relu:.4f}\")\n",
        "    results_exp1.append({'region': region_name, 'model': 'Direct + ReLU', 'r2': r2_relu})\n",
        "\n",
        "    # 4. Direct + Learned (end-to-end)\n",
        "    print(\"  Direct + Learned (end-to-end)...\")\n",
        "    encoder = LocationEncoder(activation='learned', n_frequencies=25)\n",
        "    r2_learned = evaluate_neural(encoder, coords_train, values[train_idx], coords_test, values[test_idx],\n",
        "                                 epochs=100, device=device)\n",
        "    print(f\"    Best R²: {r2_learned:.4f}\")\n",
        "    results_exp1.append({'region': region_name, 'model': 'Direct + Learned', 'r2': r2_learned})\n",
        "\n",
        "    # 5. Direct + SIREN (fixed omega=1.0 for [-1,1] range)\n",
        "    print(\"  Direct + SIREN (ω=1.0, fixed)...\")\n",
        "    encoder = LocationEncoder(activation='siren', omega_0=1.0)\n",
        "    r2_siren = evaluate_neural(encoder, coords_train, values[train_idx], coords_test, values[test_idx],\n",
        "                               epochs=100, device=device)\n",
        "    print(f\"    Best R²: {r2_siren:.4f}\")\n",
        "    results_exp1.append({'region': region_name, 'model': 'Direct + SIREN (ω=1)', 'r2': r2_siren})\n",
        "\n",
        "# Summary\n",
        "df1 = pd.DataFrame(results_exp1)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXPERIMENT 1 RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(df1.pivot(index='model', columns='region', values='r2').round(3).to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3F8CV9iKdcdb"
      },
      "source": [
        "---\n",
        "## 5. Experiment 2: Hybrid Approach (SH + Learned Activations)\n",
        "\n",
        "Can we improve SatCLIP by replacing SIREN with learned activations?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lW3evHQQdcdb",
        "outputId": "9a0fe684-d9c8-4981-ffde-0e7e47cf75eb"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "EXPERIMENT 2: Hybrid (SH input + Learned Activations)\n",
            "================================================================================\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "Region: Global\n",
            "────────────────────────────────────────────────────────────\n",
            "\n",
            "  SH(L=10) + relu...\n",
            "  SH dim: 100\n",
            "    Epoch 25/100: R²=0.7900\n",
            "    Epoch 50/100: R²=0.7962\n",
            "    Epoch 75/100: R²=0.7912\n",
            "    Epoch 100/100: R²=0.7920\n",
            "    Best R²: 0.8067\n",
            "\n",
            "  SH(L=10) + siren...\n",
            "  SH dim: 100\n",
            "    Epoch 25/100: R²=-0.0147\n",
            "    Epoch 50/100: R²=-0.0092\n",
            "    Epoch 75/100: R²=-0.0146\n",
            "    Epoch 100/100: R²=-0.0062\n",
            "    Best R²: -0.0055\n",
            "\n",
            "  SH(L=10) + learned...\n",
            "  SH dim: 100\n",
            "    Epoch 25/100: R²=0.7619\n",
            "    Epoch 50/100: R²=0.7621\n",
            "    Epoch 75/100: R²=0.7808\n",
            "    Epoch 100/100: R²=0.7787\n",
            "    Best R²: 0.7861\n",
            "\n",
            "  SH(L=40) + relu...\n",
            "  SH dim: 1600\n",
            "    Epoch 25/100: R²=0.7787\n",
            "    Epoch 50/100: R²=0.7682\n",
            "    Epoch 75/100: R²=0.7685\n",
            "    Epoch 100/100: R²=0.7626\n",
            "    Best R²: 0.8009\n",
            "\n",
            "  SH(L=40) + learned...\n",
            "  SH dim: 1600\n",
            "    Epoch 25/100: R²=0.7199\n",
            "    Epoch 50/100: R²=0.7342\n",
            "    Epoch 75/100: R²=0.7483\n",
            "    Epoch 100/100: R²=0.7503\n",
            "    Best R²: 0.7578\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "Region: USA\n",
            "────────────────────────────────────────────────────────────\n",
            "\n",
            "  SH(L=10) + relu...\n",
            "  SH dim: 100\n",
            "    Epoch 25/100: R²=0.4983\n",
            "    Epoch 50/100: R²=0.5449\n",
            "    Epoch 75/100: R²=0.5561\n",
            "    Epoch 100/100: R²=0.5538\n",
            "    Best R²: 0.5836\n",
            "\n",
            "  SH(L=10) + siren...\n",
            "  SH dim: 100\n",
            "    Epoch 25/100: R²=-0.0140\n",
            "    Epoch 50/100: R²=-0.0124\n",
            "    Epoch 75/100: R²=-0.0100\n",
            "    Epoch 100/100: R²=-0.0067\n",
            "    Best R²: -0.0067\n",
            "\n",
            "  SH(L=10) + learned...\n",
            "  SH dim: 100\n",
            "    Epoch 25/100: R²=0.5914\n",
            "    Epoch 50/100: R²=0.6213\n",
            "    Epoch 75/100: R²=0.6399\n",
            "    Epoch 100/100: R²=0.6360\n",
            "    Best R²: 0.6564\n",
            "\n",
            "  SH(L=40) + relu...\n",
            "  SH dim: 1600\n",
            "    Epoch 25/100: R²=0.6294\n",
            "    Epoch 50/100: R²=0.6330\n",
            "    Epoch 75/100: R²=0.6688\n",
            "    Epoch 100/100: R²=0.6721\n",
            "    Best R²: 0.6772\n",
            "\n",
            "  SH(L=40) + learned...\n",
            "  SH dim: 1600\n",
            "    Epoch 25/100: R²=0.6253\n",
            "    Epoch 50/100: R²=0.6487\n",
            "    Epoch 75/100: R²=0.6558\n",
            "    Epoch 100/100: R²=0.6406\n",
            "    Best R²: 0.6697\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "Region: Europe\n",
            "────────────────────────────────────────────────────────────\n",
            "\n",
            "  SH(L=10) + relu...\n",
            "  SH dim: 100\n",
            "    Epoch 25/100: R²=0.6503\n",
            "    Epoch 50/100: R²=0.6574\n",
            "    Epoch 75/100: R²=0.6780\n",
            "    Epoch 100/100: R²=0.6955\n",
            "    Best R²: 0.7106\n",
            "\n",
            "  SH(L=10) + siren...\n",
            "  SH dim: 100\n",
            "    Epoch 25/100: R²=-0.0490\n",
            "    Epoch 50/100: R²=-0.0292\n",
            "    Epoch 75/100: R²=-0.0396\n",
            "    Epoch 100/100: R²=-0.0348\n",
            "    Best R²: -0.0231\n",
            "\n",
            "  SH(L=10) + learned...\n",
            "  SH dim: 100\n",
            "    Epoch 25/100: R²=0.7307\n",
            "    Epoch 50/100: R²=0.7486\n",
            "    Epoch 75/100: R²=0.7502\n",
            "    Epoch 100/100: R²=0.7612\n",
            "    Best R²: 0.7633\n",
            "\n",
            "  SH(L=40) + relu...\n",
            "  SH dim: 1600\n",
            "    Epoch 25/100: R²=0.7095\n",
            "    Epoch 50/100: R²=0.7504\n",
            "    Epoch 75/100: R²=0.7448\n",
            "    Epoch 100/100: R²=0.7594\n",
            "    Best R²: 0.7638\n",
            "\n",
            "  SH(L=40) + learned...\n",
            "  SH dim: 1600\n",
            "    Epoch 25/100: R²=0.7609\n",
            "    Epoch 50/100: R²=0.7710\n",
            "    Epoch 75/100: R²=0.7676\n",
            "    Epoch 100/100: R²=0.7638\n",
            "    Best R²: 0.7732\n",
            "\n",
            "────────────────────────────────────────────────────────────\n",
            "Region: China\n",
            "────────────────────────────────────────────────────────────\n",
            "\n",
            "  SH(L=10) + relu...\n",
            "  SH dim: 100\n",
            "    Epoch 25/100: R²=0.8601\n",
            "    Epoch 50/100: R²=0.8794\n",
            "    Epoch 75/100: R²=0.8903\n",
            "    Epoch 100/100: R²=0.8941\n",
            "    Best R²: 0.8960\n",
            "\n",
            "  SH(L=10) + siren...\n",
            "  SH dim: 100\n",
            "    Epoch 25/100: R²=-0.0220\n",
            "    Epoch 50/100: R²=-0.0244\n",
            "    Epoch 75/100: R²=-0.0168\n",
            "    Epoch 100/100: R²=-0.0157\n",
            "    Best R²: -0.0074\n",
            "\n",
            "  SH(L=10) + learned...\n",
            "  SH dim: 100\n",
            "    Epoch 25/100: R²=0.9116\n",
            "    Epoch 50/100: R²=0.9126\n",
            "    Epoch 75/100: R²=0.9162\n",
            "    Epoch 100/100: R²=0.9128\n",
            "    Best R²: 0.9183\n",
            "\n",
            "  SH(L=40) + relu...\n",
            "  SH dim: 1600\n"
          ]
        }
      ],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"EXPERIMENT 2: Hybrid (SH input + Learned Activations)\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "results_exp2 = []\n",
        "\n",
        "for region_name, bounds in REGIONS.items():\n",
        "    print(f\"\\n{'─'*60}\")\n",
        "    print(f\"Region: {region_name}\")\n",
        "    print(f\"{'─'*60}\")\n",
        "\n",
        "    # Sample data\n",
        "    coords, values = sample_from_raster(pop_data, pop_coords, n_samples=N_SAMPLES, bounds=bounds)\n",
        "\n",
        "    np.random.seed(42)\n",
        "    idx = np.random.permutation(len(coords))\n",
        "    split = len(coords) // 2\n",
        "    train_idx, test_idx = idx[:split], idx[split:]\n",
        "\n",
        "    coords_train, coords_test = coords[train_idx], coords[test_idx]\n",
        "\n",
        "    # Test hybrid approaches with L=10 SH\n",
        "    for act_type in ['relu', 'siren', 'learned']:\n",
        "        print(f\"\\n  SH(L=10) + {act_type}...\")\n",
        "        encoder = HybridEncoder(satclip_l10, activation=act_type, n_frequencies=25)\n",
        "        r2 = evaluate_neural(encoder, coords_train, values[train_idx], coords_test, values[test_idx],\n",
        "                            epochs=100, device=device)\n",
        "        print(f\"    Best R²: {r2:.4f}\")\n",
        "        results_exp2.append({'region': region_name, 'model': f'SH(L=10) + {act_type}', 'r2': r2})\n",
        "\n",
        "    # Test with L=40 SH\n",
        "    for act_type in ['relu', 'learned']:\n",
        "        print(f\"\\n  SH(L=40) + {act_type}...\")\n",
        "        encoder = HybridEncoder(satclip_l40, activation=act_type, n_frequencies=25)\n",
        "        r2 = evaluate_neural(encoder, coords_train, values[train_idx], coords_test, values[test_idx],\n",
        "                            epochs=100, device=device)\n",
        "        print(f\"    Best R²: {r2:.4f}\")\n",
        "        results_exp2.append({'region': region_name, 'model': f'SH(L=40) + {act_type}', 'r2': r2})\n",
        "\n",
        "# Summary\n",
        "df2 = pd.DataFrame(results_exp2)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"EXPERIMENT 2 RESULTS: Hybrid Approaches\")\n",
        "print(\"=\"*70)\n",
        "print(df2.pivot(index='model', columns='region', values='r2').round(3).to_string())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw-mfksMdcdb"
      },
      "source": [
        "---\n",
        "## 6. Experiment 3: Frequency Ablation\n",
        "\n",
        "How many Fourier frequencies do we need?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "snMBO8Fldcdb"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"EXPERIMENT 3: Frequency Ablation\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Use Global region for ablation\n",
        "coords, values = sample_from_raster(pop_data, pop_coords, n_samples=15000, bounds=None)\n",
        "\n",
        "np.random.seed(42)\n",
        "idx = np.random.permutation(len(coords))\n",
        "split = len(coords) // 2\n",
        "train_idx, test_idx = idx[:split], idx[split:]\n",
        "coords_train, coords_test = coords[train_idx], coords[test_idx]\n",
        "\n",
        "FREQ_VALUES = [5, 10, 25, 50, 100]\n",
        "results_exp3 = []\n",
        "\n",
        "for n_freq in FREQ_VALUES:\n",
        "    print(f\"\\n  Testing n_frequencies={n_freq}...\")\n",
        "    encoder = LocationEncoder(activation='learned', n_frequencies=n_freq)\n",
        "    n_params = sum(p.numel() for p in encoder.parameters())\n",
        "\n",
        "    r2 = evaluate_neural(encoder, coords_train, values[train_idx], coords_test, values[test_idx],\n",
        "                        epochs=100, device=device)\n",
        "    print(f\"    R²: {r2:.4f}, Params: {n_params:,}\")\n",
        "    results_exp3.append({'n_frequencies': n_freq, 'r2': r2, 'params': n_params})\n",
        "\n",
        "df3 = pd.DataFrame(results_exp3)\n",
        "print(\"\\n\" + \"=\"*70)\n",
        "print(\"FREQUENCY ABLATION RESULTS\")\n",
        "print(\"=\"*70)\n",
        "print(df3.to_string(index=False))\n",
        "\n",
        "# Plot\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "ax.plot(df3['n_frequencies'], df3['r2'], 'o-', markersize=10, linewidth=2)\n",
        "ax.set_xlabel('Number of Frequencies')\n",
        "ax.set_ylabel('R² Score')\n",
        "ax.set_title('Frequency Ablation: More frequencies = better?')\n",
        "ax.grid(True, alpha=0.3)\n",
        "for i, row in df3.iterrows():\n",
        "    ax.annotate(f\"{row['r2']:.3f}\", (row['n_frequencies'], row['r2']),\n",
        "                textcoords=\"offset points\", xytext=(0,10), ha='center')\n",
        "plt.tight_layout()\n",
        "plt.savefig('frequency_ablation.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54Vom-Xydcdb"
      },
      "source": [
        "---\n",
        "## 7. Combined Results & Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mZBlm7_Vdcdb"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*80)\n",
        "print(\"COMBINED RESULTS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "# Combine experiment 1 and 2\n",
        "all_results = pd.concat([df1, df2])\n",
        "pivot = all_results.pivot(index='model', columns='region', values='r2')\n",
        "\n",
        "# Reorder columns\n",
        "col_order = ['Global', 'USA', 'Europe', 'China']\n",
        "pivot = pivot[[c for c in col_order if c in pivot.columns]]\n",
        "\n",
        "print(\"\\nR² Scores by Model and Region:\")\n",
        "print(pivot.round(3).to_string())\n",
        "\n",
        "# Save\n",
        "all_results.to_csv('learned_activations_v2_results.csv', index=False)\n",
        "print(\"\\nResults saved to learned_activations_v2_results.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "g8s1jsD3dcdb"
      },
      "outputs": [],
      "source": [
        "# Visualization\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Experiment 1: Direct approaches\n",
        "ax = axes[0]\n",
        "exp1_pivot = df1.pivot(index='model', columns='region', values='r2')\n",
        "exp1_pivot.T.plot(kind='bar', ax=ax)\n",
        "ax.set_ylabel('R² Score')\n",
        "ax.set_xlabel('Region')\n",
        "ax.set_title('Exp 1: Fair Comparison\\n(SatCLIP+Ridge vs Direct+Neural)')\n",
        "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "# Experiment 2: Hybrid approaches\n",
        "ax = axes[1]\n",
        "exp2_pivot = df2.pivot(index='model', columns='region', values='r2')\n",
        "exp2_pivot.T.plot(kind='bar', ax=ax)\n",
        "ax.set_ylabel('R² Score')\n",
        "ax.set_xlabel('Region')\n",
        "ax.set_title('Exp 2: Hybrid Approaches\\n(SH features + different activations)')\n",
        "ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8)\n",
        "ax.set_ylim(0, 1)\n",
        "ax.grid(True, alpha=0.3, axis='y')\n",
        "plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('learned_activations_v2_comparison.png', dpi=150, bbox_inches='tight')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "enS6czzfdcdc"
      },
      "outputs": [],
      "source": [
        "# Key findings analysis\n",
        "print(\"=\"*80)\n",
        "print(\"KEY FINDINGS\")\n",
        "print(\"=\"*80)\n",
        "\n",
        "print(\"\\n1. SatCLIP BASELINE (with fair sklearn comparison):\")\n",
        "for region in ['Global', 'USA', 'Europe', 'China']:\n",
        "    l10 = df1[(df1['model'] == 'SatCLIP L=10 (Ridge)') & (df1['region'] == region)]['r2'].values\n",
        "    l40 = df1[(df1['model'] == 'SatCLIP L=40 (Ridge)') & (df1['region'] == region)]['r2'].values\n",
        "    if len(l10) > 0 and len(l40) > 0:\n",
        "        print(f\"  {region}: L=10={l10[0]:.3f}, L=40={l40[0]:.3f}, diff={l40[0]-l10[0]:+.3f}\")\n",
        "\n",
        "print(\"\\n2. DIRECT LEARNED vs SatCLIP:\")\n",
        "for region in ['Global', 'USA', 'Europe', 'China']:\n",
        "    l10 = df1[(df1['model'] == 'SatCLIP L=10 (Ridge)') & (df1['region'] == region)]['r2'].values\n",
        "    learned = df1[(df1['model'] == 'Direct + Learned') & (df1['region'] == region)]['r2'].values\n",
        "    if len(l10) > 0 and len(learned) > 0:\n",
        "        print(f\"  {region}: Learned={learned[0]:.3f} vs L=10={l10[0]:.3f} ({learned[0]-l10[0]:+.3f})\")\n",
        "\n",
        "print(\"\\n3. HYBRID APPROACHES (best per region):\")\n",
        "for region in ['Global', 'USA', 'Europe', 'China']:\n",
        "    region_data = df2[df2['region'] == region]\n",
        "    if len(region_data) > 0:\n",
        "        best = region_data.loc[region_data['r2'].idxmax()]\n",
        "        print(f\"  {region}: {best['model']} = {best['r2']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y1ftEgaudcdc"
      },
      "source": [
        "---\n",
        "## 8. Conclusions & Next Steps\n",
        "\n",
        "### Key Findings:\n",
        "1. **Fair comparison changes everything** - SatCLIP with Ridge regression performs much better than the broken MLP head approach\n",
        "2. **Learned activations vs SatCLIP** - How do they compare with fair evaluation?\n",
        "3. **Hybrid potential** - Does SH + learned activations beat SH + SIREN?\n",
        "\n",
        "### Next Steps Based on Results:\n",
        "- If learned activations match SatCLIP: Scale up to contrastive pretraining\n",
        "- If hybrid is best: Use SH encoding with learned activations\n",
        "- If neither works: The pretrained nature of SatCLIP is the key advantage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KSruyWSxdcdc"
      },
      "outputs": [],
      "source": [
        "# Save all results\n",
        "summary = {\n",
        "    'experiment_1': df1.to_dict('records'),\n",
        "    'experiment_2': df2.to_dict('records'),\n",
        "    'experiment_3': df3.to_dict('records') if 'df3' in dir() else [],\n",
        "}\n",
        "\n",
        "import json\n",
        "with open('learned_activations_v2_summary.json', 'w') as f:\n",
        "    json.dump(summary, f, indent=2)\n",
        "\n",
        "print(\"All results saved!\")\n",
        "print(\"\\nFiles created:\")\n",
        "print(\"  - learned_activations_v2_results.csv\")\n",
        "print(\"  - learned_activations_v2_summary.json\")\n",
        "print(\"  - learned_activations_v2_comparison.png\")\n",
        "print(\"  - frequency_ablation.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10.0"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cd6a6641f442432aa7abcbad80658647": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_c472a5bb9f4c4a829666410473653856",
              "IPY_MODEL_3eb573d2f45e45f6ae7e98d9ac2c4c35",
              "IPY_MODEL_299b6b44faf442378fbcbd5bddde392b"
            ],
            "layout": "IPY_MODEL_b9d8136e8fe746239f0b2c6cd45e4c45"
          }
        },
        "c472a5bb9f4c4a829666410473653856": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_575ca4714d9a4ac099ae0904a755d586",
            "placeholder": "​",
            "style": "IPY_MODEL_702cecc9989c481cb361eb5a074679ab",
            "value": "satclip-vit16-l10.ckpt: 100%"
          }
        },
        "3eb573d2f45e45f6ae7e98d9ac2c4c35": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_17833f6301894ca0bc8f71f38757f2ed",
            "max": 102550795,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_74199fe627584691bff1ba0850bec523",
            "value": 102550795
          }
        },
        "299b6b44faf442378fbcbd5bddde392b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_264020de34a94461b1bbbd89873b0987",
            "placeholder": "​",
            "style": "IPY_MODEL_8d21d473ef1846a3a0885d5c2bc498d6",
            "value": " 103M/103M [00:02&lt;00:00, 46.5MB/s]"
          }
        },
        "b9d8136e8fe746239f0b2c6cd45e4c45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "575ca4714d9a4ac099ae0904a755d586": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "702cecc9989c481cb361eb5a074679ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "17833f6301894ca0bc8f71f38757f2ed": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "74199fe627584691bff1ba0850bec523": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "264020de34a94461b1bbbd89873b0987": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8d21d473ef1846a3a0885d5c2bc498d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "d03169ba3c33462eaf09ad9906bd80c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_b9eb1860466144f79df3f7c56c330796",
              "IPY_MODEL_b95efef721f64fd3b47362c87df84671",
              "IPY_MODEL_fc66a2da8fa34aa6bbe4641b2cd8cf67"
            ],
            "layout": "IPY_MODEL_d54a288babe0456ba8dd989403496391"
          }
        },
        "b9eb1860466144f79df3f7c56c330796": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e60ad04c9e90429387ef116097f9fdcc",
            "placeholder": "​",
            "style": "IPY_MODEL_5fa3e4a6304b46308e10fea28e0739e1",
            "value": "satclip-vit16-l40.ckpt: 100%"
          }
        },
        "b95efef721f64fd3b47362c87df84671": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fc5f30273c7844058b11018735142884",
            "max": 120982795,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_159dc92f5ab6405b8e03722fb723150f",
            "value": 120982795
          }
        },
        "fc66a2da8fa34aa6bbe4641b2cd8cf67": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f6cbfc2bb84248759841db6202eda09d",
            "placeholder": "​",
            "style": "IPY_MODEL_1957487960df4026b89e6443dd999a0d",
            "value": " 121M/121M [00:02&lt;00:00, 44.7MB/s]"
          }
        },
        "d54a288babe0456ba8dd989403496391": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e60ad04c9e90429387ef116097f9fdcc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5fa3e4a6304b46308e10fea28e0739e1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "fc5f30273c7844058b11018735142884": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "159dc92f5ab6405b8e03722fb723150f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f6cbfc2bb84248759841db6202eda09d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1957487960df4026b89e6443dd999a0d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}