{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Population Density Prediction: Multi-Resolution Analysis\n\nThis notebook tests SatCLIP's ability to predict population density at different spatial resolutions,\n**matching the methodology from Table 2 of the SatCLIP paper**.\n\n## Dataset\n**GPWv4** (Gridded Population of the World, Version 4)\n- 5 native resolutions: 30-sec (~1km), 2.5-min (~5km), 15-min (~28km), 30-min (~55km), 1-deg (~111km)\n- Years: 2000, 2005, 2010, 2015, 2020\n- Values: Population density (persons per kmÂ²)\n\n## Paper Methodology (Table 2)\n- **Scope**: Global (standard evaluation)\n- **Split**: 50% train / 50% test (random)\n- **Model**: MLP on frozen SatCLIP embeddings\n- **Target**: Log-transformed population density\n- **Metric**: RÂ² score\n- **Paper Results**: L=10 RÂ²=0.79, L=40 RÂ²=0.82\n\n## Available Models\n- **L=10**: 100 spherical harmonic features â†’ 256-dim embedding\n- **L=40**: 1600 spherical harmonic features â†’ 256-dim embedding\n- **Note**: No L=20 model exists on HuggingFace (only L=10 and L=40)\n\n## Tests in This Notebook\n1. **Paper Replication**: Global test matching Table 2 methodology\n2. **Multi-Resolution**: Compare L=10 vs L=40 at each GPW resolution\n3. **Regional Analysis**: Additional tests within constrained regions (exploratory)\n\n**Key Question**: Does L=40 advantage change with population data resolution?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Handle both Colab and local environments\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    !rm -rf sample_data .config satclip gpw_data 2>/dev/null\n",
    "    !git clone https://github.com/1hamzaiqbal/satclip.git\n",
    "    \n",
    "    # Download GPW data (we'll use a subset)\n",
    "    !pip install gdown --quiet\n",
    "    # Note: You'll need to upload the gpw_data folder or download from source\n",
    "    print(\"Please upload gpw_data folder with population TIF files\")\n",
    "\n",
    "!pip install lightning torchgeo huggingface_hub geopandas shapely requests --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import zipfile\n",
    "import io\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Path setup\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    sys.path.append('./satclip/satclip')\n",
    "    GPW_DIR = './gpw_data'\n",
    "else:\n",
    "    sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'satclip'))\n",
    "    GPW_DIR = './gpw_data'\n",
    "\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "from load import get_satclip\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load models\n",
    "print(\"Loading SatCLIP models...\")\n",
    "model_l10 = get_satclip(hf_hub_download(\"microsoft/SatCLIP-ViT16-L10\", \"satclip-vit16-l10.ckpt\"), device=device)\n",
    "model_l40 = get_satclip(hf_hub_download(\"microsoft/SatCLIP-ViT16-L40\", \"satclip-vit16-l40.ckpt\"), device=device)\n",
    "model_l10.eval()\n",
    "model_l40.eval()\n",
    "print(\"Models loaded!\")\n",
    "\n",
    "def get_embeddings(model, coords, batch_size=1000):\n",
    "    \"\"\"Get embeddings with batching for large coordinate sets.\"\"\"\n",
    "    all_emb = []\n",
    "    for i in range(0, len(coords), batch_size):\n",
    "        batch = coords[i:i+batch_size]\n",
    "        coords_tensor = torch.tensor(batch).double()\n",
    "        with torch.no_grad():\n",
    "            emb = model(coords_tensor.to(device)).cpu().numpy()\n",
    "        all_emb.append(emb)\n",
    "    return np.vstack(all_emb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Load and Explore GPW Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define available resolutions\n",
    "RESOLUTIONS = {\n",
    "    '1_deg': {'name': '1 degree', 'km': 111, 'grid': (360, 180)},\n",
    "    '30_min': {'name': '30 arc-min', 'km': 55, 'grid': (720, 360)},\n",
    "    '15_min': {'name': '15 arc-min', 'km': 28, 'grid': (1440, 720)},\n",
    "    '2pt5_min': {'name': '2.5 arc-min', 'km': 5, 'grid': (8640, 4320)},\n",
    "    # '30_sec': {'name': '30 arc-sec', 'km': 1, 'grid': (43200, 21600)},  # Too large\n",
    "}\n",
    "\n",
    "YEAR = 2020  # Use most recent year\n",
    "\n",
    "def load_gpw_raster(resolution, year=2020):\n",
    "    \"\"\"Load GPW population density raster at given resolution.\"\"\"\n",
    "    zip_file = f\"{GPW_DIR}/gpw-v4-population-density-rev11_{year}_{resolution}_tif.zip\"\n",
    "    \n",
    "    if not os.path.exists(zip_file):\n",
    "        print(f\"File not found: {zip_file}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Extract and read TIF\n",
    "    with zipfile.ZipFile(zip_file, 'r') as z:\n",
    "        tif_name = [n for n in z.namelist() if n.endswith('.tif')][0]\n",
    "        with z.open(tif_name) as f:\n",
    "            img = Image.open(io.BytesIO(f.read()))\n",
    "            data = np.array(img)\n",
    "    \n",
    "    # Create coordinate grids\n",
    "    height, width = data.shape\n",
    "    lon_step = 360 / width\n",
    "    lat_step = 180 / height\n",
    "    \n",
    "    lons = np.linspace(-180 + lon_step/2, 180 - lon_step/2, width)\n",
    "    lats = np.linspace(90 - lat_step/2, -90 + lat_step/2, height)\n",
    "    \n",
    "    return data, (lons, lats)\n",
    "\n",
    "# Test loading\n",
    "print(\"Testing data loading...\")\n",
    "for res_key, res_info in RESOLUTIONS.items():\n",
    "    data, coords = load_gpw_raster(res_key)\n",
    "    if data is not None:\n",
    "        valid = data > -1e30\n",
    "        print(f\"{res_info['name']:15} ({res_info['km']:>3}km): {data.shape} - {valid.sum():,} valid cells\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_from_raster(data, coords, n_samples=10000, seed=42, bounds=None):\n",
    "    \"\"\"\n",
    "    Sample random valid points from population raster.\n",
    "    \n",
    "    Args:\n",
    "        data: 2D array of population density\n",
    "        coords: (lons, lats) arrays\n",
    "        n_samples: number of samples\n",
    "        seed: random seed\n",
    "        bounds: optional (lon_min, lat_min, lon_max, lat_max) to constrain region\n",
    "    \n",
    "    Returns:\n",
    "        coords_arr: (N, 2) array of [lon, lat]\n",
    "        values: (N,) array of population density values\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    lons, lats = coords\n",
    "    \n",
    "    # Create valid mask\n",
    "    valid_mask = data > -1e30\n",
    "    \n",
    "    # Apply bounds if specified\n",
    "    if bounds is not None:\n",
    "        lon_min, lat_min, lon_max, lat_max = bounds\n",
    "        lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "        bounds_mask = (\n",
    "            (lon_grid >= lon_min) & (lon_grid <= lon_max) &\n",
    "            (lat_grid >= lat_min) & (lat_grid <= lat_max)\n",
    "        )\n",
    "        valid_mask = valid_mask & bounds_mask\n",
    "    \n",
    "    # Get valid indices\n",
    "    valid_idx = np.where(valid_mask)\n",
    "    n_valid = len(valid_idx[0])\n",
    "    \n",
    "    if n_valid < n_samples:\n",
    "        print(f\"Warning: Only {n_valid} valid cells, sampling all\")\n",
    "        sample_idx = np.arange(n_valid)\n",
    "    else:\n",
    "        sample_idx = np.random.choice(n_valid, n_samples, replace=False)\n",
    "    \n",
    "    # Get coordinates and values\n",
    "    row_idx = valid_idx[0][sample_idx]\n",
    "    col_idx = valid_idx[1][sample_idx]\n",
    "    \n",
    "    sample_lons = lons[col_idx]\n",
    "    sample_lats = lats[row_idx]\n",
    "    sample_values = data[row_idx, col_idx]\n",
    "    \n",
    "    coords_arr = np.stack([sample_lons, sample_lats], axis=1)\n",
    "    \n",
    "    return coords_arr, sample_values\n",
    "\n",
    "# Test sampling\n",
    "data_15min, coords_15min = load_gpw_raster('15_min')\n",
    "sample_coords, sample_vals = sample_from_raster(data_15min, coords_15min, n_samples=1000)\n",
    "print(f\"Sampled {len(sample_coords)} points\")\n",
    "print(f\"Coords range: lon [{sample_coords[:,0].min():.1f}, {sample_coords[:,0].max():.1f}], lat [{sample_coords[:,1].min():.1f}, {sample_coords[:,1].max():.1f}]\")\n",
    "print(f\"Pop density range: [{sample_vals.min():.1f}, {sample_vals.max():.1f}] persons/kmÂ²\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Multi-Resolution Population Prediction (Global)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def run_population_regression(coords, values, model_l10, model_l40, test_size=0.5, seed=42):\n    \"\"\"\n    Train and evaluate population regression for both L=10 and L=40.\n    \n    Matches SatCLIP paper methodology:\n    - 50/50 train/test split (paper uses 50%)\n    - Log-transformed population density\n    - MLP regressor on frozen embeddings\n    \n    Returns dict with RÂ², MSE for each model.\n    \"\"\"\n    # Log-transform population (paper uses \"logged Population Density\")\n    # Add small constant to handle zeros\n    y = np.log1p(values)\n    \n    # Get embeddings\n    emb_l10 = get_embeddings(model_l10, coords)\n    emb_l40 = get_embeddings(model_l40, coords)\n    \n    # Train/test split - Paper uses 50/50\n    X_train_l10, X_test_l10, y_train, y_test = train_test_split(\n        emb_l10, y, test_size=test_size, random_state=seed\n    )\n    X_train_l40, X_test_l40, _, _ = train_test_split(\n        emb_l40, y, test_size=test_size, random_state=seed\n    )\n    \n    # Scale features\n    scaler_l10 = StandardScaler()\n    scaler_l40 = StandardScaler()\n    X_train_l10 = scaler_l10.fit_transform(X_train_l10)\n    X_test_l10 = scaler_l10.transform(X_test_l10)\n    X_train_l40 = scaler_l40.fit_transform(X_train_l40)\n    X_test_l40 = scaler_l40.transform(X_test_l40)\n    \n    # Train MLP regressors (paper uses MLP with tuned hyperparameters)\n    mlp_l10 = MLPRegressor(hidden_layer_sizes=(128, 64), max_iter=500, \n                           random_state=seed, early_stopping=True, validation_fraction=0.1)\n    mlp_l40 = MLPRegressor(hidden_layer_sizes=(128, 64), max_iter=500,\n                           random_state=seed, early_stopping=True, validation_fraction=0.1)\n    \n    mlp_l10.fit(X_train_l10, y_train)\n    mlp_l40.fit(X_train_l40, y_train)\n    \n    # Predict and evaluate\n    pred_l10 = mlp_l10.predict(X_test_l10)\n    pred_l40 = mlp_l40.predict(X_test_l40)\n    \n    r2_l10 = r2_score(y_test, pred_l10)\n    r2_l40 = r2_score(y_test, pred_l40)\n    mse_l10 = mean_squared_error(y_test, pred_l10)\n    mse_l40 = mean_squared_error(y_test, pred_l40)\n    \n    return {\n        'r2_l10': r2_l10,\n        'r2_l40': r2_l40,\n        'mse_l10': mse_l10,\n        'mse_l40': mse_l40,\n        'n_train': len(y_train),\n        'n_test': len(y_test)\n    }"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"GLOBAL POPULATION REGRESSION AT DIFFERENT RESOLUTIONS\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "N_SAMPLES = 15000  # Samples per resolution\n",
    "\n",
    "global_results = []\n",
    "\n",
    "print(f\"\\n{'Resolution':>15} | {'~km':>6} | {'RÂ² L=10':>10} | {'RÂ² L=40':>10} | {'Î”':>8} | {'Winner'}\")\n",
    "print(\"-\" * 75)\n",
    "\n",
    "for res_key, res_info in RESOLUTIONS.items():\n",
    "    # Load data\n",
    "    data, coords = load_gpw_raster(res_key)\n",
    "    if data is None:\n",
    "        continue\n",
    "    \n",
    "    # Sample points\n",
    "    sample_coords, sample_vals = sample_from_raster(data, coords, n_samples=N_SAMPLES)\n",
    "    \n",
    "    # Run regression\n",
    "    results = run_population_regression(sample_coords, sample_vals, model_l10, model_l40)\n",
    "    \n",
    "    diff = results['r2_l40'] - results['r2_l10']\n",
    "    winner = \"L=40\" if diff > 0.01 else (\"L=10\" if diff < -0.01 else \"~Same\")\n",
    "    \n",
    "    print(f\"{res_info['name']:>15} | {res_info['km']:>5}km | {results['r2_l10']:>10.3f} | {results['r2_l40']:>10.3f} | {diff:>+7.3f} | {winner}\")\n",
    "    \n",
    "    global_results.append({\n",
    "        'resolution': res_info['name'],\n",
    "        'km': res_info['km'],\n",
    "        'r2_l10': results['r2_l10'],\n",
    "        'r2_l40': results['r2_l40'],\n",
    "        'diff': diff,\n",
    "        'n_samples': len(sample_coords)\n",
    "    })\n",
    "\n",
    "global_df = pd.DataFrame(global_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize global results\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# RÂ² by resolution\n",
    "ax = axes[0]\n",
    "x = range(len(global_df))\n",
    "width = 0.35\n",
    "ax.bar([i - width/2 for i in x], global_df['r2_l10'], width, label='L=10', color='steelblue')\n",
    "ax.bar([i + width/2 for i in x], global_df['r2_l40'], width, label='L=40', color='coral')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"{r}\\n({k}km)\" for r, k in zip(global_df['resolution'], global_df['km'])], fontsize=9)\n",
    "ax.set_ylabel('RÂ² Score')\n",
    "ax.set_title('Population Prediction RÂ² by Resolution (Global)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "# L=40 advantage\n",
    "ax = axes[1]\n",
    "colors = ['green' if d > 0 else 'red' for d in global_df['diff']]\n",
    "ax.bar(x, global_df['diff'], color=colors, alpha=0.7)\n",
    "ax.axhline(y=0, color='black', linewidth=1)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels([f\"{r}\\n({k}km)\" for r, k in zip(global_df['resolution'], global_df['km'])], fontsize=9)\n",
    "ax.set_ylabel('RÂ² Difference (L=40 - L=10)')\n",
    "ax.set_title('L=40 Advantage by Resolution')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('population_global_resolution.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Regional Analysis (Global vs Within-Country)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define regions to test\n",
    "REGIONS = {\n",
    "    'Global': None,  # No bounds\n",
    "    'USA': (-125, 24, -66, 50),\n",
    "    'Europe': (-10, 35, 40, 70),\n",
    "    'China': (73, 18, 135, 54),\n",
    "    'India': (68, 6, 98, 36),\n",
    "    'Brazil': (-74, -34, -34, 6),\n",
    "    'Africa': (-18, -35, 52, 37),\n",
    "}\n",
    "\n",
    "print(\"=\"*70)\n",
    "print(\"REGIONAL POPULATION REGRESSION (15-min resolution, ~28km)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Use 15-min resolution for regional tests\n",
    "data_15min, coords_15min = load_gpw_raster('15_min')\n",
    "\n",
    "regional_results = []\n",
    "\n",
    "print(f\"\\n{'Region':>12} | {'Samples':>8} | {'RÂ² L=10':>10} | {'RÂ² L=40':>10} | {'Î”':>8} | {'Winner'}\")\n",
    "print(\"-\" * 70)\n",
    "\n",
    "for region_name, bounds in REGIONS.items():\n",
    "    # Sample from region\n",
    "    sample_coords, sample_vals = sample_from_raster(\n",
    "        data_15min, coords_15min, n_samples=10000, bounds=bounds\n",
    "    )\n",
    "    \n",
    "    if len(sample_coords) < 100:\n",
    "        print(f\"{region_name:>12} | Too few samples\")\n",
    "        continue\n",
    "    \n",
    "    # Run regression\n",
    "    results = run_population_regression(sample_coords, sample_vals, model_l10, model_l40)\n",
    "    \n",
    "    diff = results['r2_l40'] - results['r2_l10']\n",
    "    winner = \"L=40\" if diff > 0.01 else (\"L=10\" if diff < -0.01 else \"~Same\")\n",
    "    \n",
    "    print(f\"{region_name:>12} | {len(sample_coords):>8} | {results['r2_l10']:>10.3f} | {results['r2_l40']:>10.3f} | {diff:>+7.3f} | {winner}\")\n",
    "    \n",
    "    regional_results.append({\n",
    "        'region': region_name,\n",
    "        'n_samples': len(sample_coords),\n",
    "        'r2_l10': results['r2_l10'],\n",
    "        'r2_l40': results['r2_l40'],\n",
    "        'diff': diff\n",
    "    })\n",
    "\n",
    "regional_df = pd.DataFrame(regional_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize regional results\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "\n",
    "x = range(len(regional_df))\n",
    "width = 0.35\n",
    "ax.bar([i - width/2 for i in x], regional_df['r2_l10'], width, label='L=10', color='steelblue')\n",
    "ax.bar([i + width/2 for i in x], regional_df['r2_l40'], width, label='L=40', color='coral')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(regional_df['region'])\n",
    "ax.set_ylabel('RÂ² Score')\n",
    "ax.set_title('Population Prediction RÂ² by Region (15-min / ~28km resolution)')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.set_ylim(0, 1)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('population_regional.png', dpi=150)\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nAverage RÂ² - L=10: {regional_df['r2_l10'].mean():.3f}, L=40: {regional_df['r2_l40'].mean():.3f}\")\n",
    "print(f\"Average advantage: {regional_df['diff'].mean():+.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Resolution Ã— Region Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"RESOLUTION Ã— REGION GRID SEARCH\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Test subset of resolutions and regions\n",
    "TEST_RESOLUTIONS = ['1_deg', '30_min', '15_min']  # Skip 2.5min for speed\n",
    "TEST_REGIONS = ['Global', 'USA', 'Europe', 'China']\n",
    "\n",
    "grid_results = []\n",
    "\n",
    "for res_key in TEST_RESOLUTIONS:\n",
    "    res_info = RESOLUTIONS[res_key]\n",
    "    data, coords = load_gpw_raster(res_key)\n",
    "    if data is None:\n",
    "        continue\n",
    "    \n",
    "    print(f\"\\n{res_info['name']} ({res_info['km']}km):\")\n",
    "    print(f\"  {'Region':>10} | {'RÂ² L=10':>8} | {'RÂ² L=40':>8} | {'Î”':>7}\")\n",
    "    print(\"  \" + \"-\" * 45)\n",
    "    \n",
    "    for region_name in TEST_REGIONS:\n",
    "        bounds = REGIONS[region_name]\n",
    "        \n",
    "        sample_coords, sample_vals = sample_from_raster(\n",
    "            data, coords, n_samples=8000, bounds=bounds\n",
    "        )\n",
    "        \n",
    "        if len(sample_coords) < 100:\n",
    "            continue\n",
    "        \n",
    "        results = run_population_regression(sample_coords, sample_vals, model_l10, model_l40)\n",
    "        diff = results['r2_l40'] - results['r2_l10']\n",
    "        \n",
    "        print(f\"  {region_name:>10} | {results['r2_l10']:>8.3f} | {results['r2_l40']:>8.3f} | {diff:>+6.3f}\")\n",
    "        \n",
    "        grid_results.append({\n",
    "            'resolution': res_info['name'],\n",
    "            'km': res_info['km'],\n",
    "            'region': region_name,\n",
    "            'r2_l10': results['r2_l10'],\n",
    "            'r2_l40': results['r2_l40'],\n",
    "            'diff': diff\n",
    "        })\n",
    "\n",
    "grid_df = pd.DataFrame(grid_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Heatmap of L=40 advantage\n",
    "if len(grid_df) > 0:\n",
    "    pivot = grid_df.pivot(index='region', columns='resolution', values='diff')\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(10, 6))\n",
    "    im = ax.imshow(pivot.values, cmap='RdYlGn', aspect='auto', vmin=-0.1, vmax=0.1)\n",
    "    \n",
    "    ax.set_xticks(range(len(pivot.columns)))\n",
    "    ax.set_xticklabels(pivot.columns)\n",
    "    ax.set_yticks(range(len(pivot.index)))\n",
    "    ax.set_yticklabels(pivot.index)\n",
    "    ax.set_xlabel('Resolution')\n",
    "    ax.set_ylabel('Region')\n",
    "    ax.set_title('L=40 - L=10 RÂ² Advantage (Population Prediction)')\n",
    "    \n",
    "    # Add values\n",
    "    for i in range(len(pivot.index)):\n",
    "        for j in range(len(pivot.columns)):\n",
    "            val = pivot.values[i, j]\n",
    "            if not np.isnan(val):\n",
    "                ax.text(j, i, f\"{val:+.3f}\", ha='center', va='center', fontsize=10)\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, label='RÂ² Difference')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('population_grid_heatmap.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Comparison with SatCLIP Paper (Table 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"COMPARISON WITH SATCLIP PAPER (Table 2)\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# Paper reports (ResNet50, not ViT16, but should be similar)\n",
    "paper_results = {\n",
    "    'L=10': 0.79,\n",
    "    'L=40': 0.82,\n",
    "}\n",
    "\n",
    "print(\"\\nSatCLIP Paper (Table 2) - Population Density:\")\n",
    "print(f\"  L=10: RÂ² = {paper_results['L=10']:.2f}\")\n",
    "print(f\"  L=40: RÂ² = {paper_results['L=40']:.2f}\")\n",
    "print(f\"  Î”: {paper_results['L=40'] - paper_results['L=10']:+.2f}\")\n",
    "\n",
    "print(\"\\nOur Results (ViT16, Global, best resolution):\")\n",
    "if len(global_df) > 0:\n",
    "    best_row = global_df.loc[global_df['r2_l10'].idxmax()]\n",
    "    print(f\"  Resolution: {best_row['resolution']} ({best_row['km']}km)\")\n",
    "    print(f\"  L=10: RÂ² = {best_row['r2_l10']:.2f}\")\n",
    "    print(f\"  L=40: RÂ² = {best_row['r2_l40']:.2f}\")\n",
    "    print(f\"  Î”: {best_row['diff']:+.2f}\")\n",
    "\n",
    "print(\"\\nNote: Paper uses ResNet50 backbone, we use ViT16. Results may differ slightly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"POPULATION RESOLUTION ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“Š GLOBAL RESULTS BY RESOLUTION:\")\n",
    "print(\"-\" * 50)\n",
    "if len(global_df) > 0:\n",
    "    for _, row in global_df.iterrows():\n",
    "        winner = \"L=40 âœ“\" if row['diff'] > 0.01 else (\"L=10 âœ“\" if row['diff'] < -0.01 else \"~Same\")\n",
    "        print(f\"  {row['resolution']:>15} ({row['km']:>3}km): L=10={row['r2_l10']:.3f}, L=40={row['r2_l40']:.3f} ({winner})\")\n",
    "\n",
    "print(\"\\nðŸ“Š REGIONAL RESULTS (15-min / ~28km):\")\n",
    "print(\"-\" * 50)\n",
    "if len(regional_df) > 0:\n",
    "    for _, row in regional_df.iterrows():\n",
    "        winner = \"L=40 âœ“\" if row['diff'] > 0.01 else (\"L=10 âœ“\" if row['diff'] < -0.01 else \"~Same\")\n",
    "        print(f\"  {row['region']:>12}: L=10={row['r2_l10']:.3f}, L=40={row['r2_l40']:.3f} ({winner})\")\n",
    "\n",
    "print(\"\\nðŸ”¬ KEY FINDINGS:\")\n",
    "print(\"-\" * 50)\n",
    "print(\"\"\"\n",
    "1. RESOLUTION EFFECT:\n",
    "   - [Fill in based on results]\n",
    "   - Does L=40 advantage change with resolution?\n",
    "\n",
    "2. REGIONAL EFFECT:\n",
    "   - [Fill in based on results]\n",
    "   - Does constraining to regions help either model?\n",
    "\n",
    "3. COMPARISON TO PAPER:\n",
    "   - Paper reports L=10: 0.79, L=40: 0.82 (RÂ²)\n",
    "   - Our results: [Fill in]\n",
    "\n",
    "4. EFFECTIVE RESOLUTION:\n",
    "   - At what resolution does prediction become unreliable?\n",
    "   - Does L=40 maintain advantage at finer scales?\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "import json\n",
    "\n",
    "all_results = {\n",
    "    'global': global_df.to_dict('records') if len(global_df) > 0 else [],\n",
    "    'regional': regional_df.to_dict('records') if len(regional_df) > 0 else [],\n",
    "    'grid': grid_df.to_dict('records') if len(grid_df) > 0 else [],\n",
    "    'paper_comparison': paper_results\n",
    "}\n",
    "\n",
    "with open('population_resolution_results.json', 'w') as f:\n",
    "    json.dump(all_results, f, indent=2)\n",
    "\n",
    "print(\"âœ… Results saved to: population_resolution_results.json\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}