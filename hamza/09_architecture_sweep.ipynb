{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# Architecture Sweep: MLP Bottlenecks in L=10 vs L=40\n",
    "\n",
    "**Goal**: Understand if MLP architecture creates bottlenecks that affect L=10 vs L=40 differently.\n",
    "\n",
    "## Hypothesis\n",
    "- L=40 has 1600 SH features → 256-dim embedding (16x compression)\n",
    "- L=10 has 100 SH features → 256-dim embedding (2.5x expansion)\n",
    "- Maybe L=40's richer embeddings need more MLP capacity to exploit?\n",
    "- Or maybe L=10's simpler features are easier to learn from?\n",
    "\n",
    "## Sweep Variables\n",
    "1. **Coverage**: Global, USA, Europe, China, Brazil, Africa + coverage sizes\n",
    "2. **Models**: L=10 vs L=40\n",
    "3. **Architectures**:\n",
    "   - Linear (Ridge) - baseline\n",
    "   - Tiny MLP (32)\n",
    "   - Small MLP (64)\n",
    "   - Medium MLP (128, 64) - default\n",
    "   - Wide MLP (512, 256)\n",
    "   - Deep MLP (256, 128, 64, 32)\n",
    "   - Very Deep (256, 256, 128, 128, 64, 64)\n",
    "\n",
    "## Key Questions\n",
    "- Does L=40 benefit more from deeper/wider networks?\n",
    "- Is there an architecture where L=10 matches L=40 regionally?\n",
    "- Does the optimal architecture differ by geographic scale?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    !rm -rf sample_data .config satclip gpw_data 2>/dev/null\n",
    "    !git clone https://github.com/1hamzaiqbal/satclip.git\n",
    "    !pip install lightning torchgeo huggingface_hub geopandas shapely requests rasterio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and extract GPW data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "GPW_DIR = './gpw_data'\n",
    "os.makedirs(GPW_DIR, exist_ok=True)\n",
    "\n",
    "SOURCE_ZIP_PATH = '/content/drive/MyDrive/grad/learned_activations/dataverse_files.zip'\n",
    "\n",
    "print(\"Extracting GPW data...\")\n",
    "with zipfile.ZipFile(SOURCE_ZIP_PATH, 'r') as z:\n",
    "    z.extractall(GPW_DIR)\n",
    "\n",
    "# Extract resolution TIFs\n",
    "RESOLUTIONS_TO_EXTRACT = ['15_min', '2pt5_min']  # Use 2 resolutions for speed\n",
    "YEAR = 2020\n",
    "\n",
    "for res in RESOLUTIONS_TO_EXTRACT:\n",
    "    zip_name = f\"gpw-v4-population-density-rev11_{YEAR}_{res}_tif.zip\"\n",
    "    zip_path = os.path.join(GPW_DIR, zip_name)\n",
    "    if os.path.exists(zip_path):\n",
    "        with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "            z.extractall(GPW_DIR)\n",
    "        print(f\"  Extracted {zip_name}\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    sys.path.append('./satclip/satclip')\n",
    "    GPW_DIR = './gpw_data'\n",
    "else:\n",
    "    sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'satclip'))\n",
    "    GPW_DIR = './gpw_data'\n",
    "\n",
    "import torch\n",
    "from huggingface_hub import hf_hub_download\n",
    "from load import get_satclip\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import time\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load models\n",
    "print(\"Loading SatCLIP models...\")\n",
    "model_l10 = get_satclip(hf_hub_download(\"microsoft/SatCLIP-ViT16-L10\", \"satclip-vit16-l10.ckpt\"), device=device)\n",
    "model_l40 = get_satclip(hf_hub_download(\"microsoft/SatCLIP-ViT16-L40\", \"satclip-vit16-l40.ckpt\"), device=device)\n",
    "model_l10.eval()\n",
    "model_l40.eval()\n",
    "print(\"Models loaded!\")\n",
    "\n",
    "def get_embeddings(model, coords, batch_size=1000):\n",
    "    \"\"\"Get embeddings with batching.\"\"\"\n",
    "    all_emb = []\n",
    "    for i in range(0, len(coords), batch_size):\n",
    "        batch = coords[i:i+batch_size]\n",
    "        coords_tensor = torch.tensor(batch).double()\n",
    "        with torch.no_grad():\n",
    "            emb = model(coords_tensor.to(device)).cpu().numpy()\n",
    "        all_emb.append(emb)\n",
    "    return np.vstack(all_emb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data loading functions\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "try:\n",
    "    import rasterio\n",
    "    HAS_RASTERIO = True\n",
    "except ImportError:\n",
    "    HAS_RASTERIO = False\n",
    "\n",
    "RESOLUTIONS = {\n",
    "    '15_min': {'name': '15 arc-min', 'km': 28},\n",
    "    '2pt5_min': {'name': '2.5 arc-min', 'km': 5},\n",
    "}\n",
    "\n",
    "def load_gpw_raster(resolution, year=2020):\n",
    "    \"\"\"Load GPW population density raster.\"\"\"\n",
    "    tif_file = f\"{GPW_DIR}/gpw_v4_population_density_rev11_{year}_{resolution}.tif\"\n",
    "    if not os.path.exists(tif_file):\n",
    "        print(f\"File not found: {tif_file}\")\n",
    "        return None, None\n",
    "    \n",
    "    if HAS_RASTERIO:\n",
    "        with rasterio.open(tif_file) as src:\n",
    "            data = src.read(1)\n",
    "            height, width = data.shape\n",
    "    else:\n",
    "        img = Image.open(tif_file)\n",
    "        data = np.array(img)\n",
    "        height, width = data.shape\n",
    "    \n",
    "    lon_step = 360 / width\n",
    "    lat_step = 180 / height\n",
    "    lons = np.linspace(-180 + lon_step/2, 180 - lon_step/2, width)\n",
    "    lats = np.linspace(90 - lat_step/2, -90 + lat_step/2, height)\n",
    "    \n",
    "    return data, (lons, lats)\n",
    "\n",
    "def sample_from_raster(data, coords, n_samples=10000, seed=42, bounds=None):\n",
    "    \"\"\"Sample random valid points from population raster.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    lons, lats = coords\n",
    "    valid_mask = data > -1e30\n",
    "    \n",
    "    if bounds is not None:\n",
    "        lon_min, lat_min, lon_max, lat_max = bounds\n",
    "        lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "        bounds_mask = (\n",
    "            (lon_grid >= lon_min) & (lon_grid <= lon_max) &\n",
    "            (lat_grid >= lat_min) & (lat_grid <= lat_max)\n",
    "        )\n",
    "        valid_mask = valid_mask & bounds_mask\n",
    "    \n",
    "    valid_idx = np.where(valid_mask)\n",
    "    n_valid = len(valid_idx[0])\n",
    "    \n",
    "    if n_valid < n_samples:\n",
    "        sample_idx = np.arange(n_valid)\n",
    "    else:\n",
    "        sample_idx = np.random.choice(n_valid, n_samples, replace=False)\n",
    "    \n",
    "    row_idx = valid_idx[0][sample_idx]\n",
    "    col_idx = valid_idx[1][sample_idx]\n",
    "    \n",
    "    sample_lons = lons[col_idx]\n",
    "    sample_lats = lats[row_idx]\n",
    "    sample_values = data[row_idx, col_idx]\n",
    "    \n",
    "    coords_arr = np.stack([sample_lons, sample_lats], axis=1)\n",
    "    return coords_arr, sample_values\n",
    "\n",
    "# Load data\n",
    "print(\"Loading population data...\")\n",
    "data_15min, coords_15min = load_gpw_raster('15_min')\n",
    "print(f\"  15-min: {data_15min.shape if data_15min is not None else 'Not found'}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-5",
   "metadata": {},
   "source": [
    "---\n",
    "## Define Architectures to Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define all architectures to test\n",
    "ARCHITECTURES = {\n",
    "    # Linear baselines\n",
    "    'Linear (Ridge)': {'type': 'ridge', 'alpha': 1.0},\n",
    "    'Linear (Ridge strong)': {'type': 'ridge', 'alpha': 100.0},\n",
    "    \n",
    "    # MLP variants - varying depth and width\n",
    "    'MLP Tiny (32)': {'type': 'mlp', 'hidden': (32,)},\n",
    "    'MLP Small (64)': {'type': 'mlp', 'hidden': (64,)},\n",
    "    'MLP Medium (128,64)': {'type': 'mlp', 'hidden': (128, 64)},  # Default\n",
    "    'MLP Wide (256,128)': {'type': 'mlp', 'hidden': (256, 128)},\n",
    "    'MLP Wider (512,256)': {'type': 'mlp', 'hidden': (512, 256)},\n",
    "    'MLP Deep (128,128,64,64)': {'type': 'mlp', 'hidden': (128, 128, 64, 64)},\n",
    "    'MLP Very Deep (256,256,128,128,64,64)': {'type': 'mlp', 'hidden': (256, 256, 128, 128, 64, 64)},\n",
    "    'MLP Bottleneck (256,32,256)': {'type': 'mlp', 'hidden': (256, 32, 256)},  # Compression test\n",
    "    \n",
    "    # Alternative regressors\n",
    "    'KNN (k=10)': {'type': 'knn', 'n_neighbors': 10},\n",
    "    'KNN (k=50)': {'type': 'knn', 'n_neighbors': 50},\n",
    "    'Random Forest': {'type': 'rf', 'n_estimators': 100, 'max_depth': 10},\n",
    "    'Gradient Boosting': {'type': 'gb', 'n_estimators': 100, 'max_depth': 5},\n",
    "}\n",
    "\n",
    "# Coverages to test\n",
    "COVERAGES = {\n",
    "    'Global': None,\n",
    "    'USA': (-125, 24, -66, 50),\n",
    "    'Europe': (-10, 35, 40, 70),\n",
    "    'China': (73, 18, 135, 54),\n",
    "    'Brazil': (-74, -34, -34, 6),\n",
    "    'Africa': (-18, -35, 52, 37),\n",
    "}\n",
    "\n",
    "print(f\"Architectures to test: {len(ARCHITECTURES)}\")\n",
    "print(f\"Coverages to test: {len(COVERAGES)}\")\n",
    "print(f\"Models: L=10, L=40\")\n",
    "print(f\"Total combinations: {len(ARCHITECTURES) * len(COVERAGES) * 2}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_regressor(arch_config, seed=42):\n",
    "    \"\"\"Create a regressor based on architecture config.\"\"\"\n",
    "    arch_type = arch_config['type']\n",
    "    \n",
    "    if arch_type == 'ridge':\n",
    "        return Ridge(alpha=arch_config.get('alpha', 1.0))\n",
    "    \n",
    "    elif arch_type == 'mlp':\n",
    "        return MLPRegressor(\n",
    "            hidden_layer_sizes=arch_config['hidden'],\n",
    "            max_iter=500,\n",
    "            random_state=seed,\n",
    "            early_stopping=True,\n",
    "            validation_fraction=0.1,\n",
    "            n_iter_no_change=10\n",
    "        )\n",
    "    \n",
    "    elif arch_type == 'knn':\n",
    "        return KNeighborsRegressor(n_neighbors=arch_config.get('n_neighbors', 10))\n",
    "    \n",
    "    elif arch_type == 'rf':\n",
    "        return RandomForestRegressor(\n",
    "            n_estimators=arch_config.get('n_estimators', 100),\n",
    "            max_depth=arch_config.get('max_depth', 10),\n",
    "            random_state=seed,\n",
    "            n_jobs=-1\n",
    "        )\n",
    "    \n",
    "    elif arch_type == 'gb':\n",
    "        return GradientBoostingRegressor(\n",
    "            n_estimators=arch_config.get('n_estimators', 100),\n",
    "            max_depth=arch_config.get('max_depth', 5),\n",
    "            random_state=seed\n",
    "        )\n",
    "    \n",
    "    else:\n",
    "        raise ValueError(f\"Unknown architecture type: {arch_type}\")\n",
    "\n",
    "\n",
    "def run_architecture_test(coords, values, model_l10, model_l40, arch_config, test_size=0.5, seed=42):\n",
    "    \"\"\"Run regression test with specified architecture for both L=10 and L=40.\"\"\"\n",
    "    # Log-transform population\n",
    "    y = np.log1p(values)\n",
    "    \n",
    "    # Get embeddings\n",
    "    emb_l10 = get_embeddings(model_l10, coords)\n",
    "    emb_l40 = get_embeddings(model_l40, coords)\n",
    "    \n",
    "    # Train/test split\n",
    "    X_train_l10, X_test_l10, y_train, y_test = train_test_split(\n",
    "        emb_l10, y, test_size=test_size, random_state=seed\n",
    "    )\n",
    "    X_train_l40, X_test_l40, _, _ = train_test_split(\n",
    "        emb_l40, y, test_size=test_size, random_state=seed\n",
    "    )\n",
    "    \n",
    "    # Scale features\n",
    "    scaler_l10, scaler_l40 = StandardScaler(), StandardScaler()\n",
    "    X_train_l10 = scaler_l10.fit_transform(X_train_l10)\n",
    "    X_test_l10 = scaler_l10.transform(X_test_l10)\n",
    "    X_train_l40 = scaler_l40.fit_transform(X_train_l40)\n",
    "    X_test_l40 = scaler_l40.transform(X_test_l40)\n",
    "    \n",
    "    # Create and train regressors\n",
    "    reg_l10 = create_regressor(arch_config, seed)\n",
    "    reg_l40 = create_regressor(arch_config, seed)\n",
    "    \n",
    "    start = time.time()\n",
    "    reg_l10.fit(X_train_l10, y_train)\n",
    "    reg_l40.fit(X_train_l40, y_train)\n",
    "    train_time = time.time() - start\n",
    "    \n",
    "    # Evaluate\n",
    "    r2_l10 = r2_score(y_test, reg_l10.predict(X_test_l10))\n",
    "    r2_l40 = r2_score(y_test, reg_l40.predict(X_test_l40))\n",
    "    \n",
    "    return {\n",
    "        'r2_l10': r2_l10,\n",
    "        'r2_l40': r2_l40,\n",
    "        'diff': r2_l40 - r2_l10,\n",
    "        'train_time': train_time\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "## Run Full Sweep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*90)\n",
    "print(\"ARCHITECTURE SWEEP: Coverage x Model x Architecture\")\n",
    "print(\"=\"*90)\n",
    "\n",
    "N_SAMPLES = 8000\n",
    "all_results = []\n",
    "\n",
    "for coverage_name, bounds in COVERAGES.items():\n",
    "    print(f\"\\n{'─'*90}\")\n",
    "    print(f\"Coverage: {coverage_name}\")\n",
    "    print(f\"{'─'*90}\")\n",
    "    \n",
    "    # Sample data for this coverage\n",
    "    sample_coords, sample_vals = sample_from_raster(\n",
    "        data_15min, coords_15min, n_samples=N_SAMPLES, bounds=bounds, seed=42\n",
    "    )\n",
    "    \n",
    "    if len(sample_coords) < 500:\n",
    "        print(f\"  Skipping - only {len(sample_coords)} samples\")\n",
    "        continue\n",
    "    \n",
    "    print(f\"  Samples: {len(sample_coords)}\")\n",
    "    print(f\"\\n  {'Architecture':<35} | {'L=10':>7} | {'L=40':>7} | {'Δ':>8} | {'Time':>6}\")\n",
    "    print(f\"  {'-'*75}\")\n",
    "    \n",
    "    # Get embeddings once (reuse across architectures)\n",
    "    emb_l10 = get_embeddings(model_l10, sample_coords)\n",
    "    emb_l40 = get_embeddings(model_l40, sample_coords)\n",
    "    y = np.log1p(sample_vals)\n",
    "    \n",
    "    # Split once\n",
    "    X_train_l10, X_test_l10, y_train, y_test = train_test_split(\n",
    "        emb_l10, y, test_size=0.5, random_state=42\n",
    "    )\n",
    "    X_train_l40, X_test_l40, _, _ = train_test_split(\n",
    "        emb_l40, y, test_size=0.5, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Scale\n",
    "    scaler_l10, scaler_l40 = StandardScaler(), StandardScaler()\n",
    "    X_train_l10_scaled = scaler_l10.fit_transform(X_train_l10)\n",
    "    X_test_l10_scaled = scaler_l10.transform(X_test_l10)\n",
    "    X_train_l40_scaled = scaler_l40.fit_transform(X_train_l40)\n",
    "    X_test_l40_scaled = scaler_l40.transform(X_test_l40)\n",
    "    \n",
    "    for arch_name, arch_config in ARCHITECTURES.items():\n",
    "        try:\n",
    "            # Create regressors\n",
    "            reg_l10 = create_regressor(arch_config, seed=42)\n",
    "            reg_l40 = create_regressor(arch_config, seed=42)\n",
    "            \n",
    "            # Train\n",
    "            start = time.time()\n",
    "            reg_l10.fit(X_train_l10_scaled, y_train)\n",
    "            reg_l40.fit(X_train_l40_scaled, y_train)\n",
    "            train_time = time.time() - start\n",
    "            \n",
    "            # Evaluate\n",
    "            r2_l10 = r2_score(y_test, reg_l10.predict(X_test_l10_scaled))\n",
    "            r2_l40 = r2_score(y_test, reg_l40.predict(X_test_l40_scaled))\n",
    "            diff = r2_l40 - r2_l10\n",
    "            \n",
    "            winner = \"L=40\" if diff > 0.02 else (\"L=10\" if diff < -0.02 else \"~\")\n",
    "            print(f\"  {arch_name:<35} | {r2_l10:>7.3f} | {r2_l40:>7.3f} | {diff:>+7.3f} | {train_time:>5.1f}s\")\n",
    "            \n",
    "            all_results.append({\n",
    "                'coverage': coverage_name,\n",
    "                'architecture': arch_name,\n",
    "                'arch_type': arch_config['type'],\n",
    "                'r2_l10': r2_l10,\n",
    "                'r2_l40': r2_l40,\n",
    "                'diff': diff,\n",
    "                'train_time': train_time,\n",
    "                'n_samples': len(sample_coords)\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"  {arch_name:<35} | ERROR: {str(e)[:30]}\")\n",
    "\n",
    "results_df = pd.DataFrame(all_results)\n",
    "print(f\"\\n\\nTotal experiments: {len(results_df)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "## Analysis: Does Architecture Affect L=40 Advantage?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pivot table: Coverage x Architecture showing L=40 advantage\n",
    "print(\"=\"*80)\n",
    "print(\"L=40 ADVANTAGE (R² diff) BY COVERAGE x ARCHITECTURE\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    pivot = results_df.pivot(index='architecture', columns='coverage', values='diff')\n",
    "    \n",
    "    # Reorder columns\n",
    "    col_order = ['Global', 'USA', 'Europe', 'China', 'Brazil', 'Africa']\n",
    "    pivot = pivot[[c for c in col_order if c in pivot.columns]]\n",
    "    \n",
    "    print(\"\\n\" + pivot.round(3).to_string())\n",
    "    \n",
    "    # Summary stats\n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"SUMMARY BY ARCHITECTURE (avg L=40 advantage across coverages):\")\n",
    "    arch_avg = results_df.groupby('architecture')['diff'].mean().sort_values(ascending=False)\n",
    "    for arch, avg in arch_avg.items():\n",
    "        print(f\"  {arch:<40}: {avg:+.3f}\")\n",
    "    \n",
    "    print(\"\\n\" + \"-\"*60)\n",
    "    print(\"SUMMARY BY COVERAGE (avg L=40 advantage across architectures):\")\n",
    "    cov_avg = results_df.groupby('coverage')['diff'].mean().sort_values(ascending=False)\n",
    "    for cov, avg in cov_avg.items():\n",
    "        print(f\"  {cov:<20}: {avg:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "if len(results_df) > 0:\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "    \n",
    "    # 1. Heatmap of L=40 advantage\n",
    "    ax = axes[0, 0]\n",
    "    pivot = results_df.pivot(index='architecture', columns='coverage', values='diff')\n",
    "    col_order = ['Global', 'USA', 'Europe', 'China', 'Brazil', 'Africa']\n",
    "    pivot = pivot[[c for c in col_order if c in pivot.columns]]\n",
    "    \n",
    "    im = ax.imshow(pivot.values, cmap='RdYlGn', aspect='auto', vmin=-0.05, vmax=0.10)\n",
    "    ax.set_xticks(range(len(pivot.columns)))\n",
    "    ax.set_xticklabels(pivot.columns, rotation=45, ha='right')\n",
    "    ax.set_yticks(range(len(pivot.index)))\n",
    "    ax.set_yticklabels(pivot.index, fontsize=8)\n",
    "    ax.set_title('L=40 Advantage by Architecture x Coverage')\n",
    "    plt.colorbar(im, ax=ax, label='L=40 - L=10 R²')\n",
    "    \n",
    "    # 2. L=40 advantage by architecture (global vs regional)\n",
    "    ax = axes[0, 1]\n",
    "    global_results = results_df[results_df['coverage'] == 'Global'].set_index('architecture')['diff']\n",
    "    regional_results = results_df[results_df['coverage'] != 'Global'].groupby('architecture')['diff'].mean()\n",
    "    \n",
    "    x = range(len(global_results))\n",
    "    width = 0.35\n",
    "    ax.bar([i - width/2 for i in x], global_results.values, width, label='Global', color='steelblue')\n",
    "    ax.bar([i + width/2 for i in x], regional_results.reindex(global_results.index).values, width, label='Regional (avg)', color='coral')\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(global_results.index, rotation=45, ha='right', fontsize=7)\n",
    "    ax.axhline(y=0, color='black', linewidth=1)\n",
    "    ax.set_ylabel('L=40 Advantage')\n",
    "    ax.set_title('Global vs Regional L=40 Advantage by Architecture')\n",
    "    ax.legend()\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # 3. Best architecture for each coverage\n",
    "    ax = axes[1, 0]\n",
    "    best_arch = results_df.loc[results_df.groupby('coverage')['r2_l40'].idxmax()]\n",
    "    ax.barh(range(len(best_arch)), best_arch['r2_l40'], color='coral', alpha=0.7, label='L=40')\n",
    "    ax.barh(range(len(best_arch)), best_arch['r2_l10'], color='steelblue', alpha=0.7, label='L=10')\n",
    "    ax.set_yticks(range(len(best_arch)))\n",
    "    ax.set_yticklabels([f\"{row['coverage']}\\n({row['architecture'][:20]})\" for _, row in best_arch.iterrows()], fontsize=8)\n",
    "    ax.set_xlabel('R² Score')\n",
    "    ax.set_title('Best Architecture per Coverage')\n",
    "    ax.legend()\n",
    "    ax.set_xlim(0, 1)\n",
    "    \n",
    "    # 4. MLP depth/width effect\n",
    "    ax = axes[1, 1]\n",
    "    mlp_results = results_df[results_df['arch_type'] == 'mlp'].copy()\n",
    "    if len(mlp_results) > 0:\n",
    "        # Group by coverage, show MLP variants\n",
    "        for cov in ['Global', 'USA', 'Europe']:\n",
    "            cov_data = mlp_results[mlp_results['coverage'] == cov]\n",
    "            if len(cov_data) > 0:\n",
    "                ax.plot(range(len(cov_data)), cov_data['diff'].values, 'o-', label=cov, markersize=8)\n",
    "        ax.set_xticks(range(len(cov_data)))\n",
    "        ax.set_xticklabels(cov_data['architecture'].values, rotation=45, ha='right', fontsize=7)\n",
    "        ax.axhline(y=0, color='black', linestyle='--', alpha=0.5)\n",
    "        ax.set_ylabel('L=40 Advantage')\n",
    "        ax.set_title('MLP Variants: L=40 Advantage by Coverage')\n",
    "        ax.legend()\n",
    "        ax.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('architecture_sweep.png', dpi=150)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## Key Questions Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"KEY FINDINGS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    # Q1: Does L=40 benefit more from deeper/wider networks?\n",
    "    print(\"\\n1. DOES L=40 BENEFIT MORE FROM DEEPER/WIDER NETWORKS?\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    mlp_results = results_df[results_df['arch_type'] == 'mlp']\n",
    "    if len(mlp_results) > 0:\n",
    "        # Compare tiny vs deep for L=40\n",
    "        for cov in ['Global', 'USA', 'Europe']:\n",
    "            cov_mlp = mlp_results[mlp_results['coverage'] == cov]\n",
    "            if len(cov_mlp) > 0:\n",
    "                tiny = cov_mlp[cov_mlp['architecture'].str.contains('Tiny')]\n",
    "                deep = cov_mlp[cov_mlp['architecture'].str.contains('Very Deep')]\n",
    "                if len(tiny) > 0 and len(deep) > 0:\n",
    "                    l40_improvement = deep['r2_l40'].values[0] - tiny['r2_l40'].values[0]\n",
    "                    l10_improvement = deep['r2_l10'].values[0] - tiny['r2_l10'].values[0]\n",
    "                    print(f\"  {cov}: Tiny→Deep improvement: L=10 {l10_improvement:+.3f}, L=40 {l40_improvement:+.3f}\")\n",
    "    \n",
    "    # Q2: Is there an architecture where L=10 matches L=40 regionally?\n",
    "    print(\"\\n2. CAN L=10 MATCH L=40 WITH BETTER ARCHITECTURE (REGIONAL)?\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    regional = results_df[results_df['coverage'] != 'Global']\n",
    "    if len(regional) > 0:\n",
    "        # Find best L=10 architecture per coverage\n",
    "        for cov in regional['coverage'].unique():\n",
    "            cov_data = regional[regional['coverage'] == cov]\n",
    "            best_l10 = cov_data.loc[cov_data['r2_l10'].idxmax()]\n",
    "            worst_l40 = cov_data.loc[cov_data['r2_l40'].idxmin()]\n",
    "            \n",
    "            print(f\"  {cov}:\")\n",
    "            print(f\"    Best L=10: {best_l10['r2_l10']:.3f} ({best_l10['architecture'][:25]})\")\n",
    "            print(f\"    Worst L=40: {worst_l40['r2_l40']:.3f} ({worst_l40['architecture'][:25]})\")\n",
    "            if best_l10['r2_l10'] > worst_l40['r2_l40']:\n",
    "                print(f\"    → L=10 CAN beat L=40 with right architecture!\")\n",
    "            else:\n",
    "                print(f\"    → L=40 still wins even with worst architecture\")\n",
    "    \n",
    "    # Q3: Does optimal architecture differ by scale?\n",
    "    print(\"\\n3. DOES OPTIMAL ARCHITECTURE DIFFER BY GEOGRAPHIC SCALE?\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    for cov in results_df['coverage'].unique():\n",
    "        cov_data = results_df[results_df['coverage'] == cov]\n",
    "        best_l10_arch = cov_data.loc[cov_data['r2_l10'].idxmax(), 'architecture']\n",
    "        best_l40_arch = cov_data.loc[cov_data['r2_l40'].idxmax(), 'architecture']\n",
    "        print(f\"  {cov}:\")\n",
    "        print(f\"    Best for L=10: {best_l10_arch}\")\n",
    "        print(f\"    Best for L=40: {best_l40_arch}\")\n",
    "    \n",
    "    # Q4: Linear vs MLP - does the gap change?\n",
    "    print(\"\\n4. LINEAR vs MLP: DOES THE L=40 ADVANTAGE CHANGE?\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    linear = results_df[results_df['arch_type'] == 'ridge']\n",
    "    mlp_med = results_df[results_df['architecture'] == 'MLP Medium (128,64)']\n",
    "    \n",
    "    for cov in results_df['coverage'].unique():\n",
    "        lin_cov = linear[linear['coverage'] == cov]\n",
    "        mlp_cov = mlp_med[mlp_med['coverage'] == cov]\n",
    "        if len(lin_cov) > 0 and len(mlp_cov) > 0:\n",
    "            lin_diff = lin_cov['diff'].values[0]\n",
    "            mlp_diff = mlp_cov['diff'].values[0]\n",
    "            print(f\"  {cov}: Linear Δ={lin_diff:+.3f}, MLP Δ={mlp_diff:+.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "if len(results_df) > 0:\n",
    "    results_df.to_csv('architecture_sweep_results.csv', index=False)\n",
    "    print(\"Saved to architecture_sweep_results.csv\")\n",
    "    \n",
    "    # Also save summary\n",
    "    summary = {\n",
    "        'by_architecture': results_df.groupby('architecture')['diff'].mean().to_dict(),\n",
    "        'by_coverage': results_df.groupby('coverage')['diff'].mean().to_dict(),\n",
    "        'best_per_coverage_l10': results_df.loc[results_df.groupby('coverage')['r2_l10'].idxmax()][['coverage', 'architecture', 'r2_l10']].to_dict('records'),\n",
    "        'best_per_coverage_l40': results_df.loc[results_df.groupby('coverage')['r2_l40'].idxmax()][['coverage', 'architecture', 'r2_l40']].to_dict('records'),\n",
    "    }\n",
    "    \n",
    "    import json\n",
    "    with open('architecture_sweep_summary.json', 'w') as f:\n",
    "        json.dump(summary, f, indent=2)\n",
    "    print(\"Saved summary to architecture_sweep_summary.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-16",
   "metadata": {},
   "source": [
    "---\n",
    "## Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"ARCHITECTURE SWEEP CONCLUSIONS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    # Overall findings\n",
    "    global_avg = results_df[results_df['coverage'] == 'Global']['diff'].mean()\n",
    "    regional_avg = results_df[results_df['coverage'] != 'Global']['diff'].mean()\n",
    "    \n",
    "    print(f\"\"\"\n",
    "1. GLOBAL vs REGIONAL EFFECT PERSISTS:\n",
    "   - Global avg L=40 advantage: {global_avg:+.3f}\n",
    "   - Regional avg L=40 advantage: {regional_avg:+.3f}\n",
    "   - The regional advantage is NOT an artifact of MLP architecture\n",
    "\n",
    "2. ARCHITECTURE SENSITIVITY:\n",
    "   - Range of L=40 advantage across architectures: {results_df['diff'].min():.3f} to {results_df['diff'].max():.3f}\n",
    "   - Architecture choice matters, but doesn't eliminate the regional effect\n",
    "\n",
    "3. IMPLICATIONS FOR LEARNABLE ACTIVATIONS:\n",
    "   - If L=40's advantage persists across architectures, it's in the EMBEDDINGS\n",
    "   - A learnable activation that captures L=40's regional advantage with L=10's\n",
    "     parameter count would be valuable regardless of downstream architecture\n",
    "\"\"\")\n",
    "    \n",
    "    # Check if any architecture eliminates L=40 advantage regionally\n",
    "    regional = results_df[results_df['coverage'] != 'Global']\n",
    "    l10_wins = regional[regional['diff'] < -0.02]\n",
    "    if len(l10_wins) > 0:\n",
    "        print(\"4. CASES WHERE L=10 BEATS L=40 REGIONALLY:\")\n",
    "        for _, row in l10_wins.iterrows():\n",
    "            print(f\"   - {row['coverage']} with {row['architecture']}: Δ={row['diff']:+.3f}\")\n",
    "    else:\n",
    "        print(\"4. L=40 WINS REGIONALLY ACROSS ALL ARCHITECTURES TESTED\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
