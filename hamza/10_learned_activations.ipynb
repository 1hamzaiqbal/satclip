{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cell-0",
   "metadata": {},
   "source": [
    "# LocALE: Learned Activation Functions for Location Encoding\n",
    "\n",
    "**Goal**: Test if learned activation functions can replace spherical harmonics + SIREN\n",
    "\n",
    "## Key Idea\n",
    "SatCLIP uses:\n",
    "- Spherical harmonics (L=10: 100 features, L=40: 1600 features) for positional encoding\n",
    "- SIREN (sine activations) to process these features\n",
    "\n",
    "We propose:\n",
    "- Direct (lat, lon) input (no spherical harmonics)\n",
    "- Learned activation functions that discover frequency structure from data\n",
    "- Spatially-varying activations that adapt to local characteristics\n",
    "\n",
    "## Experiments\n",
    "1. **Baselines**: Direct + ReLU, Direct + SIREN, SatCLIP L=10, SatCLIP L=40\n",
    "2. **Learned Activations**: Fourier-parameterized activation functions\n",
    "3. **Spatially-Varying**: Mixture of expert activations based on location\n",
    "\n",
    "## Evaluation\n",
    "- Population density prediction at multiple scales\n",
    "- Global vs Regional performance (our key finding: L=40 wins regionally)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup\n",
    "import os\n",
    "import sys\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    !rm -rf sample_data .config satclip gpw_data 2>/dev/null\n",
    "    !git clone https://github.com/1hamzaiqbal/satclip.git\n",
    "    !pip install lightning torchgeo huggingface_hub rasterio --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and extract GPW data\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "import os\n",
    "import zipfile\n",
    "\n",
    "GPW_DIR = './gpw_data'\n",
    "os.makedirs(GPW_DIR, exist_ok=True)\n",
    "\n",
    "SOURCE_ZIP_PATH = '/content/drive/MyDrive/grad/learned_activations/dataverse_files.zip'\n",
    "\n",
    "print(\"Extracting GPW data...\")\n",
    "with zipfile.ZipFile(SOURCE_ZIP_PATH, 'r') as z:\n",
    "    z.extractall(GPW_DIR)\n",
    "\n",
    "# Extract 15-min resolution (good balance of speed and detail)\n",
    "zip_path = os.path.join(GPW_DIR, 'gpw-v4-population-density-rev11_2020_15_min_tif.zip')\n",
    "if os.path.exists(zip_path):\n",
    "    with zipfile.ZipFile(zip_path, 'r') as z:\n",
    "        z.extractall(GPW_DIR)\n",
    "    print(\"Extracted 15-min resolution\")\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.optim as optim\n",
    "\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    sys.path.append('./satclip/satclip')\n",
    "    GPW_DIR = './gpw_data'\n",
    "else:\n",
    "    sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'satclip'))\n",
    "    GPW_DIR = './gpw_data'\n",
    "\n",
    "from huggingface_hub import hf_hub_download\n",
    "from load import get_satclip\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Load SatCLIP models for comparison\n",
    "print(\"Loading SatCLIP models...\")\n",
    "satclip_l10 = get_satclip(hf_hub_download(\"microsoft/SatCLIP-ViT16-L10\", \"satclip-vit16-l10.ckpt\"), device=device)\n",
    "satclip_l40 = get_satclip(hf_hub_download(\"microsoft/SatCLIP-ViT16-L40\", \"satclip-vit16-l40.ckpt\"), device=device)\n",
    "satclip_l10.eval()\n",
    "satclip_l40.eval()\n",
    "print(\"SatCLIP models loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-4",
   "metadata": {},
   "source": [
    "---\n",
    "## 1. Learned Activation Functions\n",
    "\n",
    "Instead of fixed ReLU or sine, we parameterize activations as Fourier series:\n",
    "\n",
    "$$g_\\psi(x) = \\sum_{k=1}^{K} a_k \\sin(\\omega_k x) + b_k \\cos(\\omega_k x)$$\n",
    "\n",
    "where $\\omega_k$ are fixed frequencies and $\\psi = \\{a_k, b_k\\}$ are learned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearnedActivation(nn.Module):\n",
    "    \"\"\"Fourier-parameterized learned activation function.\n",
    "    \n",
    "    g(x) = sum_k a_k * sin(w_k * x) + b_k * cos(w_k * x)\n",
    "    \n",
    "    Args:\n",
    "        n_frequencies: Number of Fourier components (K)\n",
    "        freq_init: How to initialize frequencies ('linear', 'log', 'random')\n",
    "        learnable_freq: Whether frequencies are learnable\n",
    "    \"\"\"\n",
    "    def __init__(self, n_frequencies=25, freq_init='linear', learnable_freq=False, \n",
    "                 max_freq=10.0):\n",
    "        super().__init__()\n",
    "        self.n_frequencies = n_frequencies\n",
    "        \n",
    "        # Initialize frequencies\n",
    "        if freq_init == 'linear':\n",
    "            freqs = torch.linspace(0.1, max_freq, n_frequencies)\n",
    "        elif freq_init == 'log':\n",
    "            freqs = torch.logspace(-1, np.log10(max_freq), n_frequencies)\n",
    "        else:  # random\n",
    "            freqs = torch.rand(n_frequencies) * max_freq\n",
    "        \n",
    "        if learnable_freq:\n",
    "            self.frequencies = nn.Parameter(freqs)\n",
    "        else:\n",
    "            self.register_buffer('frequencies', freqs)\n",
    "        \n",
    "        # Learnable coefficients for sin and cos\n",
    "        # Initialize to approximate ReLU-like behavior\n",
    "        self.sin_coeffs = nn.Parameter(torch.randn(n_frequencies) * 0.1)\n",
    "        self.cos_coeffs = nn.Parameter(torch.randn(n_frequencies) * 0.1)\n",
    "        \n",
    "        # Learnable bias and scale\n",
    "        self.bias = nn.Parameter(torch.zeros(1))\n",
    "        self.scale = nn.Parameter(torch.ones(1))\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # x: any shape\n",
    "        # Compute Fourier series\n",
    "        # Shape: (*x.shape, n_frequencies)\n",
    "        wx = x.unsqueeze(-1) * self.frequencies  # broadcast multiply\n",
    "        \n",
    "        sin_terms = torch.sin(wx) * self.sin_coeffs\n",
    "        cos_terms = torch.cos(wx) * self.cos_coeffs\n",
    "        \n",
    "        # Sum over frequencies\n",
    "        result = (sin_terms + cos_terms).sum(dim=-1)\n",
    "        \n",
    "        return self.scale * result + self.bias\n",
    "    \n",
    "    def num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "\n",
    "# Test\n",
    "act = LearnedActivation(n_frequencies=25)\n",
    "x = torch.linspace(-3, 3, 100)\n",
    "y = act(x)\n",
    "print(f\"Learned activation params: {act.num_params()}\")\n",
    "\n",
    "# Visualize initial shape\n",
    "plt.figure(figsize=(10, 4))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(x.numpy(), y.detach().numpy(), label='Learned (init)')\n",
    "plt.plot(x.numpy(), torch.relu(x).numpy(), '--', label='ReLU')\n",
    "plt.plot(x.numpy(), torch.sin(x).numpy(), '--', label='Sine')\n",
    "plt.legend()\n",
    "plt.title('Activation Function Comparison')\n",
    "plt.xlabel('x')\n",
    "plt.ylabel('g(x)')\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.bar(range(act.n_frequencies), act.sin_coeffs.detach().numpy(), alpha=0.5, label='sin coeffs')\n",
    "plt.bar(range(act.n_frequencies), act.cos_coeffs.detach().numpy(), alpha=0.5, label='cos coeffs')\n",
    "plt.legend()\n",
    "plt.title('Fourier Coefficients (initial)')\n",
    "plt.xlabel('Frequency index')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-6",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Spatially-Varying Activations (Mixture of Experts)\n",
    "\n",
    "Different regions need different activation shapes:\n",
    "- Urban: sharp boundaries\n",
    "- Rural: smooth gradients  \n",
    "- Agricultural: periodic patterns\n",
    "\n",
    "We use a mixture of K expert activations, with weights from a gating network:\n",
    "\n",
    "$$g^{(l)}(x; \\lambda, \\phi) = \\sum_{m=1}^{M} w_m(\\lambda, \\phi) \\cdot g_{\\psi_l^m}(x)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatiallyVaryingActivation(nn.Module):\n",
    "    \"\"\"Mixture of expert activations with location-based gating.\n",
    "    \n",
    "    Args:\n",
    "        n_experts: Number of expert activation functions\n",
    "        n_frequencies: Frequencies per expert\n",
    "        gate_hidden: Hidden dim for gating network\n",
    "    \"\"\"\n",
    "    def __init__(self, n_experts=8, n_frequencies=25, gate_hidden=64):\n",
    "        super().__init__()\n",
    "        self.n_experts = n_experts\n",
    "        \n",
    "        # Create expert activations\n",
    "        self.experts = nn.ModuleList([\n",
    "            LearnedActivation(n_frequencies=n_frequencies)\n",
    "            for _ in range(n_experts)\n",
    "        ])\n",
    "        \n",
    "        # Gating network: (lat, lon) -> expert weights\n",
    "        self.gate = nn.Sequential(\n",
    "            nn.Linear(2, gate_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_hidden, gate_hidden),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(gate_hidden, n_experts),\n",
    "        )\n",
    "    \n",
    "    def forward(self, x, coords):\n",
    "        \"\"\"Apply spatially-varying activation.\n",
    "        \n",
    "        Args:\n",
    "            x: Input tensor (batch, features) or (batch, seq, features)\n",
    "            coords: Location (batch, 2) as (lon, lat) normalized to [-1, 1]\n",
    "        \n",
    "        Returns:\n",
    "            Activated tensor, same shape as x\n",
    "        \"\"\"\n",
    "        # Get gating weights\n",
    "        weights = F.softmax(self.gate(coords), dim=-1)  # (batch, n_experts)\n",
    "        \n",
    "        # Apply each expert and combine\n",
    "        expert_outputs = torch.stack([expert(x) for expert in self.experts], dim=-1)  # (*x.shape, n_experts)\n",
    "        \n",
    "        # Reshape weights for broadcasting\n",
    "        if x.dim() == 2:  # (batch, features)\n",
    "            weights = weights.unsqueeze(1)  # (batch, 1, n_experts)\n",
    "        elif x.dim() == 3:  # (batch, seq, features)\n",
    "            weights = weights.unsqueeze(1).unsqueeze(2)  # (batch, 1, 1, n_experts)\n",
    "        \n",
    "        # Weighted combination\n",
    "        result = (expert_outputs * weights).sum(dim=-1)\n",
    "        \n",
    "        return result\n",
    "    \n",
    "    def get_expert_weights(self, coords):\n",
    "        \"\"\"Get expert weights for visualization.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return F.softmax(self.gate(coords), dim=-1)\n",
    "    \n",
    "    def num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "\n",
    "# Test\n",
    "spatial_act = SpatiallyVaryingActivation(n_experts=8, n_frequencies=25)\n",
    "print(f\"Spatially-varying activation params: {spatial_act.num_params()}\")\n",
    "\n",
    "# Visualize expert weights across globe\n",
    "lons = torch.linspace(-1, 1, 50)\n",
    "lats = torch.linspace(-1, 1, 50)\n",
    "lon_grid, lat_grid = torch.meshgrid(lons, lats, indexing='xy')\n",
    "coords = torch.stack([lon_grid.flatten(), lat_grid.flatten()], dim=1)\n",
    "\n",
    "weights = spatial_act.get_expert_weights(coords)  # (2500, 8)\n",
    "weights = weights.view(50, 50, 8)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    im = ax.imshow(weights[:, :, i].numpy(), origin='lower', cmap='viridis',\n",
    "                   extent=[-180, 180, -90, 90])\n",
    "    ax.set_title(f'Expert {i+1} weight')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "plt.suptitle('Initial Expert Gating Weights (before training)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-8",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Location Encoder Architectures\n",
    "\n",
    "We'll test several location encoder variants:\n",
    "1. **Direct + ReLU**: Simple MLP baseline\n",
    "2. **Direct + SIREN**: Sine activations (like SatCLIP)\n",
    "3. **Direct + Learned**: Our learned activations\n",
    "4. **Direct + Spatial**: Spatially-varying learned activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SineActivation(nn.Module):\n",
    "    \"\"\"Sine activation for SIREN networks.\"\"\"\n",
    "    def __init__(self, omega_0=30.0):\n",
    "        super().__init__()\n",
    "        self.omega_0 = omega_0\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return torch.sin(self.omega_0 * x)\n",
    "\n",
    "\n",
    "class LocationEncoder(nn.Module):\n",
    "    \"\"\"Location encoder with configurable activation functions.\n",
    "    \n",
    "    Args:\n",
    "        input_dim: Input dimension (2 for direct coords, or SH features)\n",
    "        hidden_dim: Hidden layer dimension\n",
    "        output_dim: Output embedding dimension\n",
    "        n_layers: Number of hidden layers\n",
    "        activation: 'relu', 'siren', 'learned', 'spatial'\n",
    "        n_frequencies: For learned activations\n",
    "        n_experts: For spatial activations\n",
    "    \"\"\"\n",
    "    def __init__(self, input_dim=2, hidden_dim=256, output_dim=256, n_layers=3,\n",
    "                 activation='relu', n_frequencies=25, n_experts=8):\n",
    "        super().__init__()\n",
    "        self.activation_type = activation\n",
    "        self.n_layers = n_layers\n",
    "        \n",
    "        # Build layers\n",
    "        layers = []\n",
    "        dims = [input_dim] + [hidden_dim] * n_layers + [output_dim]\n",
    "        \n",
    "        for i in range(len(dims) - 1):\n",
    "            layers.append(nn.Linear(dims[i], dims[i+1]))\n",
    "        \n",
    "        self.linears = nn.ModuleList(layers)\n",
    "        \n",
    "        # Create activations\n",
    "        if activation == 'relu':\n",
    "            self.activations = nn.ModuleList([nn.ReLU() for _ in range(n_layers)])\n",
    "        elif activation == 'siren':\n",
    "            self.activations = nn.ModuleList([SineActivation() for _ in range(n_layers)])\n",
    "        elif activation == 'learned':\n",
    "            self.activations = nn.ModuleList([\n",
    "                LearnedActivation(n_frequencies=n_frequencies) for _ in range(n_layers)\n",
    "            ])\n",
    "        elif activation == 'spatial':\n",
    "            self.activations = nn.ModuleList([\n",
    "                SpatiallyVaryingActivation(n_experts=n_experts, n_frequencies=n_frequencies)\n",
    "                for _ in range(n_layers)\n",
    "            ])\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown activation: {activation}\")\n",
    "        \n",
    "        # Initialize weights\n",
    "        self._init_weights()\n",
    "    \n",
    "    def _init_weights(self):\n",
    "        for i, linear in enumerate(self.linears):\n",
    "            if self.activation_type == 'siren':\n",
    "                # SIREN initialization\n",
    "                if i == 0:\n",
    "                    nn.init.uniform_(linear.weight, -1/linear.in_features, 1/linear.in_features)\n",
    "                else:\n",
    "                    nn.init.uniform_(linear.weight, \n",
    "                                    -np.sqrt(6/linear.in_features)/30,\n",
    "                                    np.sqrt(6/linear.in_features)/30)\n",
    "            else:\n",
    "                nn.init.kaiming_normal_(linear.weight)\n",
    "            nn.init.zeros_(linear.bias)\n",
    "    \n",
    "    def forward(self, coords):\n",
    "        \"\"\"Encode coordinates to embeddings.\n",
    "        \n",
    "        Args:\n",
    "            coords: (batch, 2) as (lon, lat)\n",
    "        \n",
    "        Returns:\n",
    "            embeddings: (batch, output_dim)\n",
    "        \"\"\"\n",
    "        # Normalize coords to [-1, 1]\n",
    "        x = coords.clone()\n",
    "        x[:, 0] = x[:, 0] / 180.0  # lon\n",
    "        x[:, 1] = x[:, 1] / 90.0   # lat\n",
    "        \n",
    "        # Store normalized coords for spatial activations\n",
    "        norm_coords = x.clone()\n",
    "        \n",
    "        # Forward pass\n",
    "        for i, (linear, act) in enumerate(zip(self.linears[:-1], self.activations)):\n",
    "            x = linear(x)\n",
    "            if self.activation_type == 'spatial':\n",
    "                x = act(x, norm_coords)\n",
    "            else:\n",
    "                x = act(x)\n",
    "        \n",
    "        # Final linear (no activation)\n",
    "        x = self.linears[-1](x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    def num_params(self):\n",
    "        return sum(p.numel() for p in self.parameters())\n",
    "\n",
    "\n",
    "# Test all variants\n",
    "print(\"Location Encoder Variants:\")\n",
    "print(\"-\" * 50)\n",
    "for act_type in ['relu', 'siren', 'learned', 'spatial']:\n",
    "    model = LocationEncoder(activation=act_type)\n",
    "    print(f\"{act_type:10s}: {model.num_params():,} params\")\n",
    "\n",
    "# Test forward pass\n",
    "coords = torch.randn(32, 2) * torch.tensor([180., 90.])\n",
    "encoder = LocationEncoder(activation='spatial')\n",
    "emb = encoder(coords)\n",
    "print(f\"\\nOutput shape: {emb.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-10",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-11",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load population data\n",
    "Image.MAX_IMAGE_PIXELS = None\n",
    "\n",
    "def load_gpw_raster(resolution='15_min', year=2020):\n",
    "    \"\"\"Load GPW population density raster.\"\"\"\n",
    "    tif_file = f\"{GPW_DIR}/gpw_v4_population_density_rev11_{year}_{resolution}.tif\"\n",
    "    if not os.path.exists(tif_file):\n",
    "        print(f\"File not found: {tif_file}\")\n",
    "        return None, None\n",
    "    \n",
    "    img = Image.open(tif_file)\n",
    "    data = np.array(img)\n",
    "    height, width = data.shape\n",
    "    \n",
    "    lon_step = 360 / width\n",
    "    lat_step = 180 / height\n",
    "    lons = np.linspace(-180 + lon_step/2, 180 - lon_step/2, width)\n",
    "    lats = np.linspace(90 - lat_step/2, -90 + lat_step/2, height)\n",
    "    \n",
    "    return data, (lons, lats)\n",
    "\n",
    "def sample_from_raster(data, coords, n_samples=10000, seed=42, bounds=None):\n",
    "    \"\"\"Sample random valid points from population raster.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    lons, lats = coords\n",
    "    valid_mask = data > -1e30\n",
    "    \n",
    "    if bounds is not None:\n",
    "        lon_min, lat_min, lon_max, lat_max = bounds\n",
    "        lon_grid, lat_grid = np.meshgrid(lons, lats)\n",
    "        bounds_mask = (\n",
    "            (lon_grid >= lon_min) & (lon_grid <= lon_max) &\n",
    "            (lat_grid >= lat_min) & (lat_grid <= lat_max)\n",
    "        )\n",
    "        valid_mask = valid_mask & bounds_mask\n",
    "    \n",
    "    valid_idx = np.where(valid_mask)\n",
    "    n_valid = len(valid_idx[0])\n",
    "    \n",
    "    if n_valid < n_samples:\n",
    "        sample_idx = np.arange(n_valid)\n",
    "    else:\n",
    "        sample_idx = np.random.choice(n_valid, n_samples, replace=False)\n",
    "    \n",
    "    row_idx = valid_idx[0][sample_idx]\n",
    "    col_idx = valid_idx[1][sample_idx]\n",
    "    \n",
    "    sample_lons = lons[col_idx]\n",
    "    sample_lats = lats[row_idx]\n",
    "    sample_values = data[row_idx, col_idx]\n",
    "    \n",
    "    coords_arr = np.stack([sample_lons, sample_lats], axis=1)\n",
    "    return coords_arr, sample_values\n",
    "\n",
    "# Load data\n",
    "print(\"Loading population data...\")\n",
    "pop_data, pop_coords = load_gpw_raster('15_min')\n",
    "print(f\"Shape: {pop_data.shape}\")\n",
    "\n",
    "# Define regions\n",
    "REGIONS = {\n",
    "    'Global': None,\n",
    "    'USA': (-125, 24, -66, 50),\n",
    "    'Europe': (-10, 35, 40, 70),\n",
    "    'China': (73, 18, 135, 54),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopulationDataset(Dataset):\n",
    "    \"\"\"Dataset for population prediction.\"\"\"\n",
    "    def __init__(self, coords, values):\n",
    "        self.coords = torch.tensor(coords, dtype=torch.float32)\n",
    "        self.values = torch.tensor(np.log1p(values), dtype=torch.float32)  # Log transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.coords)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.coords[idx], self.values[idx]\n",
    "\n",
    "\n",
    "def create_dataloaders(coords, values, test_size=0.5, batch_size=256, seed=42):\n",
    "    \"\"\"Create train/test dataloaders.\"\"\"\n",
    "    np.random.seed(seed)\n",
    "    n = len(coords)\n",
    "    indices = np.random.permutation(n)\n",
    "    split = int(n * (1 - test_size))\n",
    "    \n",
    "    train_idx, test_idx = indices[:split], indices[split:]\n",
    "    \n",
    "    train_dataset = PopulationDataset(coords[train_idx], values[train_idx])\n",
    "    test_dataset = PopulationDataset(coords[test_idx], values[test_idx])\n",
    "    \n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-13",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. Training Infrastructure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PopulationPredictor(nn.Module):\n",
    "    \"\"\"Full model: location encoder + prediction head.\"\"\"\n",
    "    def __init__(self, encoder, freeze_encoder=False):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "        \n",
    "        if freeze_encoder:\n",
    "            for param in self.encoder.parameters():\n",
    "                param.requires_grad = False\n",
    "    \n",
    "    def forward(self, coords):\n",
    "        emb = self.encoder(coords)\n",
    "        return self.head(emb).squeeze(-1)\n",
    "\n",
    "\n",
    "class SatCLIPPredictor(nn.Module):\n",
    "    \"\"\"Population predictor using frozen SatCLIP embeddings.\"\"\"\n",
    "    def __init__(self, satclip_model):\n",
    "        super().__init__()\n",
    "        self.satclip = satclip_model\n",
    "        for param in self.satclip.parameters():\n",
    "            param.requires_grad = False\n",
    "        \n",
    "        self.head = nn.Sequential(\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "    \n",
    "    def forward(self, coords):\n",
    "        with torch.no_grad():\n",
    "            emb = self.satclip(coords.double()).float()\n",
    "        return self.head(emb).squeeze(-1)\n",
    "\n",
    "\n",
    "def train_model(model, train_loader, test_loader, epochs=50, lr=1e-3, device='cuda'):\n",
    "    \"\"\"Train model and return metrics.\"\"\"\n",
    "    model = model.to(device)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "    criterion = nn.MSELoss()\n",
    "    \n",
    "    history = {'train_loss': [], 'test_loss': [], 'test_r2': []}\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        # Train\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for coords, targets in train_loader:\n",
    "            coords, targets = coords.to(device), targets.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(coords)\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            train_loss += loss.item()\n",
    "        \n",
    "        train_loss /= len(train_loader)\n",
    "        \n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        all_preds, all_targets = [], []\n",
    "        test_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for coords, targets in test_loader:\n",
    "                coords, targets = coords.to(device), targets.to(device)\n",
    "                outputs = model(coords)\n",
    "                test_loss += criterion(outputs, targets).item()\n",
    "                all_preds.append(outputs.cpu())\n",
    "                all_targets.append(targets.cpu())\n",
    "        \n",
    "        test_loss /= len(test_loader)\n",
    "        all_preds = torch.cat(all_preds).numpy()\n",
    "        all_targets = torch.cat(all_targets).numpy()\n",
    "        \n",
    "        # R² score\n",
    "        ss_res = np.sum((all_targets - all_preds) ** 2)\n",
    "        ss_tot = np.sum((all_targets - all_targets.mean()) ** 2)\n",
    "        r2 = 1 - ss_res / ss_tot\n",
    "        \n",
    "        history['train_loss'].append(train_loss)\n",
    "        history['test_loss'].append(test_loss)\n",
    "        history['test_r2'].append(r2)\n",
    "        \n",
    "        if (epoch + 1) % 10 == 0:\n",
    "            print(f\"  Epoch {epoch+1}/{epochs}: train_loss={train_loss:.4f}, test_loss={test_loss:.4f}, R²={r2:.4f}\")\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-15",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Run Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-16",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"EXPERIMENT: Learned Activations vs SatCLIP\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "N_SAMPLES = 15000\n",
    "EPOCHS = 50\n",
    "BATCH_SIZE = 256\n",
    "\n",
    "all_results = []\n",
    "\n",
    "for region_name, bounds in REGIONS.items():\n",
    "    print(f\"\\n{'─'*60}\")\n",
    "    print(f\"Region: {region_name}\")\n",
    "    print(f\"{'─'*60}\")\n",
    "    \n",
    "    # Sample data\n",
    "    coords, values = sample_from_raster(pop_data, pop_coords, n_samples=N_SAMPLES, bounds=bounds)\n",
    "    print(f\"Samples: {len(coords)}\")\n",
    "    \n",
    "    if len(coords) < 1000:\n",
    "        print(\"Skipping - too few samples\")\n",
    "        continue\n",
    "    \n",
    "    # Create dataloaders\n",
    "    train_loader, test_loader = create_dataloaders(coords, values, batch_size=BATCH_SIZE)\n",
    "    \n",
    "    # Test different models\n",
    "    models_to_test = {\n",
    "        'SatCLIP L=10': lambda: SatCLIPPredictor(satclip_l10),\n",
    "        'SatCLIP L=40': lambda: SatCLIPPredictor(satclip_l40),\n",
    "        'Direct + ReLU': lambda: PopulationPredictor(LocationEncoder(activation='relu')),\n",
    "        'Direct + SIREN': lambda: PopulationPredictor(LocationEncoder(activation='siren')),\n",
    "        'Direct + Learned': lambda: PopulationPredictor(LocationEncoder(activation='learned', n_frequencies=25)),\n",
    "        'Direct + Spatial (K=8)': lambda: PopulationPredictor(LocationEncoder(activation='spatial', n_experts=8)),\n",
    "    }\n",
    "    \n",
    "    print(f\"\\n{'Model':<25} | {'R²':>8} | {'Params':>10}\")\n",
    "    print(\"-\" * 50)\n",
    "    \n",
    "    for model_name, model_fn in models_to_test.items():\n",
    "        print(f\"Training {model_name}...\")\n",
    "        model = model_fn()\n",
    "        n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "        \n",
    "        history = train_model(model, train_loader, test_loader, epochs=EPOCHS, device=device)\n",
    "        final_r2 = history['test_r2'][-1]\n",
    "        \n",
    "        print(f\"{model_name:<25} | {final_r2:>8.4f} | {n_params:>10,}\")\n",
    "        \n",
    "        all_results.append({\n",
    "            'region': region_name,\n",
    "            'model': model_name,\n",
    "            'r2': final_r2,\n",
    "            'params': n_params,\n",
    "            'history': history\n",
    "        })\n",
    "\n",
    "results_df = pd.DataFrame([{k: v for k, v in r.items() if k != 'history'} for r in all_results])\n",
    "print(f\"\\n\\nTotal experiments: {len(results_df)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-17",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize results\n",
    "if len(results_df) > 0:\n",
    "    # Pivot table\n",
    "    pivot = results_df.pivot(index='model', columns='region', values='r2')\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"RESULTS: R² by Model and Region\")\n",
    "    print(\"=\"*70)\n",
    "    print(pivot.round(3).to_string())\n",
    "    \n",
    "    # Plot\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "    \n",
    "    # Bar chart by region\n",
    "    ax = axes[0]\n",
    "    pivot.T.plot(kind='bar', ax=ax)\n",
    "    ax.set_ylabel('R² Score')\n",
    "    ax.set_xlabel('Region')\n",
    "    ax.set_title('Population Prediction R² by Region')\n",
    "    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    plt.setp(ax.xaxis.get_majorticklabels(), rotation=45, ha='right')\n",
    "    \n",
    "    # Compare SatCLIP vs Ours\n",
    "    ax = axes[1]\n",
    "    regions = results_df['region'].unique()\n",
    "    x = np.arange(len(regions))\n",
    "    width = 0.15\n",
    "    \n",
    "    models_order = ['SatCLIP L=10', 'SatCLIP L=40', 'Direct + ReLU', 'Direct + SIREN', 'Direct + Learned', 'Direct + Spatial (K=8)']\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(models_order)))\n",
    "    \n",
    "    for i, model in enumerate(models_order):\n",
    "        model_data = results_df[results_df['model'] == model]\n",
    "        if len(model_data) > 0:\n",
    "            r2_vals = [model_data[model_data['region'] == r]['r2'].values[0] if len(model_data[model_data['region'] == r]) > 0 else 0 for r in regions]\n",
    "            ax.bar(x + i*width, r2_vals, width, label=model, color=colors[i])\n",
    "    \n",
    "    ax.set_xticks(x + width * (len(models_order)-1) / 2)\n",
    "    ax.set_xticklabels(regions)\n",
    "    ax.set_ylabel('R² Score')\n",
    "    ax.set_title('Model Comparison by Region')\n",
    "    ax.legend(bbox_to_anchor=(1.02, 1), loc='upper left', fontsize=8)\n",
    "    ax.set_ylim(0, 1)\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.savefig('learned_activations_results.png', dpi=150, bbox_inches='tight')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-18",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze: Can learned activations match SatCLIP?\n",
    "print(\"=\"*70)\n",
    "print(\"ANALYSIS: Learned Activations vs SatCLIP\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "if len(results_df) > 0:\n",
    "    for region in results_df['region'].unique():\n",
    "        region_data = results_df[results_df['region'] == region]\n",
    "        \n",
    "        satclip_l10 = region_data[region_data['model'] == 'SatCLIP L=10']['r2'].values\n",
    "        satclip_l40 = region_data[region_data['model'] == 'SatCLIP L=40']['r2'].values\n",
    "        learned = region_data[region_data['model'] == 'Direct + Learned']['r2'].values\n",
    "        spatial = region_data[region_data['model'] == 'Direct + Spatial (K=8)']['r2'].values\n",
    "        \n",
    "        if len(satclip_l10) > 0 and len(learned) > 0:\n",
    "            print(f\"\\n{region}:\")\n",
    "            print(f\"  SatCLIP L=10: {satclip_l10[0]:.3f}\")\n",
    "            print(f\"  SatCLIP L=40: {satclip_l40[0]:.3f}\" if len(satclip_l40) > 0 else \"  SatCLIP L=40: N/A\")\n",
    "            print(f\"  Learned:      {learned[0]:.3f} (vs L=10: {learned[0] - satclip_l10[0]:+.3f})\")\n",
    "            print(f\"  Spatial:      {spatial[0]:.3f} (vs L=10: {spatial[0] - satclip_l10[0]:+.3f})\" if len(spatial) > 0 else \"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-19",
   "metadata": {},
   "source": [
    "---\n",
    "## 7. Visualize Learned Activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train a model and visualize the learned activations\n",
    "print(\"Training model to visualize learned activations...\")\n",
    "\n",
    "# Sample global data\n",
    "coords, values = sample_from_raster(pop_data, pop_coords, n_samples=15000, bounds=None)\n",
    "train_loader, test_loader = create_dataloaders(coords, values, batch_size=256)\n",
    "\n",
    "# Train learned activation model\n",
    "encoder = LocationEncoder(activation='learned', n_frequencies=25, n_layers=3)\n",
    "model = PopulationPredictor(encoder)\n",
    "history = train_model(model, train_loader, test_loader, epochs=50, device=device)\n",
    "\n",
    "# Visualize learned activations\n",
    "x = torch.linspace(-3, 3, 200)\n",
    "\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
    "\n",
    "for i, act in enumerate(encoder.activations):\n",
    "    y = act(x.to(device)).cpu().detach().numpy()\n",
    "    axes[i].plot(x.numpy(), y, linewidth=2, label='Learned')\n",
    "    axes[i].plot(x.numpy(), torch.relu(x).numpy(), '--', alpha=0.5, label='ReLU')\n",
    "    axes[i].plot(x.numpy(), torch.sin(30*x).numpy(), '--', alpha=0.5, label='SIREN')\n",
    "    axes[i].set_title(f'Layer {i+1} Activation')\n",
    "    axes[i].set_xlabel('x')\n",
    "    axes[i].set_ylabel('g(x)')\n",
    "    axes[i].legend()\n",
    "    axes[i].grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Learned Activation Functions (after training)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('learned_activations_shapes.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-21",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize spatial activation expert weights (after training)\n",
    "print(\"Training spatial model to visualize expert weights...\")\n",
    "\n",
    "encoder_spatial = LocationEncoder(activation='spatial', n_experts=8, n_frequencies=25)\n",
    "model_spatial = PopulationPredictor(encoder_spatial)\n",
    "history_spatial = train_model(model_spatial, train_loader, test_loader, epochs=50, device=device)\n",
    "\n",
    "# Get expert weights across the globe\n",
    "lons = torch.linspace(-180, 180, 100)\n",
    "lats = torch.linspace(-90, 90, 50)\n",
    "lon_grid, lat_grid = torch.meshgrid(lons, lats, indexing='xy')\n",
    "coords_grid = torch.stack([lon_grid.flatten(), lat_grid.flatten()], dim=1)\n",
    "\n",
    "# Normalize coords\n",
    "norm_coords = coords_grid.clone()\n",
    "norm_coords[:, 0] = norm_coords[:, 0] / 180.0\n",
    "norm_coords[:, 1] = norm_coords[:, 1] / 90.0\n",
    "\n",
    "# Get weights from first layer\n",
    "spatial_act = encoder_spatial.activations[0].to(device)\n",
    "weights = spatial_act.get_expert_weights(norm_coords.to(device)).cpu()\n",
    "weights = weights.view(100, 50, 8).permute(1, 0, 2)  # (lat, lon, experts)\n",
    "\n",
    "fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    im = ax.imshow(weights[:, :, i].numpy(), origin='lower', cmap='viridis',\n",
    "                   extent=[-180, 180, -90, 90], aspect='auto')\n",
    "    ax.set_title(f'Expert {i+1}')\n",
    "    ax.set_xlabel('Longitude')\n",
    "    ax.set_ylabel('Latitude')\n",
    "    plt.colorbar(im, ax=ax)\n",
    "\n",
    "plt.suptitle('Spatial Expert Weights (after training on population)', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.savefig('spatial_expert_weights.png', dpi=150)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cell-22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results\n",
    "if len(results_df) > 0:\n",
    "    results_df.to_csv('learned_activations_results.csv', index=False)\n",
    "    print(\"Results saved to learned_activations_results.csv\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*70)\n",
    "    print(\"SUMMARY\")\n",
    "    print(\"=\"*70)\n",
    "    print(results_df.pivot(index='model', columns='region', values='r2').round(3).to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cell-23",
   "metadata": {},
   "source": [
    "---\n",
    "## 8. Next Steps\n",
    "\n",
    "Based on these initial results, here's what to try next:\n",
    "\n",
    "### If Learned Activations Work Well:\n",
    "1. **Scale up**: More frequencies, more experts, deeper networks\n",
    "2. **Contrastive pretraining**: Train full SatCLIP-style with image-location pairs\n",
    "3. **Multi-task**: Test on temperature, elevation, land cover\n",
    "\n",
    "### If They Don't Match SatCLIP:\n",
    "1. **Hybrid approach**: Use spherical harmonics BUT with learned activations\n",
    "2. **Different parameterization**: Try B-splines, wavelets, or other basis functions\n",
    "3. **Architecture search**: More layers, residual connections, attention\n",
    "\n",
    "### Key Experiments:\n",
    "1. **Regional advantage**: Does spatial varying help more in regional tasks?\n",
    "2. **Parameter efficiency**: Can we match L=40 with fewer params?\n",
    "3. **Activation visualization**: What shapes do the learned activations converge to?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
