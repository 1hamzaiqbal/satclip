{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SatCLIP Deep Dive: Interpolation, Ecoregions, MLP & Visualization\n",
    "\n",
    "This notebook continues our investigation with 4 experiments:\n",
    "1. **Spatial Interpolation** - Task where L=40 should outperform L=10\n",
    "2. **Ecoregion Classification** - Real-world multi-level classification\n",
    "3. **MLP vs Logistic Regression** - Does a more complex classifier help?\n",
    "4. **t-SNE/UMAP Visualization** - Understand embedding structure\n",
    "\n",
    "For best performance, use `Runtime -> Change runtime type -> T4 GPU`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone SatCLIP repository (only needed in Colab)\n",
    "import os\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    !rm -rf sample_data .config satclip 2>/dev/null\n",
    "    !git clone https://github.com/1hamzaiqbal/satclip.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install dependencies\n",
    "!pip install lightning --quiet\n",
    "!pip install rasterio --quiet\n",
    "!pip install torchgeo --quiet\n",
    "!pip install huggingface_hub --quiet\n",
    "!pip install umap-learn --quiet\n",
    "!pip install geopandas --quiet\n",
    "!pip install shapely --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Handle path for both Colab and local execution\n",
    "if 'COLAB_GPU' in os.environ:\n",
    "    sys.path.append('./satclip/satclip')\n",
    "else:\n",
    "    sys.path.append(os.path.join(os.path.dirname(os.getcwd()), 'satclip'))\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from huggingface_hub import hf_hub_download\n",
    "from load import get_satclip\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, mean_squared_error, r2_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load both models\n",
    "print(\"Loading L=10 model...\")\n",
    "model_l10 = get_satclip(\n",
    "    hf_hub_download(\"microsoft/SatCLIP-ViT16-L10\", \"satclip-vit16-l10.ckpt\"),\n",
    "    device=device,\n",
    ")\n",
    "model_l10.eval()\n",
    "print(\"L=10 model loaded!\")\n",
    "\n",
    "print(\"\\nLoading L=40 model...\")\n",
    "model_l40 = get_satclip(\n",
    "    hf_hub_download(\"microsoft/SatCLIP-ViT16-L40\", \"satclip-vit16-l40.ckpt\"),\n",
    "    device=device,\n",
    ")\n",
    "model_l40.eval()\n",
    "print(\"L=40 model loaded!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 2. Spatial Interpolation Task (L=40 Should Win)\n",
    "\n",
    "**Key insight from paper**: L=40 is better at predicting values at locations *between* training points.\n",
    "\n",
    "**Design**: Create a continuous function over space, train on a grid of points, predict at points *between* grid cells. This tests interpolation ability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interpolation_dataset(grid_spacing_deg=5, noise_std=0.1, bounds=(-120, -70, 25, 50)):\n",
    "    \"\"\"\n",
    "    Create a spatial interpolation task over the continental US.\n",
    "    \n",
    "    Training: Regular grid of points\n",
    "    Testing: Points at the CENTER of each grid cell (interpolation)\n",
    "    Target: A smooth spatial function (latitude + some spatial variation)\n",
    "    \"\"\"\n",
    "    lon_min, lon_max, lat_min, lat_max = bounds\n",
    "    \n",
    "    # Create training grid\n",
    "    train_lons = np.arange(lon_min, lon_max + grid_spacing_deg, grid_spacing_deg)\n",
    "    train_lats = np.arange(lat_min, lat_max + grid_spacing_deg, grid_spacing_deg)\n",
    "    train_lon_grid, train_lat_grid = np.meshgrid(train_lons, train_lats)\n",
    "    train_coords = np.stack([train_lon_grid.flatten(), train_lat_grid.flatten()], axis=1)\n",
    "    \n",
    "    # Create test points at CENTER of each grid cell (interpolation points)\n",
    "    test_lons = train_lons[:-1] + grid_spacing_deg / 2\n",
    "    test_lats = train_lats[:-1] + grid_spacing_deg / 2\n",
    "    test_lon_grid, test_lat_grid = np.meshgrid(test_lons, test_lats)\n",
    "    test_coords = np.stack([test_lon_grid.flatten(), test_lat_grid.flatten()], axis=1)\n",
    "    \n",
    "    # Target function: latitude normalized + spatial wave pattern\n",
    "    def target_function(lon, lat):\n",
    "        # Normalize latitude to [0, 1]\n",
    "        lat_norm = (lat - lat_min) / (lat_max - lat_min)\n",
    "        # Add spatial wave pattern\n",
    "        wave = 0.2 * np.sin(np.radians(lon) * 3) * np.cos(np.radians(lat) * 2)\n",
    "        return lat_norm + wave\n",
    "    \n",
    "    train_y = target_function(train_coords[:, 0], train_coords[:, 1])\n",
    "    test_y = target_function(test_coords[:, 0], test_coords[:, 1])\n",
    "    \n",
    "    # Add noise to training targets\n",
    "    train_y += np.random.normal(0, noise_std, len(train_y))\n",
    "    \n",
    "    return (\n",
    "        torch.tensor(train_coords).double(),\n",
    "        torch.tensor(train_y).float(),\n",
    "        torch.tensor(test_coords).double(),\n",
    "        torch.tensor(test_y).float()\n",
    "    )\n",
    "\n",
    "# Create dataset with different grid spacings\n",
    "print(\"Testing interpolation at different grid spacings...\\n\")\n",
    "print(f\"{'Grid Spacing':>12} | {'L=10 R²':>10} | {'L=40 R²':>10} | {'Winner':>10}\")\n",
    "print(\"-\" * 52)\n",
    "\n",
    "grid_spacings = [10, 5, 2, 1]  # degrees\n",
    "results_interp = {'spacing': [], 'l10_r2': [], 'l40_r2': []}\n",
    "\n",
    "for spacing in grid_spacings:\n",
    "    train_coords, train_y, test_coords, test_y = create_interpolation_dataset(\n",
    "        grid_spacing_deg=spacing, noise_std=0.05\n",
    "    )\n",
    "    \n",
    "    # Get embeddings\n",
    "    with torch.no_grad():\n",
    "        train_emb_l10 = model_l10(train_coords.to(device)).cpu().numpy()\n",
    "        train_emb_l40 = model_l40(train_coords.to(device)).cpu().numpy()\n",
    "        test_emb_l10 = model_l10(test_coords.to(device)).cpu().numpy()\n",
    "        test_emb_l40 = model_l40(test_coords.to(device)).cpu().numpy()\n",
    "    \n",
    "    # Train simple linear regressors\n",
    "    from sklearn.linear_model import Ridge\n",
    "    \n",
    "    reg_l10 = Ridge(alpha=1.0)\n",
    "    reg_l10.fit(train_emb_l10, train_y.numpy())\n",
    "    pred_l10 = reg_l10.predict(test_emb_l10)\n",
    "    r2_l10 = r2_score(test_y.numpy(), pred_l10)\n",
    "    \n",
    "    reg_l40 = Ridge(alpha=1.0)\n",
    "    reg_l40.fit(train_emb_l40, train_y.numpy())\n",
    "    pred_l40 = reg_l40.predict(test_emb_l40)\n",
    "    r2_l40 = r2_score(test_y.numpy(), pred_l40)\n",
    "    \n",
    "    winner = \"L=40\" if r2_l40 > r2_l10 + 0.01 else (\"L=10\" if r2_l10 > r2_l40 + 0.01 else \"~Same\")\n",
    "    print(f\"{spacing:>10}° | {r2_l10:>10.4f} | {r2_l40:>10.4f} | {winner:>10}\")\n",
    "    \n",
    "    results_interp['spacing'].append(spacing)\n",
    "    results_interp['l10_r2'].append(r2_l10)\n",
    "    results_interp['l40_r2'].append(r2_l40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize interpolation results\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(results_interp['spacing'], results_interp['l10_r2'], 'o-', label='L=10', linewidth=2, markersize=10)\n",
    "plt.plot(results_interp['spacing'], results_interp['l40_r2'], 's-', label='L=40', linewidth=2, markersize=10)\n",
    "plt.xlabel('Grid Spacing (degrees)', fontsize=12)\n",
    "plt.ylabel('R² Score (Interpolation)', fontsize=12)\n",
    "plt.title('Spatial Interpolation Task\\n(L=40 should excel at predicting between training points)', fontsize=14)\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.gca().invert_xaxis()  # Smaller spacing = harder task\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nInterpretation:\")\n",
    "print(\"- This tests interpolation (predicting between known points)\")\n",
    "print(\"- Per the paper, L=40 should perform better here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Ecoregion Classification\n",
    "\n",
    "Test with real WWF Ecoregions data at different hierarchy levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download WWF Ecoregions shapefile\n",
    "import geopandas as gpd\n",
    "from shapely.geometry import Point\n",
    "\n",
    "# Try to load ecoregions - use a simplified approach with direct download\n",
    "print(\"Loading WWF Ecoregions data...\")\n",
    "\n",
    "try:\n",
    "    # Try loading from a GeoJSON source (more reliable than shapefile)\n",
    "    ecoregions_url = \"https://raw.githubusercontent.com/Resolve-Collective/wwf-ecoregions/main/data/wwf_terr_ecos.geojson\"\n",
    "    ecoregions = gpd.read_file(ecoregions_url)\n",
    "    print(f\"Loaded {len(ecoregions)} ecoregions\")\n",
    "    print(f\"Columns: {list(ecoregions.columns)}\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load from URL: {e}\")\n",
    "    print(\"\\nUsing synthetic ecoregion-like data instead...\")\n",
    "    ecoregions = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_points_with_ecoregions(ecoregions_gdf, n_samples=5000, seed=42):\n",
    "    \"\"\"\n",
    "    Sample random points and assign ecoregion labels.\n",
    "    \"\"\"\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    # Sample from land bounds\n",
    "    lons = np.random.uniform(-180, 180, n_samples * 3)\n",
    "    lats = np.random.uniform(-60, 70, n_samples * 3)\n",
    "    \n",
    "    points = gpd.GeoDataFrame(\n",
    "        geometry=[Point(lon, lat) for lon, lat in zip(lons, lats)],\n",
    "        crs=ecoregions_gdf.crs\n",
    "    )\n",
    "    \n",
    "    # Spatial join to get ecoregion for each point\n",
    "    joined = gpd.sjoin(points, ecoregions_gdf, how='inner', predicate='within')\n",
    "    \n",
    "    # Take first n_samples\n",
    "    joined = joined.head(n_samples)\n",
    "    \n",
    "    coords = np.array([[p.x, p.y] for p in joined.geometry])\n",
    "    \n",
    "    return coords, joined\n",
    "\n",
    "if ecoregions is not None:\n",
    "    # Identify the column names for different hierarchy levels\n",
    "    print(\"\\nEcoregion columns:\")\n",
    "    for col in ecoregions.columns:\n",
    "        if ecoregions[col].dtype == 'object':\n",
    "            n_unique = ecoregions[col].nunique()\n",
    "            print(f\"  {col}: {n_unique} unique values\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ecoregions is not None:\n",
    "    print(\"Sampling points with ecoregion labels...\")\n",
    "    coords, joined_df = sample_points_with_ecoregions(ecoregions, n_samples=5000)\n",
    "    print(f\"Sampled {len(coords)} points with ecoregion labels\")\n",
    "    \n",
    "    # Identify hierarchy columns (adjust based on actual column names)\n",
    "    # Common names: BIOME, ECO_NAME, REALM, etc.\n",
    "    hierarchy_cols = []\n",
    "    for col in ['BIOME', 'REALM', 'ECO_NAME', 'BIOME_NAME']:\n",
    "        if col in joined_df.columns:\n",
    "            hierarchy_cols.append(col)\n",
    "    \n",
    "    if not hierarchy_cols:\n",
    "        # Find columns with categorical data\n",
    "        for col in joined_df.columns:\n",
    "            if joined_df[col].dtype == 'object' and col != 'geometry':\n",
    "                n_unique = joined_df[col].nunique()\n",
    "                if 5 <= n_unique <= 500:\n",
    "                    hierarchy_cols.append(col)\n",
    "        hierarchy_cols = hierarchy_cols[:3]  # Take up to 3\n",
    "    \n",
    "    print(f\"Using hierarchy columns: {hierarchy_cols}\")\n",
    "else:\n",
    "    print(\"Skipping ecoregion test (data not available)\")\n",
    "    hierarchy_cols = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if ecoregions is not None and len(hierarchy_cols) > 0:\n",
    "    print(\"\\nEcoregion Classification Results:\")\n",
    "    print(f\"{'Level':>20} | {'# Classes':>10} | {'L=10 Acc':>10} | {'L=40 Acc':>10} | {'Winner':>10}\")\n",
    "    print(\"-\" * 70)\n",
    "    \n",
    "    coords_tensor = torch.tensor(coords).double()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        emb_l10 = model_l10(coords_tensor.to(device)).cpu().numpy()\n",
    "        emb_l40 = model_l40(coords_tensor.to(device)).cpu().numpy()\n",
    "    \n",
    "    ecoregion_results = []\n",
    "    \n",
    "    for col in hierarchy_cols:\n",
    "        labels = joined_df[col].values\n",
    "        le = LabelEncoder()\n",
    "        y = le.fit_transform(labels)\n",
    "        n_classes = len(le.classes_)\n",
    "        \n",
    "        # Skip if too few classes or too many\n",
    "        if n_classes < 3 or n_classes > 300:\n",
    "            continue\n",
    "        \n",
    "        # Split data\n",
    "        X_train_l10, X_test_l10, y_train, y_test = train_test_split(\n",
    "            emb_l10, y, test_size=0.3, random_state=42, stratify=y\n",
    "        )\n",
    "        X_train_l40, X_test_l40, _, _ = train_test_split(\n",
    "            emb_l40, y, test_size=0.3, random_state=42, stratify=y\n",
    "        )\n",
    "        \n",
    "        # Train classifiers\n",
    "        clf_l10 = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "        clf_l10.fit(X_train_l10, y_train)\n",
    "        acc_l10 = accuracy_score(y_test, clf_l10.predict(X_test_l10))\n",
    "        \n",
    "        clf_l40 = LogisticRegression(max_iter=1000, random_state=42, n_jobs=-1)\n",
    "        clf_l40.fit(X_train_l40, y_train)\n",
    "        acc_l40 = accuracy_score(y_test, clf_l40.predict(X_test_l40))\n",
    "        \n",
    "        winner = \"L=40\" if acc_l40 > acc_l10 + 0.01 else (\"L=10\" if acc_l10 > acc_l40 + 0.01 else \"~Same\")\n",
    "        print(f\"{col:>20} | {n_classes:>10} | {acc_l10:>10.2%} | {acc_l40:>10.2%} | {winner:>10}\")\n",
    "        \n",
    "        ecoregion_results.append({\n",
    "            'level': col,\n",
    "            'n_classes': n_classes,\n",
    "            'l10_acc': acc_l10,\n",
    "            'l40_acc': acc_l40\n",
    "        })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 4. MLP vs Logistic Regression (Checkerboard Revisited)\n",
    "\n",
    "Does a more powerful classifier help extract finer-grained spatial information?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_checkerboard_dataset(cell_size_deg, n_samples=5000, bounds=(-180, 180, -60, 60)):\n",
    "    \"\"\"\n",
    "    Create a checkerboard classification dataset.\n",
    "    \"\"\"\n",
    "    lon_min, lon_max, lat_min, lat_max = bounds\n",
    "    \n",
    "    lons = np.random.uniform(lon_min, lon_max, n_samples)\n",
    "    lats = np.random.uniform(lat_min, lat_max, n_samples)\n",
    "    \n",
    "    cell_x = (lons / cell_size_deg).astype(int)\n",
    "    cell_y = (lats / cell_size_deg).astype(int)\n",
    "    labels = (cell_x + cell_y) % 2\n",
    "    \n",
    "    coords = torch.tensor(np.stack([lons, lats], axis=1)).double()\n",
    "    labels = torch.tensor(labels).long()\n",
    "    \n",
    "    return coords, labels\n",
    "\n",
    "def evaluate_checkerboard_with_classifiers(model, cell_size_deg, n_samples=5000):\n",
    "    \"\"\"\n",
    "    Evaluate with both Logistic Regression and MLP.\n",
    "    \"\"\"\n",
    "    coords, labels = create_checkerboard_dataset(cell_size_deg, n_samples)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        embeddings = model(coords.to(device)).cpu().numpy()\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        embeddings, labels.numpy(), test_size=0.3, random_state=42\n",
    "    )\n",
    "    \n",
    "    # Logistic Regression\n",
    "    clf_lr = LogisticRegression(max_iter=1000, random_state=42)\n",
    "    clf_lr.fit(X_train, y_train)\n",
    "    acc_lr = accuracy_score(y_test, clf_lr.predict(X_test))\n",
    "    \n",
    "    # MLP (2 hidden layers)\n",
    "    clf_mlp = MLPClassifier(\n",
    "        hidden_layer_sizes=(128, 64),\n",
    "        max_iter=500,\n",
    "        random_state=42,\n",
    "        early_stopping=True,\n",
    "        validation_fraction=0.1\n",
    "    )\n",
    "    clf_mlp.fit(X_train, y_train)\n",
    "    acc_mlp = accuracy_score(y_test, clf_mlp.predict(X_test))\n",
    "    \n",
    "    return acc_lr, acc_mlp\n",
    "\n",
    "# Test different cell sizes\n",
    "cell_sizes = [45, 20, 10, 5, 2, 1, 0.5]\n",
    "approx_km = [c * 111 for c in cell_sizes]\n",
    "\n",
    "print(\"Comparing Logistic Regression vs MLP on Checkerboard Task\")\n",
    "print(\"\\nL=10 Model:\")\n",
    "print(f\"{'Cell Size':>10} | {'≈ km':>8} | {'LogReg':>10} | {'MLP':>10} | {'Improvement':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "mlp_results = {'cell_size': [], 'l10_lr': [], 'l10_mlp': [], 'l40_lr': [], 'l40_mlp': []}\n",
    "\n",
    "for cell_size, km in zip(cell_sizes, approx_km):\n",
    "    acc_lr, acc_mlp = evaluate_checkerboard_with_classifiers(model_l10, cell_size, n_samples=4000)\n",
    "    improvement = acc_mlp - acc_lr\n",
    "    print(f\"{cell_size:>8.1f}° | {km:>7.0f} | {acc_lr:>10.2%} | {acc_mlp:>10.2%} | {improvement:>+11.2%}\")\n",
    "    mlp_results['cell_size'].append(cell_size)\n",
    "    mlp_results['l10_lr'].append(acc_lr)\n",
    "    mlp_results['l10_mlp'].append(acc_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nL=40 Model:\")\n",
    "print(f\"{'Cell Size':>10} | {'≈ km':>8} | {'LogReg':>10} | {'MLP':>10} | {'Improvement':>12}\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "for i, (cell_size, km) in enumerate(zip(cell_sizes, approx_km)):\n",
    "    acc_lr, acc_mlp = evaluate_checkerboard_with_classifiers(model_l40, cell_size, n_samples=4000)\n",
    "    improvement = acc_mlp - acc_lr\n",
    "    print(f\"{cell_size:>8.1f}° | {km:>7.0f} | {acc_lr:>10.2%} | {acc_mlp:>10.2%} | {improvement:>+11.2%}\")\n",
    "    mlp_results['l40_lr'].append(acc_lr)\n",
    "    mlp_results['l40_mlp'].append(acc_mlp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot comparison\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# L=10\n",
    "axes[0].plot(approx_km, mlp_results['l10_lr'], 'o-', label='Logistic Regression', linewidth=2, markersize=8)\n",
    "axes[0].plot(approx_km, mlp_results['l10_mlp'], 's-', label='MLP (128, 64)', linewidth=2, markersize=8)\n",
    "axes[0].axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "axes[0].set_xscale('log')\n",
    "axes[0].set_xlabel('Checkerboard Cell Size (km)')\n",
    "axes[0].set_ylabel('Accuracy')\n",
    "axes[0].set_title('L=10: LogReg vs MLP')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "axes[0].set_ylim(0.4, 1.05)\n",
    "\n",
    "# L=40\n",
    "axes[1].plot(approx_km, mlp_results['l40_lr'], 'o-', label='Logistic Regression', linewidth=2, markersize=8)\n",
    "axes[1].plot(approx_km, mlp_results['l40_mlp'], 's-', label='MLP (128, 64)', linewidth=2, markersize=8)\n",
    "axes[1].axhline(y=0.5, color='red', linestyle='--', alpha=0.5, label='Random')\n",
    "axes[1].set_xscale('log')\n",
    "axes[1].set_xlabel('Checkerboard Cell Size (km)')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('L=40: LogReg vs MLP')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "axes[1].set_ylim(0.4, 1.05)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nKey Question: Does MLP help extract finer spatial information from embeddings?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 5. t-SNE/UMAP Visualization\n",
    "\n",
    "Visualize the embedding space to understand its structure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "try:\n",
    "    import umap\n",
    "    HAS_UMAP = True\n",
    "except ImportError:\n",
    "    HAS_UMAP = False\n",
    "    print(\"UMAP not available, using t-SNE only\")\n",
    "\n",
    "# Sample locations from around the world\n",
    "np.random.seed(42)\n",
    "n_viz = 2000\n",
    "\n",
    "# Sample with geographic clustering for visualization\n",
    "viz_lons = np.random.uniform(-180, 180, n_viz)\n",
    "viz_lats = np.random.uniform(-60, 70, n_viz)\n",
    "viz_coords = torch.tensor(np.stack([viz_lons, viz_lats], axis=1)).double()\n",
    "\n",
    "# Get embeddings\n",
    "with torch.no_grad():\n",
    "    viz_emb_l10 = model_l10(viz_coords.to(device)).cpu().numpy()\n",
    "    viz_emb_l40 = model_l40(viz_coords.to(device)).cpu().numpy()\n",
    "\n",
    "print(f\"Generated embeddings for {n_viz} locations\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run dimensionality reduction\n",
    "print(\"Running t-SNE for L=10...\")\n",
    "tsne_l10 = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000)\n",
    "viz_2d_l10_tsne = tsne_l10.fit_transform(viz_emb_l10)\n",
    "\n",
    "print(\"Running t-SNE for L=40...\")\n",
    "tsne_l40 = TSNE(n_components=2, perplexity=30, random_state=42, n_iter=1000)\n",
    "viz_2d_l40_tsne = tsne_l40.fit_transform(viz_emb_l40)\n",
    "\n",
    "if HAS_UMAP:\n",
    "    print(\"Running UMAP for L=10...\")\n",
    "    umap_l10 = umap.UMAP(n_components=2, random_state=42)\n",
    "    viz_2d_l10_umap = umap_l10.fit_transform(viz_emb_l10)\n",
    "    \n",
    "    print(\"Running UMAP for L=40...\")\n",
    "    umap_l40 = umap.UMAP(n_components=2, random_state=42)\n",
    "    viz_2d_l40_umap = umap_l40.fit_transform(viz_emb_l40)\n",
    "\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize - colored by latitude (should show gradient if embeddings capture geography)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "\n",
    "# t-SNE L=10 - colored by latitude\n",
    "sc1 = axes[0, 0].scatter(viz_2d_l10_tsne[:, 0], viz_2d_l10_tsne[:, 1], \n",
    "                          c=viz_lats, cmap='RdYlBu_r', s=5, alpha=0.6)\n",
    "axes[0, 0].set_title('L=10 t-SNE (colored by latitude)')\n",
    "plt.colorbar(sc1, ax=axes[0, 0], label='Latitude')\n",
    "\n",
    "# t-SNE L=40 - colored by latitude\n",
    "sc2 = axes[0, 1].scatter(viz_2d_l40_tsne[:, 0], viz_2d_l40_tsne[:, 1], \n",
    "                          c=viz_lats, cmap='RdYlBu_r', s=5, alpha=0.6)\n",
    "axes[0, 1].set_title('L=40 t-SNE (colored by latitude)')\n",
    "plt.colorbar(sc2, ax=axes[0, 1], label='Latitude')\n",
    "\n",
    "# t-SNE L=10 - colored by longitude\n",
    "sc3 = axes[1, 0].scatter(viz_2d_l10_tsne[:, 0], viz_2d_l10_tsne[:, 1], \n",
    "                          c=viz_lons, cmap='twilight', s=5, alpha=0.6)\n",
    "axes[1, 0].set_title('L=10 t-SNE (colored by longitude)')\n",
    "plt.colorbar(sc3, ax=axes[1, 0], label='Longitude')\n",
    "\n",
    "# t-SNE L=40 - colored by longitude\n",
    "sc4 = axes[1, 1].scatter(viz_2d_l40_tsne[:, 0], viz_2d_l40_tsne[:, 1], \n",
    "                          c=viz_lons, cmap='twilight', s=5, alpha=0.6)\n",
    "axes[1, 1].set_title('L=40 t-SNE (colored by longitude)')\n",
    "plt.colorbar(sc4, ax=axes[1, 1], label='Longitude')\n",
    "\n",
    "plt.suptitle('Embedding Space Visualization (t-SNE)', fontsize=14, y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if HAS_UMAP:\n",
    "    # UMAP visualization\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 12))\n",
    "    \n",
    "    # UMAP L=10 - colored by latitude\n",
    "    sc1 = axes[0, 0].scatter(viz_2d_l10_umap[:, 0], viz_2d_l10_umap[:, 1], \n",
    "                              c=viz_lats, cmap='RdYlBu_r', s=5, alpha=0.6)\n",
    "    axes[0, 0].set_title('L=10 UMAP (colored by latitude)')\n",
    "    plt.colorbar(sc1, ax=axes[0, 0], label='Latitude')\n",
    "    \n",
    "    # UMAP L=40 - colored by latitude\n",
    "    sc2 = axes[0, 1].scatter(viz_2d_l40_umap[:, 0], viz_2d_l40_umap[:, 1], \n",
    "                              c=viz_lats, cmap='RdYlBu_r', s=5, alpha=0.6)\n",
    "    axes[0, 1].set_title('L=40 UMAP (colored by latitude)')\n",
    "    plt.colorbar(sc2, ax=axes[0, 1], label='Latitude')\n",
    "    \n",
    "    # UMAP L=10 - colored by longitude\n",
    "    sc3 = axes[1, 0].scatter(viz_2d_l10_umap[:, 0], viz_2d_l10_umap[:, 1], \n",
    "                              c=viz_lons, cmap='twilight', s=5, alpha=0.6)\n",
    "    axes[1, 0].set_title('L=10 UMAP (colored by longitude)')\n",
    "    plt.colorbar(sc3, ax=axes[1, 0], label='Longitude')\n",
    "    \n",
    "    # UMAP L=40 - colored by longitude\n",
    "    sc4 = axes[1, 1].scatter(viz_2d_l40_umap[:, 0], viz_2d_l40_umap[:, 1], \n",
    "                              c=viz_lons, cmap='twilight', s=5, alpha=0.6)\n",
    "    axes[1, 1].set_title('L=40 UMAP (colored by longitude)')\n",
    "    plt.colorbar(sc4, ax=axes[1, 1], label='Longitude')\n",
    "    \n",
    "    plt.suptitle('Embedding Space Visualization (UMAP)', fontsize=14, y=1.02)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## 6. Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*70)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "print(\"\\n1. SPATIAL INTERPOLATION TASK\")\n",
    "print(\"-\" * 40)\n",
    "if results_interp['l10_r2']:\n",
    "    l40_wins = sum(1 for l10, l40 in zip(results_interp['l10_r2'], results_interp['l40_r2']) if l40 > l10 + 0.01)\n",
    "    print(f\"   L=40 won in {l40_wins}/{len(results_interp['spacing'])} grid spacings\")\n",
    "    print(f\"   Conclusion: {'L=40 better at interpolation as expected' if l40_wins > len(results_interp['spacing'])//2 else 'Mixed results'}\")\n",
    "\n",
    "print(\"\\n2. ECOREGION CLASSIFICATION\")\n",
    "print(\"-\" * 40)\n",
    "if 'ecoregion_results' in dir() and ecoregion_results:\n",
    "    for r in ecoregion_results:\n",
    "        print(f\"   {r['level']}: L=10={r['l10_acc']:.1%}, L=40={r['l40_acc']:.1%}\")\n",
    "else:\n",
    "    print(\"   Skipped (data not available)\")\n",
    "\n",
    "print(\"\\n3. MLP VS LOGISTIC REGRESSION\")\n",
    "print(\"-\" * 40)\n",
    "if mlp_results['l10_lr']:\n",
    "    avg_improvement_l10 = np.mean([m - l for m, l in zip(mlp_results['l10_mlp'], mlp_results['l10_lr'])])\n",
    "    avg_improvement_l40 = np.mean([m - l for m, l in zip(mlp_results['l40_mlp'], mlp_results['l40_lr'])])\n",
    "    print(f\"   L=10 avg MLP improvement: {avg_improvement_l10:+.1%}\")\n",
    "    print(f\"   L=40 avg MLP improvement: {avg_improvement_l40:+.1%}\")\n",
    "    print(f\"   Conclusion: MLP {'helps' if avg_improvement_l10 > 0.02 or avg_improvement_l40 > 0.02 else 'does not significantly help'}\")\n",
    "\n",
    "print(\"\\n4. VISUALIZATION\")\n",
    "print(\"-\" * 40)\n",
    "print(\"   See plots above for embedding structure\")\n",
    "print(\"   Look for: smooth gradients by lat/lon = geographic structure preserved\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
